{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install bs4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=a8c0392252e60fcbf30817ddc4dca28d93468703138641a45ef78fe240c08b50\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "n6ztDOVcubl0CragMT9OYL",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az6Ayr5JIIsV",
        "outputId": "b8885cde-0a97-4396-f86a-b512c18e0608"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "HAOAIdVWUxckuG8mk5DySU",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "RXoarZtmIIsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29ee1b7d96f347ac87d2c85b7df18d4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d9de26ea494895a9bdc89e2868e0b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d699a90d1d8e4d2d965e77e3655f6f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee28e93593284c32b35f846ca909ffcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f56d89e4def34d4ab7a8dd528d79639d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b60dd14e57a4baf958bd82b2bdd28b9"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "eTboOkJjylvagQTXgOFmX1",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "29ee1b7d96f347ac87d2c85b7df18d4e",
            "3d35939e546c41dea7312bbdfc2de293",
            "4b3b81b6a2b746eaaf2e1b735d6bfc59",
            "f7053e9a9c5345dcb86798ca8df7d550",
            "4b3672eba4ee41f09fdb245e2b26b822",
            "8613e45a006648d9997631ea4272c4bc",
            "1eecc1beae1342d099daa3bfdb07390b",
            "243516f7b77645e1bd1d425fcb3f4c77",
            "9d081f67995b4eebb37936893ce7ad6c",
            "ae41d02ab5324cf38bc4423177d4348d",
            "8191c6824dd649439918b15f42449067",
            "10d9de26ea494895a9bdc89e2868e0b7",
            "94a29508c2984e7197c3401c873fdfeb",
            "bc0dab1658eb40428652618f65842398",
            "65e212d1b64e46efa0bc9d37f0cd23a1",
            "23493061151f4ffbbfe8aaf95850a6e9",
            "1e6f7fbae61e4363b92839697f583fdb",
            "2ac305a9b41c4646b41881a062e2185a",
            "1906740a37b94f399b4601ef7d192b10",
            "44bb76ee3f1a419ca63ab41fed99716a",
            "ec82a23e7f3d4fd6ac526ee792dcfdd2",
            "67c5899d27264bb9b7371c894f5e4e34",
            "d699a90d1d8e4d2d965e77e3655f6f77",
            "c47610d259ad482c93f8b919636368ba",
            "02f0a6d1ce8e4ec8b3b70a383d1c0124",
            "dcbc9d6444b846dd8d8ff55cd1360c5c",
            "42945f080d7d46fba74bac12646641f7",
            "51cb1a0725604c278b390494be7780bd",
            "ebc6bd7eaa5e436b96dbee8a2b3f0cbb",
            "c7d5f57027de48aea175a8c60a03a82e",
            "75909d21de3b44d1a5850145f7ef58ba",
            "03efe9ea03bb4e9bbccaea7392bff3c5",
            "057606b171a34fbe804ce0e36cedf03b",
            "ee28e93593284c32b35f846ca909ffcd",
            "946bef713c5b425999d75eb5ba2a643b",
            "e1d3c09d38ba422eb6f6e06a90008f68",
            "57f3022848c14ea2b37c2a67d4f886e2",
            "55b315fd16cf4faaa4cf5b3fef1b43c3",
            "d7e2da9f4acd4396ac34a9bd7631e4df",
            "541ab76cb0ee482eb8b6f0ec01081a64",
            "75160d163d854664b461fa7f895a3348",
            "32199f660eac4a5cbe3cde821a588555",
            "1093ec4782904073b39536328580a06e",
            "698787a400bd4cd39af24f9f105cff4f",
            "f56d89e4def34d4ab7a8dd528d79639d",
            "d131dee1b6aa402ab5bb5f06f5bea702",
            "b34bb350ad314fb79bd7c1c7189dcd89",
            "88509447ff3b4c3c823ed0f7ebbff72c",
            "bdf25daaebe04550a1a7b79b7fd46695",
            "2740ca4a456d4f91aee237a08b999292",
            "2a93e6c725b74b3a922448665ebe174f",
            "c6f716bfc9b64843b4bdb07f084bf984",
            "15d626dc5c7647c1b72d851688ef4024",
            "0906ee44fc4c440aab4c6e51a6568b86",
            "d257a40dc32b45efb17969d5fdf45495",
            "0b60dd14e57a4baf958bd82b2bdd28b9",
            "6070947f67e54c7db0a07d3a80363d8b",
            "611de2c8477c4d66aeea4dbb98fd221a",
            "1d9c17d53f7c47e2a0e28de6041de85e",
            "2b76093fa11640f8b6f38cd93cd64033",
            "3bbbd256b6194f0e8efbed69956b5d2a",
            "cb62c22770ba47febd146186be221880",
            "c6dbf618b9db459bb06c28fcc0756075",
            "0bbc28ddc8044ad299e715830ea303cd",
            "044acbcbb6cd442880c8a7b4b44632fc",
            "4567677e22ac4da284a3aae3dfdc5b65"
          ]
        },
        "id": "tITdXKQnIIsZ",
        "outputId": "41bbfe26-6cea-4066-ccc1-81abefcf0553"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://medium.com/towards-data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb'"
      ],
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "jbzHmcf2p3bv0gE28pLypM",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "LyP15pInIIsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(URL)\n",
        "r.text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html><html lang=\"en\"><head><title data-rh=\"true\">Build your own Transformer from scratch using Pytorch | by Arjun Sarkar | Towards Data Science</title><meta data-rh=\"true\" charset=\"utf-8\"/><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1\"/><meta data-rh=\"true\" name=\"theme-color\" content=\"#000000\"/><meta data-rh=\"true\" name=\"twitter:app:name:iphone\" content=\"Medium\"/><meta data-rh=\"true\" name=\"twitter:app:id:iphone\" content=\"828256236\"/><meta data-rh=\"true\" property=\"al:ios:app_name\" content=\"Medium\"/><meta data-rh=\"true\" property=\"al:ios:app_store_id\" content=\"828256236\"/><meta data-rh=\"true\" property=\"al:android:package\" content=\"com.medium.reader\"/><meta data-rh=\"true\" property=\"fb:app_id\" content=\"542599432471018\"/><meta data-rh=\"true\" property=\"og:site_name\" content=\"Medium\"/><meta data-rh=\"true\" property=\"og:type\" content=\"article\"/><meta data-rh=\"true\" property=\"article:published_time\" content=\"2023-04-26T13:19:47.389Z\"/><meta data-rh=\"true\" name=\"title\" content=\"Build your own Transformer from scratch using Pytorch | by Arjun Sarkar | Towards Data Science\"/><meta data-rh=\"true\" property=\"og:title\" content=\"Build your own Transformer from scratch using Pytorch\"/><meta data-rh=\"true\" property=\"al:android:url\" content=\"medium://p/84c850470dcb\"/><meta data-rh=\"true\" property=\"al:ios:url\" content=\"medium://p/84c850470dcb\"/><meta data-rh=\"true\" property=\"al:android:app_name\" content=\"Medium\"/><meta data-rh=\"true\" name=\"description\" content=\"In this tutorial, we will build a basic Transformer model from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning…\"/><meta data-rh=\"true\" property=\"og:description\" content=\"Building a Transformer model step by step in Pytorch\"/><meta data-rh=\"true\" property=\"og:url\" content=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"/><meta data-rh=\"true\" property=\"al:web:url\" content=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"/><meta data-rh=\"true\" property=\"og:image\" content=\"https://miro.medium.com/v2/resize:fit:1200/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"/><meta data-rh=\"true\" property=\"article:author\" content=\"https://arjun-sarkar786.medium.com\"/><meta data-rh=\"true\" name=\"author\" content=\"Arjun Sarkar\"/><meta data-rh=\"true\" name=\"robots\" content=\"index,follow,max-image-preview:large\"/><meta data-rh=\"true\" name=\"referrer\" content=\"unsafe-url\"/><meta data-rh=\"true\" property=\"twitter:title\" content=\"Build your own Transformer from scratch using Pytorch\"/><meta data-rh=\"true\" name=\"twitter:site\" content=\"@TDataScience\"/><meta data-rh=\"true\" name=\"twitter:app:url:iphone\" content=\"medium://p/84c850470dcb\"/><meta data-rh=\"true\" property=\"twitter:description\" content=\"Building a Transformer model step by step in Pytorch\"/><meta data-rh=\"true\" name=\"twitter:image:src\" content=\"https://miro.medium.com/v2/resize:fit:1200/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"/><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"/><meta data-rh=\"true\" name=\"twitter:label1\" content=\"Reading time\"/><meta data-rh=\"true\" name=\"twitter:data1\" content=\"7 min read\"/><meta data-rh=\"true\" name=\"twitter:tile:template:testing\" content=\"2\"/><meta data-rh=\"true\" name=\"twitter:tile:image\" content=\"https://miro.medium.com/v2/resize:fit:1200/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"/><meta data-rh=\"true\" name=\"twitter:tile:info1:icon\" content=\"Person\"/><meta data-rh=\"true\" name=\"twitter:tile:info1:text\" content=\"Arjun Sarkar\"/><meta data-rh=\"true\" name=\"twitter:tile:info2:icon\" content=\"Calendar\"/><meta data-rh=\"true\" name=\"twitter:tile:info2:text\" content=\"Apr 26, 2023\"/><meta data-rh=\"true\" name=\"twitter:cta\" content=\"Read on Medium\"/><link data-rh=\"true\" rel=\"icon\" href=\"https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png\"/><link data-rh=\"true\" rel=\"search\" type=\"application/opensearchdescription+xml\" title=\"Medium\" href=\"/osd.xml\"/><link data-rh=\"true\" rel=\"apple-touch-icon\" sizes=\"152x152\" href=\"https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\"/><link data-rh=\"true\" rel=\"apple-touch-icon\" sizes=\"120x120\" href=\"https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\"/><link data-rh=\"true\" rel=\"apple-touch-icon\" sizes=\"76x76\" href=\"https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\"/><link data-rh=\"true\" rel=\"apple-touch-icon\" sizes=\"60x60\" href=\"https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\"/><link data-rh=\"true\" rel=\"mask-icon\" href=\"https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg\" color=\"#171717\"/><link data-rh=\"true\" id=\"glyph_preload_link\" rel=\"preload\" as=\"style\" type=\"text/css\" href=\"https://glyph.medium.com/css/unbound.css\"/><link data-rh=\"true\" id=\"glyph_link\" rel=\"stylesheet\" type=\"text/css\" href=\"https://glyph.medium.com/css/unbound.css\"/><link data-rh=\"true\" rel=\"author\" href=\"https://arjun-sarkar786.medium.com\"/><link data-rh=\"true\" rel=\"canonical\" href=\"https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"/><link data-rh=\"true\" rel=\"alternate\" href=\"android-app://com.medium.reader/https/medium.com/p/84c850470dcb\"/><script data-rh=\"true\" type=\"application/ld+json\">{\"@context\":\"http:\\\\u002F\\\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":[\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fv2\\\\u002Fresize:fit:1200\\\\u002F1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"],\"url\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\",\"dateCreated\":\"2023-04-26T13:19:47.389Z\",\"datePublished\":\"2023-04-26T13:19:47.389Z\",\"dateModified\":\"2023-05-19T09:11:52.007Z\",\"headline\":\"Build your own Transformer from scratch using Pytorch\",\"name\":\"Build your own Transformer from scratch using Pytorch\",\"description\":\"In this tutorial, we will build a basic Transformer model from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning…\",\"identifier\":\"84c850470dcb\",\"author\":{\"@type\":\"Person\",\"name\":\"Arjun Sarkar\",\"url\":\"https:\\\\u002F\\\\u002Farjun-sarkar786.medium.com\"},\"creator\":[\"Arjun Sarkar\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Towards Data Science\",\"url\":\"towardsdatascience.com\",\"logo\":{\"@type\":\"ImageObject\",\"width\":192,\"height\":60,\"url\":\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fv2\\\\u002Fresize:fit:384\\\\u002F1*cFFKn8rFH4ZndmaYeAs6iQ.png\"}},\"mainEntityOfPage\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\"}</script><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"STATIC\">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden=\"true\"]{visibility:hidden;pointer-events:none}\\n/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;\\n}/* Gray DOCTYPE selectors like WebKit */\\n.xml .hljs-meta {color: #c0c0c0;\\n}.hljs-comment,\\n.hljs-quote {color: #007400;\\n}.hljs-tag,\\n.hljs-attribute,\\n.hljs-keyword,\\n.hljs-selector-tag,\\n.hljs-literal,\\n.hljs-name {color: #aa0d91;\\n}.hljs-variable,\\n.hljs-template-variable {color: #3F6E74;\\n}.hljs-code,\\n.hljs-string,\\n.hljs-meta .hljs-string {color: #c41a16;\\n}.hljs-regexp,\\n.hljs-link {color: #0E0EFF;\\n}.hljs-title,\\n.hljs-symbol,\\n.hljs-bullet,\\n.hljs-number {color: #1c00cf;\\n}.hljs-section,\\n.hljs-meta {color: #643820;\\n}.hljs-title.class_,\\n.hljs-class .hljs-title,\\n.hljs-type,\\n.hljs-built_in,\\n.hljs-params {color: #5c2699;\\n}.hljs-attr {color: #836C28;\\n}.hljs-subst {color: #000;\\n}.hljs-formula {background-color: #eee;font-style: italic;\\n}.hljs-addition {background-color: #baeeba;\\n}.hljs-deletion {background-color: #ffc8bd;\\n}.hljs-selector-id,\\n.hljs-selector-class {color: #9b703f;\\n}.hljs-doctag,\\n.hljs-strong {font-weight: bold;\\n}.hljs-emphasis {font-style: italic;\\n}\\n</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"KEYFRAME\">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{height:25px}.av{fill:rgba(41, 41, 41, 1)}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, \"Helvetica Neue\", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.ds{margin-left:8px}.dt{color:#6B6B6B}.du{font-size:13px}.dv{height:100%}.el{color:#FFFFFF}.em{fill:#FFFFFF}.eo{background:rgba(102, 138, 170, 1)}.ep{border-color:rgba(102, 138, 170, 1)}.et:disabled{cursor:inherit !important}.eu:disabled{opacity:0.3}.ev:disabled:hover{background:rgba(102, 138, 170, 1)}.ew:disabled:hover{border-color:rgba(102, 138, 170, 1)}.ex{border-radius:99em}.ey{border-width:1px}.ez{border-style:solid}.fa{box-sizing:border-box}.fb{text-decoration:none}.fe{margin-right:32px}.ff{position:relative}.fg{fill:#6B6B6B}.fj{background:transparent}.fk svg{margin-left:4px}.fl svg{fill:#6B6B6B}.fn{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fo{position:absolute}.fv{margin:0 24px}.fz{background:rgba(255, 255, 255, 1)}.ga{border:1px solid #F2F2F2}.gb{box-shadow:0 1px 4px #F2F2F2}.gc{max-height:100vh}.gd{overflow-y:auto}.ge{left:0}.gf{top:calc(100vh + 100px)}.gg{bottom:calc(100vh + 100px)}.gh{width:10px}.gi{pointer-events:none}.gj{word-break:break-word}.gk{word-wrap:break-word}.gl:after{display:block}.gm:after{content:\"\"}.gn:after{clear:both}.go{line-height:1.23}.gp{letter-spacing:0}.gq{font-style:normal}.gr{font-weight:700}.hm{margin-bottom:-0.27em}.hn{line-height:1.394}.id{@media all and (max-width: 551.98px):8px}.ie{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.if{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.ig{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.ih{@media all and (min-width: 1080px):16px}.in{align-items:baseline}.io{width:48px}.ip{height:48px}.iq{border:2px solid rgba(255, 255, 255, 1)}.ir{z-index:0}.is{box-shadow:none}.it{border:1px solid rgba(0, 0, 0, 0.05)}.iv{margin-left:-12px}.iw{width:28px}.ix{height:28px}.iy{z-index:1}.iz{width:24px}.ja{margin-bottom:2px}.jb{flex-wrap:nowrap}.jc{font-size:16px}.jd{line-height:24px}.jf{margin:0 8px}.jg{display:inline}.jh{color:rgba(102, 138, 170, 1)}.ji{fill:rgba(102, 138, 170, 1)}.jl{flex:0 0 auto}.jo{flex-wrap:wrap}.jr{white-space:pre-wrap}.js{margin-right:4px}.jt{overflow:hidden}.ju{max-height:20px}.jv{text-overflow:ellipsis}.jw{display:-webkit-box}.jx{-webkit-line-clamp:1}.jy{-webkit-box-orient:vertical}.jz{word-break:break-all}.kb{padding-left:8px}.kc{padding-right:8px}.ld> *{flex-shrink:0}.le{overflow-x:scroll}.lf::-webkit-scrollbar{display:none}.lg{scrollbar-width:none}.lh{-ms-overflow-style:none}.li{width:74px}.lj{flex-direction:row}.lm{-webkit-user-select:none}.ln{border:0}.lo{fill:rgba(117, 117, 117, 1)}.lr{outline:0}.ls{user-select:none}.lt> svg{pointer-events:none}.mc{cursor:progress}.md{margin-left:4px}.me{margin-top:0px}.mf{opacity:1}.mg{padding:4px 0}.mj{width:16px}.mk path{fill:#242424}.ml{padding:8px 2px}.mo svg path{fill:#6B6B6B}.mp svg{color:#6B6B6B}.ng{margin-left:auto}.nh{margin-right:auto}.ni{max-width:3353px}.no{clear:both}.nq{cursor:zoom-in}.nr{z-index:auto}.nt{max-width:100%}.nu{height:auto}.nv{margin-top:10px}.nw{text-align:center}.nx{max-width:728px}.oa{text-decoration:underline}.ob{line-height:1.58}.oc{letter-spacing:-0.004em}.od{font-family:source-serif-pro, Georgia, Cambria, \"Times New Roman\", Times, serif}.ow{margin-bottom:-0.46em}.ox{line-height:1.18}.oy{letter-spacing:-0.022em}.oz{font-weight:600}.pp{margin-bottom:-0.31em}.pq{margin-top:32px}.pr{margin-bottom:14px}.ps{padding-top:24px}.pt{padding-bottom:10px}.pu{background-color:#000000}.pv{height:3px}.pw{width:3px}.px{margin-right:20px}.qb{list-style-type:decimal}.qc{margin-left:30px}.qd{padding-left:0px}.qj{overflow-x:auto}.qk{font-family:source-code-pro, Menlo, Monaco, \"Courier New\", Courier, monospace}.ql{padding:32px}.qm{border:1px solid #E5E5E5}.qn{line-height:1.4}.qo{margin-top:-0.2em}.qp{margin-bottom:-0.2em}.qq{white-space:pre}.qr{min-width:fit-content}.qs{line-height:1.12}.rj{margin-bottom:-0.28em}.rk{max-width:1215px}.rq{max-width:276px}.rr{max-width:545px}.rx{font-style:italic}.ry{margin-bottom:26px}.rz{margin-top:6px}.sa{margin-top:8px}.sb{margin-right:8px}.sc{padding:8px 16px}.sd{border-radius:100px}.se{transition:background 300ms ease}.sg{white-space:nowrap}.sh{border-top:none}.sn{height:52px}.so{max-height:52px}.sp{box-sizing:content-box}.sq{position:static}.ss{max-width:155px}.td{align-items:flex-end}.te{width:76px}.tf{height:76px}.tg{border:2px solid #F9F9F9}.th{height:72px}.ti{width:72px}.tj{margin-left:-16px}.tk{width:36px}.tl{height:36px}.tm{width:auto}.tn{stroke:#F2F2F2}.to{color:#F2F2F2}.tp{fill:#F2F2F2}.tq{background:#F2F2F2}.tr{border-color:#F2F2F2}.tx{font-weight:500}.ty{font-size:24px}.tz{line-height:30px}.ua{letter-spacing:-0.016em}.ub{margin-top:16px}.uc{height:0px}.ud{border-bottom:solid 1px #E5E5E5}.uj{margin-top:72px}.uk{padding:24px 0}.ul{margin-bottom:0px}.um{margin-right:16px}.un{display:inline-flex}.ut{margin-bottom:32px}.uu{margin-top:40px}.uv{align-items:stretch}.wf{flex-grow:0}.wl{display:grid}.wm{grid-template-columns:repeat(12, 1fr)}.wn{grid-template-rows:auto 1fr}.wy{grid-area:image}.wz{grid-area:content}.xf{border-radius:2px}.xg{aspect-ratio:2}.xh{object-fit:cover}.xi{object-position:50% 50%}.xo{height:20px}.xp{width:20px}.xq{padding-right:4px}.yl{padding-top:8px}.ym{max-height:40px}.yn{-webkit-line-clamp:2}.zd{margin-left:20px}.ze{justify-content:flex-end}.zf{flex:0 0 0}.zg{margin-left:2px}.zh{margin-top:2px}.zi{cursor:initial}.zp{fill:#242424}.zq{background:0}.zr{border-color:#242424}.zu:disabled:hover{color:#242424}.zv:disabled:hover{fill:#242424}.zw:disabled:hover{border-color:#242424}.abq{padding-bottom:40px}.abr{padding-top:88px}.abs{margin-bottom:40px}.abt{margin-top:4px}.abu{border-right:3px solid #F9F9F9}.abv{z-index:3}.abw{z-index:2}.abx{margin-left:-24px}.aby{margin-left:-36px}.abz{border-radius:0 3px 3px 0}.aca{width:93px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.eq:hover{background:rgba(90, 118, 144, 1)}.er:hover{border-color:rgba(90, 118, 144, 1)}.es:hover{cursor:pointer}.fh:hover{color:#242424}.fi:hover{fill:#242424}.fm:hover svg{fill:#242424}.fp:hover{background-color:none}.iu:hover{background-color:rgba(0, 0, 0, 0.1)}.je:hover{text-decoration:underline}.jj:hover:not(:disabled){color:rgba(90, 118, 144, 1)}.jk:hover:not(:disabled){fill:rgba(90, 118, 144, 1)}.lq:hover{fill:rgba(8, 8, 8, 1)}.mh:hover{fill:#000000}.mi:hover p{color:#000000}.mm:hover:not(:disabled) svg path{fill:#000000}.mq:hover svg{color:#000000}.sf:hover{background-color:#F2F2F2}.ts:hover{background:#F2F2F2}.tt:hover{border-color:#F2F2F2}.tu:hover{cursor:wait}.tv:hover{color:#F2F2F2}.tw:hover{fill:#F2F2F2}.zs:hover{color:#000000}.zt:hover{border-color:#242424}.bc:focus-within path{fill:#242424}.lp:focus{fill:rgba(8, 8, 8, 1)}.mn:focus svg path{fill:#000000}.mr:focus svg{color:#000000}.ns:focus{transform:scale(1.01)}.lu:active{border-style:none}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (min-width: 1080px)\">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ee{font-size:14px}.ef{line-height:20px}.ek{padding:5px 12px}.fd{display:flex}.fu{margin-bottom:68px}.fy{max-width:680px}.hi{font-size:42px}.hj{margin-top:1.19em}.hk{line-height:52px}.hl{letter-spacing:-0.011em}.ia{font-size:22px}.ib{margin-top:0.92em}.ic{line-height:28px}.im{align-items:center}.kp{border-top:solid 1px #F2F2F2}.kq{border-bottom:solid 1px #F2F2F2}.kr{margin:32px 0 0}.ks{padding:3px 8px}.lb> *{margin-right:24px}.lc> :last-child{margin-right:0}.mb{margin-top:0px}.nn{margin-top:56px}.os{font-size:20px}.ot{margin-top:2em}.ou{line-height:32px}.ov{letter-spacing:-0.003em}.pm{margin-top:1.72em}.pn{line-height:24px}.po{letter-spacing:0}.qa{font-size:21px}.qi{margin-top:1.05em}.rf{font-size:24px}.rg{margin-top:1.95em}.rh{line-height:30px}.ri{letter-spacing:-0.016em}.rp{margin-top:0.86em}.rw{margin-top:1.25em}.sm{margin-bottom:88px}.sx{display:inline-block}.tc{padding-top:72px}.ui{margin-top:40px}.us{margin:0}.vi{width:calc(100% + 32px)}.vj{margin-left:-16px}.vk{margin-right:-16px}.wb{padding-left:16px}.wc{padding-right:16px}.wd{flex-basis:50%}.we{max-width:50%}.wk{padding-bottom:56px}.ww{gap:24px 0}.wx{grid-template-areas:\"image image image image image image image image image image image image\" \"content content content content content content content content content content content content\"}.xe{display:block}.xn{margin-bottom:16px}.xz{padding-bottom:16px}.ya{flex:1 0 auto}.yj{max-height:48px}.yk{-webkit-line-clamp:2}.ys{padding-top:16px}.zb{max-width:56%}.zc{flex:1 0 0}.zl{margin-bottom:24px}.zo{flex-direction:row}.abb{width:min-width}.abk{margin-left:16px}.abp{margin-top:96px}.acf{margin-top:24px}.acg{margin-bottom:56px}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (max-width: 1079.98px)\">.e{display:none}.ma{margin-top:0px}.ny{margin-left:auto}.nz{text-align:center}.sw{display:inline-block}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (max-width: 903.98px)\">.f{display:none}.lz{margin-top:0px}.sv{display:inline-block}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (max-width: 727.98px)\">.g{display:none}.lx{margin-top:0px}.ly{margin-right:0px}.su{display:inline-block}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (max-width: 551.98px)\">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.dw{font-size:13px}.dx{line-height:20px}.eg{padding:0px 8px 1px}.fq{margin-bottom:4px}.gs{font-size:32px}.gt{margin-top:1.01em}.gu{line-height:38px}.gv{letter-spacing:-0.014em}.ho{font-size:18px}.hp{margin-top:0.79em}.hq{line-height:24px}.ii{align-items:flex-start}.jm{flex-direction:column}.jp{margin-bottom:2px}.kd{margin:24px -24px 0}.ke{padding:0}.kt> *{margin-right:8px}.ku> :last-child{margin-right:24px}.lk{margin-left:0px}.lv{margin-top:0px}.lw{margin-right:0px}.ms{border:1px solid #F2F2F2}.mt{border-radius:99em}.mu{padding:0px 16px 0px 12px}.mv{height:38px}.mw{align-items:center}.my svg{margin-right:8px}.nj{margin-top:40px}.oe{margin-top:1.56em}.of{line-height:28px}.og{letter-spacing:-0.003em}.pa{font-size:16px}.pb{margin-top:1.23em}.pc{letter-spacing:0}.qe{margin-top:1.34em}.qt{font-size:20px}.qu{margin-top:1.2em}.rl{margin-top:0.67em}.rs{margin-top:0.93em}.si{margin-bottom:80px}.st{display:inline-block}.sy{padding-top:48px}.ue{margin-top:32px}.uo{margin:0}.uw{width:calc(100% + 24px)}.ux{margin-left:-12px}.uy{margin-right:-12px}.vl{padding-left:12px}.vm{padding-right:12px}.vn{flex-basis:100%}.vo{max-width:100%}.wg{padding-bottom:32px}.wo{gap:24px 0}.wp{grid-template-areas:\"image image image image image image image image image image image image\" \"content content content content content content content content content content content content\"}.xa{display:block}.xj{margin-bottom:16px}.xr{padding-bottom:16px}.xs{flex:1 0 auto}.yb{max-height:48px}.yc{-webkit-line-clamp:2}.yo{padding-top:16px}.yt{max-width:56%}.yu{flex:1 0 0}.zx{width:100%}.abc{margin-left:0}.abd{margin-top:16px}.abl{margin-top:72px}.mx:hover{border-color:#E5E5E5}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (min-width: 904px) and (max-width: 1079.98px)\">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{font-size:14px}.ed{line-height:20px}.ej{padding:5px 12px}.fc{display:flex}.ft{margin-bottom:68px}.fx{max-width:680px}.he{font-size:42px}.hf{margin-top:1.19em}.hg{line-height:52px}.hh{letter-spacing:-0.011em}.hx{font-size:22px}.hy{margin-top:0.92em}.hz{line-height:28px}.il{align-items:center}.kl{border-top:solid 1px #F2F2F2}.km{border-bottom:solid 1px #F2F2F2}.kn{margin:32px 0 0}.ko{padding:3px 8px}.kz> *{margin-right:24px}.la> :last-child{margin-right:0}.nm{margin-top:56px}.oo{font-size:20px}.op{margin-top:2em}.oq{line-height:32px}.or{letter-spacing:-0.003em}.pj{margin-top:1.72em}.pk{line-height:24px}.pl{letter-spacing:0}.pz{font-size:21px}.qh{margin-top:1.05em}.rb{font-size:24px}.rc{margin-top:1.95em}.rd{line-height:30px}.re{letter-spacing:-0.016em}.ro{margin-top:0.86em}.rv{margin-top:1.25em}.sl{margin-bottom:88px}.tb{padding-top:72px}.uh{margin-top:40px}.ur{margin:0}.vf{width:calc(100% + 32px)}.vg{margin-left:-16px}.vh{margin-right:-16px}.vx{padding-left:16px}.vy{padding-right:16px}.vz{flex-basis:50%}.wa{max-width:50%}.wj{padding-bottom:56px}.wu{gap:24px 0}.wv{grid-template-areas:\"image image image image image image image image image image image image\" \"content content content content content content content content content content content content\"}.xd{display:block}.xm{margin-bottom:16px}.xx{padding-bottom:16px}.xy{flex:1 0 auto}.yh{max-height:48px}.yi{-webkit-line-clamp:2}.yr{padding-top:16px}.yz{max-width:56%}.za{flex:1 0 0}.zk{margin-bottom:24px}.zn{flex-direction:row}.aba{width:min-width}.abi{margin-left:16px}.abj{margin-top:0px}.abo{margin-top:96px}.acd{margin-top:24px}.ace{margin-bottom:56px}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (min-width: 728px) and (max-width: 903.98px)\">.j{display:none}.w{display:flex}.x{justify-content:flex-end}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.ea{font-size:13px}.eb{line-height:20px}.ei{padding:0px 8px 1px}.fs{margin-bottom:68px}.fw{max-width:680px}.ha{font-size:42px}.hb{margin-top:1.19em}.hc{line-height:52px}.hd{letter-spacing:-0.011em}.hu{font-size:22px}.hv{margin-top:0.92em}.hw{line-height:28px}.ik{align-items:center}.kh{border-top:solid 1px #F2F2F2}.ki{border-bottom:solid 1px #F2F2F2}.kj{margin:32px 0 0}.kk{padding:3px 8px}.kx> *{margin-right:24px}.ky> :last-child{margin-right:0}.nl{margin-top:56px}.ok{font-size:20px}.ol{margin-top:2em}.om{line-height:32px}.on{letter-spacing:-0.003em}.pg{margin-top:1.72em}.ph{line-height:24px}.pi{letter-spacing:0}.py{font-size:21px}.qg{margin-top:1.05em}.qx{font-size:24px}.qy{margin-top:1.95em}.qz{line-height:30px}.ra{letter-spacing:-0.016em}.rn{margin-top:0.86em}.ru{margin-top:1.25em}.sk{margin-bottom:88px}.ta{padding-top:72px}.ug{margin-top:40px}.uq{margin:0}.vc{width:calc(100% + 28px)}.vd{margin-left:-14px}.ve{margin-right:-14px}.vt{padding-left:14px}.vu{padding-right:14px}.vv{flex-basis:50%}.vw{max-width:50%}.wi{padding-bottom:56px}.ws{gap:24px 0}.wt{grid-template-areas:\"image image image image image image image image image image image image\" \"content content content content content content content content content content content content\"}.xc{display:block}.xl{margin-bottom:16px}.xv{padding-bottom:16px}.xw{flex:1 0 auto}.yf{max-height:48px}.yg{-webkit-line-clamp:2}.yq{padding-top:16px}.yx{max-width:56%}.yy{flex:1 0 0}.zj{margin-bottom:24px}.zm{flex-direction:row}.zz{width:min-width}.abg{margin-left:16px}.abh{margin-top:0px}.abn{margin-top:96px}.acb{margin-top:24px}.acc{margin-bottom:56px}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"all and (min-width: 552px) and (max-width: 727.98px)\">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dy{font-size:13px}.dz{line-height:20px}.eh{padding:0px 8px 1px}.fr{margin-bottom:4px}.gw{font-size:32px}.gx{margin-top:1.01em}.gy{line-height:38px}.gz{letter-spacing:-0.014em}.hr{font-size:18px}.hs{margin-top:0.79em}.ht{line-height:24px}.ij{align-items:flex-start}.jn{flex-direction:column}.jq{margin-bottom:2px}.kf{margin:24px 0 0}.kg{padding:0}.kv> *{margin-right:8px}.kw> :last-child{margin-right:8px}.ll{margin-left:0px}.mz{border:1px solid #F2F2F2}.na{border-radius:99em}.nb{padding:0px 16px 0px 12px}.nc{height:38px}.nd{align-items:center}.nf svg{margin-right:8px}.nk{margin-top:40px}.oh{margin-top:1.56em}.oi{line-height:28px}.oj{letter-spacing:-0.003em}.pd{font-size:16px}.pe{margin-top:1.23em}.pf{letter-spacing:0}.qf{margin-top:1.34em}.qv{font-size:20px}.qw{margin-top:1.2em}.rm{margin-top:0.67em}.rt{margin-top:0.93em}.sj{margin-bottom:80px}.sz{padding-top:48px}.uf{margin-top:32px}.up{margin:0}.uz{width:calc(100% + 24px)}.va{margin-left:-12px}.vb{margin-right:-12px}.vp{padding-left:12px}.vq{padding-right:12px}.vr{flex-basis:100%}.vs{max-width:100%}.wh{padding-bottom:32px}.wq{gap:24px 0}.wr{grid-template-areas:\"image image image image image image image image image image image image\" \"content content content content content content content content content content content content\"}.xb{display:block}.xk{margin-bottom:16px}.xt{padding-bottom:16px}.xu{flex:1 0 auto}.yd{max-height:48px}.ye{-webkit-line-clamp:2}.yp{padding-top:16px}.yv{max-width:56%}.yw{flex:1 0 0}.zy{width:100%}.abe{margin-left:0}.abf{margin-top:16px}.abm{margin-top:72px}.ne:hover{border-color:#E5E5E5}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"print\">.sr{display:none}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"(orientation: landscape) and (max-width: 903.98px)\">.ka{max-height:none}</style><style type=\"text/css\" data-fela-rehydration=\"734\" data-fela-type=\"RULE\" media=\"(prefers-reduced-motion: no-preference)\">.np{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id=\"root\"><div class=\"a b c\"><div class=\"d e f g h i j k\"></div><script>document.domain = document.domain;</script><div class=\"l c\"><div class=\"l m n o c\"><div class=\"p q r s t u v w x i d y z\"><a class=\"dt ag du be ak b am an ao ap aq ar as at s u j i d q dv z\" href=\"https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F84c850470dcb&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\">Open in app<svg width=\"10\" height=\"10\" viewBox=\"0 0 10 10\" fill=\"none\" class=\"ds\"><path d=\"M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z\" fill=\"currentColor\"></path></svg></a><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"be b dw dx eg dy dz eh ea eb ei ec ed ej ee ef ek el em eo ep eq er es et eu ev ew ex ey ez fa bl fb\" data-testid=\"headerSignUpButton\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign up</a></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign In</a></span></p></div></div></div><div class=\"p q r ab ac\"><div class=\"ab q ae\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab\" aria-label=\"Homepage\" data-testid=\"headerMediumLogo\" href=\"https://medium.com/?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><svg viewBox=\"0 0 1043.63 592.71\" class=\"au av\"><g data-name=\"Layer 2\"><g data-name=\"Layer 1\"><path d=\"M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94\"></path></g></g></svg></a><div class=\"aw h\"><div class=\"ab ax ay az ba q bb bc\"><div class=\"bl\" aria-hidden=\"false\" aria-describedby=\"searchResults\" aria-labelledby=\"searchResults\"></div><div class=\"bm bn ab\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\"></path></svg></div><input role=\"combobox\" aria-controls=\"searchResults\" aria-expanded=\"false\" aria-label=\"search\" data-testid=\"headerSearchInput\" tabindex=\"0\" class=\"ax bd be bf z bg bh bi bj bk\" placeholder=\"Search Medium\" value=\"\"/></div></div></div><div class=\"h k w fc fd\"><div class=\"fe ab\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerWriteButton\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---two_column_layout_nav-----------------------new_post_topnav-----------\" rel=\"noopener follow\"><div class=\"be b bf z dt ff fg ab q fh fi\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" aria-label=\"Write\"><path d=\"M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z\" fill=\"currentColor\"></path><path d=\"M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2\" stroke=\"currentColor\"></path></svg><div class=\"ds l\">Write</div></div></a></span></div></div><div class=\"k j i d\"><div class=\"fe ab\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSearchButton\" href=\"https://medium.com/search?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><div class=\"be b bf z dt ff fg ab q fh fi\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" aria-label=\"Search\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\"></path></svg></div></a></div></div><div class=\"fe h k j\"><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"be b dw dx eg dy dz eh ea eb ei ec ed ej ee ef ek el em eo ep eq er es et eu ev ew ex ey ez fa bl fb\" data-testid=\"headerSignUpButton\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign up</a></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign In</a></span></p></div></div></div><div class=\"l\" aria-hidden=\"false\"><button class=\"ax fj am ab q ao fk fl fm\" aria-label=\"user options menu\" data-testid=\"headerUserIcon\"><div class=\"l ff\"><img alt=\"\" class=\"l fa bx by bz cw\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png\" width=\"32\" height=\"32\" loading=\"lazy\" role=\"presentation\"/><div class=\"fn bx l by bz fo n ax fp\"></div></div><svg width=\"12px\" height=\"12px\" viewBox=\"0 0 15 15\"><path d=\"M3.85 5.15a.5.5 0 0 0-.7.7l4.35 4.36 4.35-4.36a.5.5 0 1 0-.7-.7L7.5 8.79 3.85 5.15z\" fill-rule=\"evenodd\"></path></svg></button></div></div></div><div class=\"l\"><div class=\"fq fr fs ft fu l\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"></div></div><article><div class=\"l\"><div class=\"l\"><span class=\"l\"></span><section><div><div class=\"fo ge gf gg gh gi\"></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"\"><h1 id=\"2ae4\" class=\"pw-post-title go gp gq be gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl hm bj\" data-testid=\"storyTitle\">Build your own Transformer from scratch using Pytorch</h1></div><div class=\"\"><h2 id=\"f3e2\" class=\"pw-subtitle-paragraph hn gp gq be b ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic cp dt\">Building a Transformer model step by step in Pytorch</h2><div class=\"id ie if ig ih\"><div class=\"speechify-ignore ab co\"><div class=\"speechify-ignore bg l\"><div class=\"ii ij ik il im ab\"><div><div class=\"ab in\"><a href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div><div class=\"bl\" aria-hidden=\"false\"><div class=\"l io ip bx iq ir\"><div class=\"l ff\"><img alt=\"Arjun Sarkar\" class=\"l fa bx dc dd cw\" src=\"https://miro.medium.com/v2/resize:fill:88:88/0*zwki88OLZVcjhvge\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/><div class=\"is bx l dc dd fo n it iu\"></div></div></div></div></div></a><a href=\"https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"iv ab ff\"><div><div class=\"bl\" aria-hidden=\"false\"><div class=\"l iw ix bx iq iy\"><div class=\"l ff\"><img alt=\"Towards Data Science\" class=\"l fa bx bq iz cw\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/><div class=\"is bx l bq iz fo n it iu\"></div></div></div></div></div></div></a></div></div><div class=\"bm bg l\"><div class=\"ab\"><div style=\"flex:1\"><span class=\"be b bf z bj\"><div class=\"ja ab q\"><div class=\"ab q jb\"><div class=\"ab q\"><div><div class=\"bl\" aria-hidden=\"false\"><p class=\"be b jc jd bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je\" data-testid=\"authorName\" href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\">Arjun Sarkar</a></p></div></div></div><span class=\"jf jg\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><p class=\"be b jc jd dt\"><span><a class=\"jh ji ah ai aj ak al am an ao ap aq ar eu jj jk\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=post_page-fa31366b2eda----84c850470dcb---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p></div></div></span></div></div><div class=\"l jl\"><span class=\"be b bf z dt\"><div class=\"ab cm jm jn jo\"><div class=\"jp jq ab\"><div class=\"be b bf z dt ab jr\"><span class=\"js l jl\">Published in</span><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" data-testid=\"publicationName\" href=\"https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b bf z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div><div class=\"h k\"><span class=\"jf jg\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span></div></div><span class=\"be b bf z dt\"><div class=\"ab ae\"><span data-testid=\"storyReadTime\">7 min read</span><div class=\"kb kc l\" aria-hidden=\"true\"><span class=\"l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span></div><span data-testid=\"storyPublishDate\">Apr 26</span></div></span></div></span></div></div></div><div class=\"ab co kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks\"><div class=\"h k w fc fd q\"><div class=\"li l\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerClapButton\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----84c850470dcb---------------------clap_footer-----------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div></div><div><div class=\"bl\" aria-hidden=\"false\"><button class=\"ao ln mf mg ab q fg mh mi\" aria-label=\"responses\" data-testid=\"headerResponseButton\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"me\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">5</span></p></button></div></div></div><div class=\"ab q kt ku kv kw kx ky kz la lb lc ld le lf lg lh\"><div class=\"mj k j i d\"></div><div class=\"h k\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerBookmarkButton\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84c850470dcb&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=-----84c850470dcb---------------------bookmark_footer-----------\" rel=\"noopener follow\"><button aria-controls=\"addToCatalogBookmarkButton\" aria-expanded=\"false\" aria-label=\"Add to list bookmark button\" class=\"af fg ah ai aj ak al ml an ao ap eu mm mn mo\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\" aria-label=\"Add to list bookmark button\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></button></a></span></div></div></div><div class=\"fa un cm\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"uo up uq ur us nt ch bg\"><div class=\"ab\"><div class=\"bl bg\" aria-hidden=\"false\"><div><div class=\"bl\" aria-hidden=\"false\"><button aria-label=\"Listen\" data-testid=\"audioPlayButton\" class=\"af fg ah ai aj ak al ml an ao ap eu mp mq mi mr ms mt mu mv s mw mx my mz na nb nc u nd ne nf\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z\" fill=\"currentColor\"></path></svg><div class=\"j i d\"><p class=\"be b bf z dt\">Listen</p></div></button></div></div></div></div></div></div></div></div><div class=\"bl\" aria-hidden=\"false\" aria-describedby=\"postFooterSocialMenu\" aria-labelledby=\"postFooterSocialMenu\"><div><div class=\"bl\" aria-hidden=\"false\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" data-testid=\"headerSocialShareButton\" class=\"af fg ah ai aj ak al ml an ao ap eu mp mq mi mr ms mt mu mv s mw mx my mz na nb nc u nd ne nf\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\"></path></svg><div class=\"j i d\"><p class=\"be b bf z dt\">Share</p></div></button></div></div></div></div></div></div></div></div></div><figure class=\"nj nk nl nm nn no ng nh paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"np nq ff nr bg ns\"><div class=\"ng nh ni\"><picture><source srcSet=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"/><source data-testid=\"og\" srcSet=\"https://miro.medium.com/v2/resize:fit:640/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"/><img alt=\"\" class=\"bg nt nu c\" width=\"700\" height=\"525\" loading=\"eager\" role=\"presentation\"/></picture></div></div><figcaption class=\"nv nw nx ng nh ny nz be b bf z dt\">Figure 1. Photo by <a class=\"af oa\" href=\"https://unsplash.com/@ikukevk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\" rel=\"noopener ugc nofollow\" target=\"_blank\">Kevin Ku</a> on <a class=\"af oa\" href=\"https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\" rel=\"noopener ugc nofollow\" target=\"_blank\">Unsplash</a></figcaption></figure><p id=\"d20e\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">In this tutorial, we will build a basic Transformer model from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.</p><p id=\"87ad\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">To understand Transformer models in detail kindly visit these two articles:</p><h2 id=\"352a\" class=\"ox oy gq be oz pa pb dx pc pd pe dz pf ok pg ph pi oo pj pk pl os pm pn po pp bj\"><a class=\"af oa\" href=\"https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\" rel=\"noopener\">1. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1</a></h2><h2 id=\"9340\" class=\"ox oy gq be oz pa pb dx pc pd pe dz pf ok pg ph pi oo pj pk pl os pm pn po pp bj\"><a class=\"af oa\" href=\"https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\" rel=\"noopener\">2. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2</a></h2></div></div></div><div class=\"ab ca pq pr ps pt\" role=\"separator\"><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><p id=\"166f\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">To build our Transformer model, we’ll follow these steps:</p><ol class=\"\"><li id=\"2d64\" class=\"ob oc gq od b ho oe of og hr oh oi oj py ol om on pz op oq or qa ot ou ov ow qb qc qd bj\">Import necessary libraries and modules</li><li id=\"f428\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding</li><li id=\"3f2f\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Build the Encoder and Decoder layers</li><li id=\"a3bd\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Combine Encoder and Decoder layers to create the complete Transformer model</li><li id=\"8fb2\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Prepare sample data</li><li id=\"6e50\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Train the model</li></ol></div></div></div><div class=\"ab ca pq pr ps pt\" role=\"separator\"><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><p id=\"24e9\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">Let’s start by importing the necessary libraries and modules.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"85ec\" class=\"qn oy gq qk b bf qo qp l qq qr\">import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.utils.data as data<br/>import math<br/>import copy</span></pre><p id=\"c34a\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">Now, we’ll define the basic building blocks of the Transformer model.</p><h1 id=\"7671\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Multi-Head Attention</h1><figure class=\"nj nk nl nm nn no ng nh paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"np nq ff nr bg ns\"><div class=\"ng nh rk\"><picture><source srcSet=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*--TCGWYxwASbv2ra.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*--TCGWYxwASbv2ra.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*--TCGWYxwASbv2ra.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*--TCGWYxwASbv2ra.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*--TCGWYxwASbv2ra.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*--TCGWYxwASbv2ra.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*--TCGWYxwASbv2ra.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"/><source data-testid=\"og\" srcSet=\"https://miro.medium.com/v2/resize:fit:640/0*--TCGWYxwASbv2ra.png 640w, https://miro.medium.com/v2/resize:fit:720/0*--TCGWYxwASbv2ra.png 720w, https://miro.medium.com/v2/resize:fit:750/0*--TCGWYxwASbv2ra.png 750w, https://miro.medium.com/v2/resize:fit:786/0*--TCGWYxwASbv2ra.png 786w, https://miro.medium.com/v2/resize:fit:828/0*--TCGWYxwASbv2ra.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*--TCGWYxwASbv2ra.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*--TCGWYxwASbv2ra.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"/><img alt=\"\" class=\"bg nt nu c\" width=\"700\" height=\"522\" loading=\"lazy\" role=\"presentation\"/></picture></div></div><figcaption class=\"nv nw nx ng nh ny nz be b bf z dt\">Figure 2. Multi-Head Attention (source: image created by author)</figcaption></figure><p id=\"10a9\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"9cf5\" class=\"qn oy gq qk b bf qo qp l qq qr\">class MultiHeadAttention(nn.Module):<br/>    def __init__(self, d_model, num_heads):<br/>        super(MultiHeadAttention, self).__init__()<br/>        assert d_model % num_heads == 0, &quot;d_model must be divisible by num_heads&quot;<br/>        <br/>        self.d_model = d_model<br/>        self.num_heads = num_heads<br/>        self.d_k = d_model // num_heads<br/>        <br/>        self.W_q = nn.Linear(d_model, d_model)<br/>        self.W_k = nn.Linear(d_model, d_model)<br/>        self.W_v = nn.Linear(d_model, d_model)<br/>        self.W_o = nn.Linear(d_model, d_model)<br/>        <br/>    def scaled_dot_product_attention(self, Q, K, V, mask=None):<br/>        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)<br/>        if mask is not None:<br/>            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)<br/>        attn_probs = torch.softmax(attn_scores, dim=-1)<br/>        output = torch.matmul(attn_probs, V)<br/>        return output<br/>        <br/>    def split_heads(self, x):<br/>        batch_size, seq_length, d_model = x.size()<br/>        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)<br/>        <br/>    def combine_heads(self, x):<br/>        batch_size, _, seq_length, d_k = x.size()<br/>        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)<br/>        <br/>    def forward(self, Q, K, V, mask=None):<br/>        Q = self.split_heads(self.W_q(Q))<br/>        K = self.split_heads(self.W_k(K))<br/>        V = self.split_heads(self.W_v(V))<br/>        <br/>        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)<br/>        output = self.W_o(self.combine_heads(attn_output))<br/>        return output</span></pre><p id=\"3965\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The MultiHeadAttention code initializes the module with input parameters and linear transformation layers. It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.</p><h1 id=\"bcae\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Position-wise Feed-Forward Networks</h1><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"d070\" class=\"qn oy gq qk b bf qo qp l qq qr\">class PositionWiseFeedForward(nn.Module):<br/>    def __init__(self, d_model, d_ff):<br/>        super(PositionWiseFeedForward, self).__init__()<br/>        self.fc1 = nn.Linear(d_model, d_ff)<br/>        self.fc2 = nn.Linear(d_ff, d_model)<br/>        self.relu = nn.ReLU()<br/><br/>    def forward(self, x):<br/>        return self.fc2(self.relu(self.fc1(x)))</span></pre><p id=\"5ad2\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements a position-wise feed-forward network. The class initializes with two linear transformation layers and a ReLU activation function. The forward method applies these transformations and activation function sequentially to compute the output. This process enables the model to consider the position of input elements while making predictions.</p><h1 id=\"69ba\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Positional Encoding</h1><p id=\"2f9a\" class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\">Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"9b1a\" class=\"qn oy gq qk b bf qo qp l qq qr\">class PositionalEncoding(nn.Module):<br/>    def __init__(self, d_model, max_seq_length):<br/>        super(PositionalEncoding, self).__init__()<br/>        <br/>        pe = torch.zeros(max_seq_length, d_model)<br/>        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)<br/>        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))<br/>        <br/>        pe[:, 0::2] = torch.sin(position * div_term)<br/>        pe[:, 1::2] = torch.cos(position * div_term)<br/>        <br/>        self.register_buffer(&#x27;pe&#x27;, pe.unsqueeze(0))<br/>        <br/>    def forward(self, x):<br/>        return x + self.pe[:, :x.size(1)]</span></pre><p id=\"18d5\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values. The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term. The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence.</p><p id=\"730e\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">Now, we’ll build the Encoder and Decoder layers.</p><h1 id=\"e8c3\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Encoder Layer</h1><figure class=\"nj nk nl nm nn no ng nh paragraph-image\"><div class=\"ng nh rq\"><picture><source srcSet=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*bPKV4ekQr9ZjYkWJ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*bPKV4ekQr9ZjYkWJ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*bPKV4ekQr9ZjYkWJ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*bPKV4ekQr9ZjYkWJ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*bPKV4ekQr9ZjYkWJ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*bPKV4ekQr9ZjYkWJ.png 1100w, https://miro.medium.com/v2/resize:fit:552/format:webp/0*bPKV4ekQr9ZjYkWJ.png 552w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 276px\" type=\"image/webp\"/><source data-testid=\"og\" srcSet=\"https://miro.medium.com/v2/resize:fit:640/0*bPKV4ekQr9ZjYkWJ.png 640w, https://miro.medium.com/v2/resize:fit:720/0*bPKV4ekQr9ZjYkWJ.png 720w, https://miro.medium.com/v2/resize:fit:750/0*bPKV4ekQr9ZjYkWJ.png 750w, https://miro.medium.com/v2/resize:fit:786/0*bPKV4ekQr9ZjYkWJ.png 786w, https://miro.medium.com/v2/resize:fit:828/0*bPKV4ekQr9ZjYkWJ.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*bPKV4ekQr9ZjYkWJ.png 1100w, https://miro.medium.com/v2/resize:fit:552/0*bPKV4ekQr9ZjYkWJ.png 552w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 276px\"/><img alt=\"\" class=\"bg nt nu c\" width=\"276\" height=\"473\" loading=\"lazy\" role=\"presentation\"/></picture></div><figcaption class=\"nv nw nx ng nh ny nz be b bf z dt\">Figure 3. The Encoder part of the transformer network (Source: image from the original paper)</figcaption></figure><p id=\"3062\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"51dc\" class=\"qn oy gq qk b bf qo qp l qq qr\">class EncoderLayer(nn.Module):<br/>    def __init__(self, d_model, num_heads, d_ff, dropout):<br/>        super(EncoderLayer, self).__init__()<br/>        self.self_attn = MultiHeadAttention(d_model, num_heads)<br/>        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)<br/>        self.norm1 = nn.LayerNorm(d_model)<br/>        self.norm2 = nn.LayerNorm(d_model)<br/>        self.dropout = nn.Dropout(dropout)<br/>        <br/>    def forward(self, x, mask):<br/>        attn_output = self.self_attn(x, x, x, mask)<br/>        x = self.norm1(x + self.dropout(attn_output))<br/>        ff_output = self.feed_forward(x)<br/>        x = self.norm2(x + self.dropout(ff_output))<br/>        return x</span></pre><p id=\"008b\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.</p><h1 id=\"c227\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Decoder Layer</h1><figure class=\"nj nk nl nm nn no ng nh paragraph-image\"><div class=\"ng nh rq\"><picture><source srcSet=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*SPZgT4k8GQi37H__.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*SPZgT4k8GQi37H__.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*SPZgT4k8GQi37H__.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*SPZgT4k8GQi37H__.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*SPZgT4k8GQi37H__.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*SPZgT4k8GQi37H__.png 1100w, https://miro.medium.com/v2/resize:fit:552/format:webp/0*SPZgT4k8GQi37H__.png 552w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 276px\" type=\"image/webp\"/><source data-testid=\"og\" srcSet=\"https://miro.medium.com/v2/resize:fit:640/0*SPZgT4k8GQi37H__.png 640w, https://miro.medium.com/v2/resize:fit:720/0*SPZgT4k8GQi37H__.png 720w, https://miro.medium.com/v2/resize:fit:750/0*SPZgT4k8GQi37H__.png 750w, https://miro.medium.com/v2/resize:fit:786/0*SPZgT4k8GQi37H__.png 786w, https://miro.medium.com/v2/resize:fit:828/0*SPZgT4k8GQi37H__.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*SPZgT4k8GQi37H__.png 1100w, https://miro.medium.com/v2/resize:fit:552/0*SPZgT4k8GQi37H__.png 552w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 276px\"/><img alt=\"\" class=\"bg nt nu c\" width=\"276\" height=\"749\" loading=\"lazy\" role=\"presentation\"/></picture></div><figcaption class=\"nv nw nx ng nh ny nz be b bf z dt\">Figure 4. The Decoder part of the Transformer network (Souce: Image from the original paper)</figcaption></figure><p id=\"b9dd\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"196d\" class=\"qn oy gq qk b bf qo qp l qq qr\">class DecoderLayer(nn.Module):<br/>    def __init__(self, d_model, num_heads, d_ff, dropout):<br/>        super(DecoderLayer, self).__init__()<br/>        self.self_attn = MultiHeadAttention(d_model, num_heads)<br/>        self.cross_attn = MultiHeadAttention(d_model, num_heads)<br/>        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)<br/>        self.norm1 = nn.LayerNorm(d_model)<br/>        self.norm2 = nn.LayerNorm(d_model)<br/>        self.norm3 = nn.LayerNorm(d_model)<br/>        self.dropout = nn.Dropout(dropout)<br/>        <br/>    def forward(self, x, enc_output, src_mask, tgt_mask):<br/>        attn_output = self.self_attn(x, x, x, tgt_mask)<br/>        x = self.norm1(x + self.dropout(attn_output))<br/>        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)<br/>        x = self.norm2(x + self.dropout(attn_output))<br/>        ff_output = self.feed_forward(x)<br/>        x = self.norm3(x + self.dropout(ff_output))<br/>        return x</span></pre><p id=\"2a69\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.</p><p id=\"6196\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The forward method computes the decoder layer output by performing the following steps:</p><ol class=\"\"><li id=\"1f3d\" class=\"ob oc gq od b ho oe of og hr oh oi oj py ol om on pz op oq or qa ot ou ov ow qb qc qd bj\">Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.</li><li id=\"877d\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.</li><li id=\"f616\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.</li><li id=\"0e37\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Return the processed tensor.</li></ol><p id=\"0beb\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">These operations enable the decoder to generate target sequences based on the input and the encoder output.</p><p id=\"8064\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">Now, let’s combine the Encoder and Decoder layers to create the complete Transformer model.</p><h1 id=\"b207\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Transformer Model</h1><figure class=\"nj nk nl nm nn no ng nh paragraph-image\"><div class=\"ng nh rr\"><picture><source srcSet=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*ljYs7oOlKC71SzSr.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*ljYs7oOlKC71SzSr.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*ljYs7oOlKC71SzSr.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*ljYs7oOlKC71SzSr.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*ljYs7oOlKC71SzSr.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*ljYs7oOlKC71SzSr.png 1100w, https://miro.medium.com/v2/resize:fit:1090/format:webp/0*ljYs7oOlKC71SzSr.png 1090w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 545px\" type=\"image/webp\"/><source data-testid=\"og\" srcSet=\"https://miro.medium.com/v2/resize:fit:640/0*ljYs7oOlKC71SzSr.png 640w, https://miro.medium.com/v2/resize:fit:720/0*ljYs7oOlKC71SzSr.png 720w, https://miro.medium.com/v2/resize:fit:750/0*ljYs7oOlKC71SzSr.png 750w, https://miro.medium.com/v2/resize:fit:786/0*ljYs7oOlKC71SzSr.png 786w, https://miro.medium.com/v2/resize:fit:828/0*ljYs7oOlKC71SzSr.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*ljYs7oOlKC71SzSr.png 1100w, https://miro.medium.com/v2/resize:fit:1090/0*ljYs7oOlKC71SzSr.png 1090w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 545px\"/><img alt=\"\" class=\"bg nt nu c\" width=\"545\" height=\"739\" loading=\"lazy\" role=\"presentation\"/></picture></div><figcaption class=\"nv nw nx ng nh ny nz be b bf z dt\">Figure 5. The Transformer Network (Source: Image from the original paper)</figcaption></figure><p id=\"1a77\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">Merging it all together:</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"3bdd\" class=\"qn oy gq qk b bf qo qp l qq qr\">class Transformer(nn.Module):<br/>    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):<br/>        super(Transformer, self).__init__()<br/>        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)<br/>        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)<br/>        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)<br/><br/>        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])<br/>        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])<br/><br/>        self.fc = nn.Linear(d_model, tgt_vocab_size)<br/>        self.dropout = nn.Dropout(dropout)<br/><br/>    def generate_mask(self, src, tgt):<br/>        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)<br/>        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)<br/>        seq_length = tgt.size(1)<br/>        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()<br/>        tgt_mask = tgt_mask &amp; nopeak_mask<br/>        return src_mask, tgt_mask<br/><br/>    def forward(self, src, tgt):<br/>        src_mask, tgt_mask = self.generate_mask(src, tgt)<br/>        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))<br/>        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))<br/><br/>        enc_output = src_embedded<br/>        for enc_layer in self.encoder_layers:<br/>            enc_output = enc_layer(enc_output, src_mask)<br/><br/>        dec_output = tgt_embedded<br/>        for dec_layer in self.decoder_layers:<br/>            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)<br/><br/>        output = self.fc(dec_output)<br/>        return output</span></pre><p id=\"117f\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The Transformer class combines the previously defined modules to create a complete Transformer model. During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.</p><p id=\"8fb5\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer model’s output through the following steps:</p><ol class=\"\"><li id=\"bd1e\" class=\"ob oc gq od b ho oe of og hr oh oi oj py ol om on pz op oq or qa ot ou ov ow qb qc qd bj\">Generate source and target masks using the generate_mask method.</li><li id=\"dd66\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Compute source and target embeddings, and apply positional encoding and dropout.</li><li id=\"4586\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Process the source sequence through encoder layers, updating the enc_output tensor.</li><li id=\"b6b3\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.</li><li id=\"bf3e\" class=\"ob oc gq od b ho qe of og hr qf oi oj py qg om on pz qh oq or qa qi ou ov ow qb qc qd bj\">Apply the linear projection layer to the decoder output, obtaining output logits.</li></ol><p id=\"67a9\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.</p></div></div></div><div class=\"ab ca pq pr ps pt\" role=\"separator\"><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><h1 id=\"66ad\" class=\"qs oy gq be oz qt rs hq pc qv rt ht pf qx ru qz ra rb rv rd re rf rw rh ri rj bj\">Preparing Sample Data</h1><p id=\"4429\" class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\">In this example, we will create a toy dataset for demonstration purposes. In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"ac94\" class=\"qn oy gq qk b bf qo qp l qq qr\">src_vocab_size = 5000<br/>tgt_vocab_size = 5000<br/>d_model = 512<br/>num_heads = 8<br/>num_layers = 6<br/>d_ff = 2048<br/>max_seq_length = 100<br/>dropout = 0.1<br/><br/>transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)<br/><br/># Generate random sample data<br/>src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)<br/>tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)</span></pre><h1 id=\"4483\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Training the Model</h1><p id=\"4509\" class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\">Now we’ll train the model using the sample data. In practice, you would use a larger dataset and split it into training and validation sets.</p><pre class=\"nj nk nl nm nn qj qk ql bo qm ba bj\"><span id=\"6107\" class=\"qn oy gq qk b bf qo qp l qq qr\">criterion = nn.CrossEntropyLoss(ignore_index=0)<br/>optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)<br/><br/>transformer.train()<br/><br/>for epoch in range(100):<br/>    optimizer.zero_grad()<br/>    output = transformer(src_data, tgt_data[:, :-1])<br/>    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))<br/>    loss.backward()<br/>    optimizer.step()<br/>    print(f&quot;Epoch: {epoch+1}, Loss: {loss.item()}&quot;)</span></pre></div></div></div><div class=\"ab ca pq pr ps pt\" role=\"separator\"><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><p id=\"c4f7\" class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\">We can use this way to build a simple Transformer from scratch in Pytorch. All Large Language Models use these Transformer encoder or decoder blocks for training. Hence understanding the network that started it all is extremely important. Hope this article helps all looking to deep dive into LLM’s.</p></div></div></div><div class=\"ab ca pq pr ps pt\" role=\"separator\"><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw px\"></span><span class=\"pu bx bl pv pw\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><h1 id=\"004c\" class=\"qs oy gq be oz qt rs hq pc qv rt ht pf qx ru qz ra rb rv rd re rf rw rh ri rj bj\">References</h1><h1 id=\"e52a\" class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\">Attention is all you need</h1><p id=\"8667\" class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\"><a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/0\" rel=\"noopener ugc nofollow\" target=\"_blank\">A. Vaswani</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1\" rel=\"noopener ugc nofollow\" target=\"_blank\">N. Shazeer</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/2\" rel=\"noopener ugc nofollow\" target=\"_blank\">N. Parmar</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/3\" rel=\"noopener ugc nofollow\" target=\"_blank\">J. Uszkoreit</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/4\" rel=\"noopener ugc nofollow\" target=\"_blank\">L. Jones</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/5\" rel=\"noopener ugc nofollow\" target=\"_blank\">A. Gomez</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/6\" rel=\"noopener ugc nofollow\" target=\"_blank\">{. Kaiser</a>, and <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/7\" rel=\"noopener ugc nofollow\" target=\"_blank\">I. Polosukhin</a>. <em class=\"rx\">Advances in Neural Information Processing Systems , page 5998–6008. </em>(<em class=\"rx\">2017</em>)</p></div></div></div></div></section></div></div></article><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"></div></div></div><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"ry rz ab jo\"><div class=\"sa ab\"><a class=\"sb ax am ao\" href=\"https://medium.com/tag/towards-data-science?source=post_page-----84c850470dcb---------------towards_data_science-----------------\" rel=\"noopener follow\"><div class=\"sc ff cw sd ga se sf be b bf z bj sg\">Towards Data Science</div></a></div><div class=\"sa ab\"><a class=\"sb ax am ao\" href=\"https://medium.com/tag/deep-learning?source=post_page-----84c850470dcb---------------deep_learning-----------------\" rel=\"noopener follow\"><div class=\"sc ff cw sd ga se sf be b bf z bj sg\">Deep Learning</div></a></div><div class=\"sa ab\"><a class=\"sb ax am ao\" href=\"https://medium.com/tag/machine-learning?source=post_page-----84c850470dcb---------------machine_learning-----------------\" rel=\"noopener follow\"><div class=\"sc ff cw sd ga se sf be b bf z bj sg\">Machine Learning</div></a></div><div class=\"sa ab\"><a class=\"sb ax am ao\" href=\"https://medium.com/tag/transformers?source=post_page-----84c850470dcb---------------transformers-----------------\" rel=\"noopener follow\"><div class=\"sc ff cw sd ga se sf be b bf z bj sg\">Transformers</div></a></div><div class=\"sa ab\"><a class=\"sb ax am ao\" href=\"https://medium.com/tag/chatgpt?source=post_page-----84c850470dcb---------------chatgpt-----------------\" rel=\"noopener follow\"><div class=\"sc ff cw sd ga se sf be b bf z bj sg\">ChatGPT</div></a></div></div></div></div><div class=\"l\"></div><footer class=\"sh si sj sk sl sm sn so sp ab q sq iy c\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"ab co sr\"><div class=\"ab q lj\"><div class=\"ss l\"><span class=\"l st su sv e d\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----84c850470dcb---------------------clap_footer-----------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div></span><span class=\"l h g f sw sx\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84c850470dcb&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----84c850470dcb---------------------clap_footer-----------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div></span></div><div class=\"bp ab\"><div><div class=\"bl\" aria-hidden=\"false\"><button class=\"ao ln mf mg ab q fg mh mi\" aria-label=\"responses\" data-testid=\"footerResponseButton\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"me\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b bf z dt\"><span class=\"pw-responses-count md me\">5</span></p></button></div></div></div></div><div class=\"ab q\"><div class=\"px l jl\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerBookmarkButton\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84c850470dcb&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=--------------------------bookmark_footer-----------\" rel=\"noopener follow\"><button aria-controls=\"addToCatalogBookmarkButton\" aria-expanded=\"false\" aria-label=\"Add to list bookmark button\" class=\"af fg ah ai aj ak al ml an ao ap eu mm mn mo\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\" aria-label=\"Add to list bookmark button\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></button></a></span></div></div></div><div class=\"px l jl\"><div class=\"bl\" aria-hidden=\"false\" aria-describedby=\"postFooterSocialMenu\" aria-labelledby=\"postFooterSocialMenu\"><div><div class=\"bl\" aria-hidden=\"false\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" data-testid=\"footerSocialShareButton\" class=\"af fg ah ai aj ak al ml an ao ap eu mp mq mi mr\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class=\"sy sz ta tb tc l bw\"><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"ck ab td co\"><div class=\"ab in\"><a href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"l te tf bx tg ir\"><div class=\"l ff\"><img alt=\"Arjun Sarkar\" class=\"l fa bx th ti cw\" src=\"https://miro.medium.com/v2/resize:fill:144:144/0*zwki88OLZVcjhvge\" width=\"72\" height=\"72\" loading=\"lazy\"/><div class=\"is bx l th ti fo n it iu\"></div></div></div></a><a href=\"https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"tj ab ff\"><div><div class=\"bl\" aria-hidden=\"false\"><div class=\"l tk tl bx tg iy\"><div class=\"l ff\"><img alt=\"Towards Data Science\" class=\"l fa bx by bz cw\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg\" width=\"32\" height=\"32\" loading=\"lazy\"/><div class=\"is bx l by bz fo n it iu\"></div></div></div></div></div></div></a></div><div class=\"j i d\"><div class=\"ab\"><span><a class=\"be b bf z el sc em eo ep eq er es et eu ev ew ex tm ey ez fa bl fb\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=post_page-fa31366b2eda----84c850470dcb---------------------follow_profile-----------\" rel=\"noopener follow\">Follow</a></span><div class=\"ds l\"><div><div><div class=\"bl\" aria-hidden=\"false\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F25711c8db9ff&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;newsletterV3=fa31366b2eda&amp;newsletterV3Id=25711c8db9ff&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----84c850470dcb---------------------subscribe_user-----------\" rel=\"noopener follow\"><button class=\"be b bf z to am tp tq tr ts tt tu tv tw et eu ev ew ex ey ez fa bl fb\" aria-label=\"Subscribe\"><svg width=\"38\" height=\"38\" viewBox=\"0 0 38 38\" fill=\"none\" class=\"tn tl tk\"><rect x=\"26.25\" y=\"9.25\" width=\"0.5\" height=\"6.5\" rx=\"0.25\"></rect><rect x=\"29.75\" y=\"12.25\" width=\"0.5\" height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"ab cm co\"><div class=\"l\"><div class=\"ab q\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab q\" href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><h2 class=\"pw-author-name be tx ty tz ua bj\"><span class=\"gj\">Written by <!-- -->Arjun Sarkar</span></h2></a></div><div class=\"sa ab\"><div class=\"l jl\"><span class=\"pw-follower-count be b bf z bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je\" href=\"https://arjun-sarkar786.medium.com/followers?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\">2.6K Followers</a></span></div><div class=\"be b bf z jt ju jv ab jx jy jz ka dt jr\"><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span class=\"l jl\">Writer for </span><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b bf z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"ub l\"><p class=\"be b bf z bj\">Ph.D. student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany. LinkedIn-<a class=\"af ag ah ai aj ak al am an ao ap aq ar oa gk\" href=\"https://www.linkedin.com/in/arjun-sarkar-9a051777/\" rel=\"noopener  ugc nofollow\">https://www.linkedin.com/in/arjun-sarkar-9a051777/</a></p></div></div><div class=\"h k\"><div class=\"ab\"><span><a class=\"be b bf z el sc em eo ep eq er es et eu ev ew ex tm ey ez fa bl fb\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=post_page-fa31366b2eda----84c850470dcb---------------------follow_profile-----------\" rel=\"noopener follow\">Follow</a></span><div class=\"ds l\"><div><div><div class=\"bl\" aria-hidden=\"false\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F25711c8db9ff&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;newsletterV3=fa31366b2eda&amp;newsletterV3Id=25711c8db9ff&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----84c850470dcb---------------------subscribe_user-----------\" rel=\"noopener follow\"><button class=\"be b bf z to am tp tq tr ts tt tu tv tw et eu ev ew ex ey ez fa bl fb\" aria-label=\"Subscribe\"><svg width=\"38\" height=\"38\" viewBox=\"0 0 38 38\" fill=\"none\" class=\"tn tl tk\"><rect x=\"26.25\" y=\"9.25\" width=\"0.5\" height=\"6.5\" rx=\"0.25\"></rect><rect x=\"29.75\" y=\"12.25\" width=\"0.5\" height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"uc bg ud ue uf ug uh ui\"></div></div></div><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"ut uu l\"><h2 class=\"be tx jc z gp bj\">More from <!-- -->Arjun Sarkar<!-- --> and Towards Data Science</h2></div><div class=\"uv ab lj jo uw ux uy uz va vb vc vd ve vf vg vh vi vj vk\"><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 1\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div class=\"xa xb xc xd xe\"><img alt=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 1\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*Rv_pntt-N2WL7LMbIptHxQ.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://arjun-sarkar786.medium.com/?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Arjun Sarkar\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/0*zwki88OLZVcjhvge\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://arjun-sarkar786.medium.com/?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Arjun Sarkar</p></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div title=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 1\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 1</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Attention, Self-Attention, Multi-head Attention, and Transformers</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021?source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>12 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Feb 15, 2022</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F552f0b41d021&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----552f0b41d021----0-----------------clap_footer----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----84c850470dcb----0---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">15</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F552f0b41d021&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021&amp;source=-----84c850470dcb----0-----------------bookmark_preview----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"130 ML Tricks And Resources Curated Carefully From 3 Years (Plus Free eBook)\" rel=\"noopener follow\" href=\"/130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div class=\"xa xb xc xd xe\"><img alt=\"130 ML Tricks And Resources Curated Carefully From 3 Years (Plus Free eBook)\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*LliAcJQ4of2KId2dpkpvDg.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://ibexorigin.medium.com/?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Bex T.\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*dEWNYKjVLledNST1EBuL_A.png\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://ibexorigin.medium.com/?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Bex T.</p></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div title=\"130 ML Tricks And Resources Curated Carefully From 3 Years (Plus Free eBook)\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">130 ML Tricks And Resources Curated Carefully From 3 Years (Plus Free eBook)</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Each one is worth your time</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef?source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><span class=\"be b du z dt\"><div class=\"ab q\"><div class=\"sp ab\"><div class=\"bl\" aria-hidden=\"false\"><button class=\"l ax ao am\" aria-label=\"Member-only story\"><div class=\"\"><div><div class=\"bl\" aria-hidden=\"false\"><svg width=\"16\" height=\"16\" viewBox=\"0 0 20 20\" fill=\"none\"><path d=\"M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z\" fill=\"#FFC017\"></path></svg></div></div></div></button></div></div><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>48 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Aug 1</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7832ca4a37ef&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef&amp;user=Bex+T.&amp;userId=39db050c2ac2&amp;source=-----7832ca4a37ef----1-----------------clap_footer----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" rel=\"noopener follow\" href=\"/130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----84c850470dcb----1---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">10</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7832ca4a37ef&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef&amp;source=-----84c850470dcb----1-----------------bookmark_preview----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"Fine-Tune Your Own Llama 2 Model in a Colab Notebook\" rel=\"noopener follow\" href=\"/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div class=\"xa xb xc xd xe\"><img alt=\"Fine-Tune Your Own Llama 2 Model in a Colab Notebook\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*jsksHuIVtuXDh-aLpwZDbw.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@mlabonne?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Maxime Labonne\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*4DZEZLsIpQAI7efBPvYUxw.jpeg\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://medium.com/@mlabonne?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Maxime Labonne</p><div class=\"zg zh l\"><div class=\"ab zi\"><div class=\"ab\"><svg width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"><path d=\"M15.16 8c0 .65-.46 1.14-.86 1.57-.23.25-.47.5-.56.72-.1.22-.09.55-.1.88 0 .6-.01 1.3-.48 1.78-.48.48-1.16.5-1.75.5-.32 0-.65.01-.86.1-.2.07-.46.33-.7.57-.42.41-.9.88-1.54.88s-1.12-.47-1.54-.88a2.87 2.87 0 0 0-.7-.58c-.22-.09-.54-.08-.87-.09-.59 0-1.27-.02-1.74-.5s-.48-1.17-.49-1.78c0-.33-.01-.67-.1-.88-.07-.2-.32-.47-.55-.71-.4-.44-.87-.93-.87-1.58s.46-1.14.87-1.58c.23-.24.47-.5.56-.71.09-.22.08-.55.09-.88 0-.6.02-1.3.49-1.78s1.15-.5 1.74-.5c.33 0 .66-.01.86-.1.2-.08.47-.33.7-.57.43-.41.91-.88 1.55-.88.63 0 1.12.47 1.54.88.24.24.49.48.7.58.22.09.54.08.86.09.6 0 1.27.02 1.75.5.47.48.48 1.17.49 1.78 0 .33 0 .67.09.88.08.2.33.47.56.71.4.44.86.93.86 1.58z\" fill=\"#437AFF\"></path><path d=\"M7.33 10.5c.2 0 .38.08.52.22.13.14.21.33.21.53 0 .07.03.13.07.18a.24.24 0 0 0 .35 0 .25.25 0 0 0 .07-.18c0-.2.08-.39.22-.53a.73.73 0 0 1 .52-.22h1.96c.13 0 .25-.05.34-.15a.5.5 0 0 0 .15-.35V6a.5.5 0 0 0-.15-.35.48.48 0 0 0-.34-.15H9.78c-.33 0-.64.13-.87.37-.23.23-.36.55-.36.88v2.5c0 .07-.02.13-.07.18a.24.24 0 0 1-.35 0 .25.25 0 0 1-.07-.18v-2.5c0-.33-.13-.65-.36-.88a1.21 1.21 0 0 0-.86-.37H5.37a.48.48 0 0 0-.35.15.5.5 0 0 0-.14.35v4c0 .13.05.26.14.35.1.1.22.15.35.15h1.96z\" fill=\"#fff\"></path></svg></div></div></div></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div title=\"\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">Fine-Tune Your Own Llama 2 Model in a Colab Notebook</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">A practical introduction to LLM fine-tuning</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><span class=\"be b du z dt\"><div class=\"ab q\"><div class=\"sp ab\"><div class=\"bl\" aria-hidden=\"false\"><button class=\"l ax ao am\" aria-label=\"Member-only story\"><div class=\"\"><div><div class=\"bl\" aria-hidden=\"false\"><svg width=\"16\" height=\"16\" viewBox=\"0 0 20 20\" fill=\"none\"><path d=\"M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z\" fill=\"#FFC017\"></path></svg></div></div></div></button></div></div><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>12 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Jul 25</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf9823a04a32&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&amp;user=Maxime+Labonne&amp;userId=dc89da634938&amp;source=-----df9823a04a32----2-----------------clap_footer----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" rel=\"noopener follow\" href=\"/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----84c850470dcb----2---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">33</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf9823a04a32&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32&amp;source=-----84c850470dcb----2-----------------bookmark_preview----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 2\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div class=\"xa xb xc xd xe\"><img alt=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 2\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/0*Wvg_pNDViACfg-IK.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://arjun-sarkar786.medium.com/?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Arjun Sarkar\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/0*zwki88OLZVcjhvge\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://arjun-sarkar786.medium.com/?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Arjun Sarkar</p></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><div title=\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 2\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 2</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Attention, Self-Attention, Multi-head Attention, Masked Multi-head Attention, Transformers, BERT, and GPT</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada?source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>9 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Sep 13, 2022</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf2403804ada&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=-----bf2403804ada----3-----------------clap_footer----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" rel=\"noopener follow\" href=\"/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----84c850470dcb----3---------------------5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">5</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf2403804ada&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada&amp;source=-----84c850470dcb----3-----------------bookmark_preview----5085f58c_28f5_40cc_8720_a2fd4ff5c91e-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class=\"uc bg ud dj dk zj zk zl\"></div><div class=\"ab jm jn zm zn zo\"><a class=\"be b bf z bj sc zp zq zr zs mh zt es et eu zu zv zw ex zx zy zz aba abb ey ez fa bl fb\" href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"l nw\">See all from <!-- -->Arjun Sarkar</div></a><div class=\"abc abd abe abf abg abh abi abj abk mb l\"><a class=\"be b bf z bj sc zp zq zr zs mh zt es et eu zu zv zw ex zx zy zz aba abb ey ez fa bl fb\" href=\"https://towardsdatascience.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"l nw\">See all from <!-- -->Towards Data Science</div></a></div></div></div></div><div class=\"uc bg ud abl abm abn abo abp\"></div><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"abq abr l\"><h2 class=\"be tx qt hq pc qv ht pf qx qz ra rb rd re rf rh ri bj\">Recommended from Medium</h2><div class=\"nj nk nl nm nn l\"><div class=\"uv ab lj jo uw ux uy uz va vb vc vd ve vf vg vh vi vj vk\"><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"All You Need to Know to Build Your First LLM App\" rel=\"noopener follow\" href=\"/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\"><div class=\"xa xb xc xd xe\"><img alt=\"All You Need to Know to Build Your First LLM App\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*njagJOgiT-VTJjQ18bugcw.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://dmnkplzr.medium.com/?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Dominik Polzer\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*KqpicOFO7jh7FXGjoJ2Bcg.jpeg\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://dmnkplzr.medium.com/?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Dominik Polzer</p></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://towardsdatascience.com/?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\"><div title=\"\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">All You Need to Know to Build Your First LLM App</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">A step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" rel=\"noopener follow\" href=\"/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\"><span class=\"be b du z dt\"><div class=\"ab q\"><div class=\"sp ab\"><div class=\"bl\" aria-hidden=\"false\"><button class=\"l ax ao am\" aria-label=\"Member-only story\"><div class=\"\"><div><div class=\"bl\" aria-hidden=\"false\"><svg width=\"16\" height=\"16\" viewBox=\"0 0 20 20\" fill=\"none\"><path d=\"M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z\" fill=\"#FFC017\"></path></svg></div></div></div></button></div></div><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>26 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Jun 22</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb982c78ffac&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac&amp;user=Dominik+Polzer&amp;userId=3ab8d3143e32&amp;source=-----eb982c78ffac----0-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" rel=\"noopener follow\" href=\"/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">42</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb982c78ffac&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac&amp;source=-----84c850470dcb----0-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"Vision Transformers vs. Convolutional Neural Networks\" href=\"https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"xa xb xc xd xe\"><img alt=\"Vision Transformers vs. Convolutional Neural Networks\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/0*brmcPLvJpiQWjZpY\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@faheemrustamy?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Fahim Rustamy, PhD\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/0*qmd1yyz_btTHNCDI.jpg\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://medium.com/@faheemrustamy?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Fahim Rustamy, PhD</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div title=\"\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">Vision Transformers vs. Convolutional Neural Networks</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">This blog post is inspired by the paper titled AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE from google’s…</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>7 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Jun 4</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5fe8f9e18efc&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40faheemrustamy%2Fvision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc&amp;user=Fahim+Rustamy%2C+PhD&amp;userId=931fc8afcda1&amp;source=-----5fe8f9e18efc----1-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" href=\"https://medium.com/@faheemrustamy/vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">6</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fe8f9e18efc&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40faheemrustamy%2Fvision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc&amp;source=-----84c850470dcb----1-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></article></div></div></div></div><div class=\"uc bg ud abs\"></div><h2 class=\"be tx jc z gp bj\">Lists</h2><div class=\"pq l\"><div class=\"cm ab lj jo uw ux uy uz va vb vc vd ve vf vg vh vi vj vk\"><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm\" href=\"https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=read_next_recirc-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"abz aca jt ab jl ff\"><div class=\"ff xf abu bw abv\"><div class=\"xf ip jt l\"><img alt=\"Image by vectorjuice on FreePik\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/0*3OsUtsnlTx9Svm4c.jpg\" width=\"48\" height=\"48\" loading=\"lazy\"/></div></div><div class=\"ff xf abu bw abw abx\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*IPZF1hcDWwpPqOz2vL7NxQ.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf bw iy aby\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*0fHUKyg3xtpNWpop35PR4g.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div></div><div class=\"aw l\"><h2 class=\"be tx jc z jt ym jv jw yn jy ka gp bj\">The New Chatbots: ChatGPT, Bard, and Beyond</h2><div class=\"be b du z dt ab abt\">13 stories<span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span>92<!-- --> <!-- -->saves</div></div></a></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm\" href=\"https://medium.com/@ben.putney/list/predictive-modeling-w-python-e3668ea008e1?source=read_next_recirc-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"abz aca jt ab jl ff\"><div class=\"ff xf abu bw abv\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/0*r4yjMpEmqzHCUvWC.jpg\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf abu bw abw abx\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*bv2KUVNLi2sFNjBTdoBmWw.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf bw iy aby\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/0*zsngbTOmFCy6sUCx.jpeg\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div></div><div class=\"aw l\"><h2 class=\"be tx jc z jt ym jv jw yn jy ka gp bj\">Predictive Modeling w/ Python</h2><div class=\"be b du z dt ab abt\">20 stories<span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span>307<!-- --> <!-- -->saves</div></div></a></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm\" href=\"https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"abz aca jt ab jl ff\"><div class=\"ff xf abu bw abv\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*0yqyeKfH-r_6PsgvS4_pgA.jpeg\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf abu bw abw abx\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*wpbt4dgRYD9O9tEUnPdtPQ.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf bw iy aby\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*-M7fV5SGuGauCrlQHElzxw.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div></div><div class=\"aw l\"><h2 class=\"be tx jc z jt ym jv jw yn jy ka gp bj\">Natural Language Processing</h2><div class=\"be b du z dt ab abt\">544 stories<span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span>163<!-- --> <!-- -->saves</div></div></a></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm\" href=\"https://medium.com/@MediumForTeams/list/what-is-chatgpt-7a5756752f49?source=read_next_recirc-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"abz aca jt ab jl ff\"><div class=\"ff xf abu bw abv\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/0*_eYHSSUS0abUxmDU\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf abu bw abw abx\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/1*wXgeNtz5OJ5O9T3c3mQRRw.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div><div class=\"ff xf bw iy aby\"><div class=\"xf ip jt l\"><img alt=\"\" class=\"\" src=\"https://miro.medium.com/v2/resize:fill:96:96/0*tIipcmrInD5UMpQI.png\" width=\"48\" height=\"48\" loading=\"lazy\" role=\"presentation\"/></div></div></div><div class=\"aw l\"><h2 class=\"be tx jc z jt ym jv jw yn jy ka gp bj\">What is ChatGPT?</h2><div class=\"be b du z dt ab abt\">9 stories<span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span>160<!-- --> <!-- -->saves</div></div></a></div></div></div><div class=\"uc bg ud abd dj abf dk acb acc acd ace acf acg\"></div><div class=\"uv ab lj jo uw ux uy uz va vb vc vd ve vf vg vh vi vj vk\"><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"Step-by-Step Illustrated Explanations of Transformer\" href=\"https://medium.com/@yulemoon/detailed-explanations-of-transformer-step-by-step-dc32d90b3a98?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"xa xb xc xd xe\"><img alt=\"Step-by-Step Illustrated Explanations of Transformer\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*07Jm441iYuRuuNlB297C3w.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@yulemoon?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"Yule Wang, PhD\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*9VFHaHkwy2zxEocPrlMOhQ.png\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://medium.com/@yulemoon?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Yule Wang, PhD</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@yulemoon/detailed-explanations-of-transformer-step-by-step-dc32d90b3a98?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div title=\"\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">Step-by-Step Illustrated Explanations of Transformer</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">with detailed explained Attention Mechanism</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@yulemoon/detailed-explanations-of-transformer-step-by-step-dc32d90b3a98?source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>8 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Feb 27</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fdc32d90b3a98&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40yulemoon%2Fdetailed-explanations-of-transformer-step-by-step-dc32d90b3a98&amp;user=Yule+Wang%2C+PhD&amp;userId=a497412b5611&amp;source=-----dc32d90b3a98----0-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" href=\"https://medium.com/@yulemoon/detailed-explanations-of-transformer-step-by-step-dc32d90b3a98?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----0---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">3</span></p></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc32d90b3a98&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40yulemoon%2Fdetailed-explanations-of-transformer-step-by-step-dc32d90b3a98&amp;source=-----84c850470dcb----0-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example\" href=\"https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"xa xb xc xd xe\"><img alt=\"In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/0*wMAccADhdvhe9yf8\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@funcry?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"FunCry\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*dmbNkD5D-u45r44go_cf0g.png\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://medium.com/@funcry?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">FunCry</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div title=\"In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Introduction</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e?source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>8 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Mar 1</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7743804e610e&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40funcry%2Fin-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e&amp;user=FunCry&amp;userId=4a992408a97e&amp;source=-----7743804e610e----1-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" href=\"https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----1---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7743804e610e&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40funcry%2Fin-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e&amp;source=-----84c850470dcb----1-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"Best Practices for Deploying Large Language Models (LLMs) in Production\" href=\"https://medium.com/@_aigeek/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a?source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"xa xb xc xd xe\"><img alt=\"Best Practices for Deploying Large Language Models (LLMs) in Production\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*ZwtGweo0PSFPwUVpDH-p7w.jpeg\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@_aigeek?source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"ai geek (wishesh)\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*IpHbl7Dg0Q35MFdCW4bK4Q.jpeg\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://medium.com/@_aigeek?source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">ai geek (wishesh)</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@_aigeek/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a?source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div title=\"Best Practices for Deploying Large Language Models (LLMs) in Production\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">Best Practices for Deploying Large Language Models (LLMs) in Production</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Large Language Models (LLMs) have revolutionized the field of natural language processing and understanding, enabling a wide range of AI…</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/@_aigeek/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a?source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>10 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Jun 26</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ffdc5bf240d6a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40_aigeek%2Fbest-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a&amp;user=ai+geek+%28wishesh%29&amp;userId=ffffcb9e269d&amp;source=-----fdc5bf240d6a----2-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" href=\"https://medium.com/@_aigeek/best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----2---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdc5bf240d6a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40_aigeek%2Fbest-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a&amp;source=-----84c850470dcb----2-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div><div class=\"j i d\"><div class=\"uc bg ud pq\"></div></div></div></div></div></div></div></article></div></div><div class=\"vl vm vn vo vp vq vr vs vt vu vv vw vx vy vz wa wb wc wd we wf\"><div class=\"wg wh wi wj wk dv l\"><article class=\"dv\"><div class=\"dv sp l\"><div class=\"bg dv\"><div class=\"dv l\"><div class=\"dv wl wm wn wo wp wq wr ws wt wu wv ww wx\"><div class=\"wy\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" aria-label=\"All You Need to Know to Build Your First LLM App\" href=\"https://pub.aimind.so/all-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"xa xb xc xd xe\"><img alt=\"All You Need to Know to Build Your First LLM App\" class=\"bg xf xg xh xi bw\" src=\"https://miro.medium.com/v2/resize:fit:1358/1*GxLaWT8zNdNzY7KxV8rlCA.png\" loading=\"lazy\"/></div></a></div><div class=\"wz ab ca cn\"><div class=\"xj xk xl xm xn ab\"><div class=\"sb l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://demogpt.medium.com/?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"l ff\"><img alt=\"DemoGPT\" class=\"l fa bx xo xp cw\" src=\"https://miro.medium.com/v2/resize:fill:40:40/1*qC8NCPcO6Q_MIByXRE9-bA.png\" width=\"20\" height=\"20\" loading=\"lazy\"/><div class=\"fn bx l xo xp fo n ax iu\"></div></div></a></div></div></div><div class=\"xq l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://demogpt.medium.com/?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">DemoGPT</p></a></div></div></div><div class=\"xq l\"><p class=\"be b du z dt\">in</p></div><div class=\"l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je ab q\" href=\"https://pub.aimind.so/?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><p class=\"be b du z jt ju jv jw jx jy jz ka bj\">AI Mind</p></a></div></div></div></div><div class=\"xr xs xt xu xv xw xx xy xz ya l gj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://pub.aimind.so/all-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div title=\"\"><h2 class=\"be gr qt hq yb yc pc qv ht yd ye pf ok ph yf yg pi oo pk yh yi pl os pn yj yk po jt jv jw jy ka bj\">All You Need to Know to Build Your First LLM App</h2></div><div class=\"yl l\"><h3 class=\"be b jc z jt ym jv jw yn jy ka dt\">Are you intrigued by the potential of Language Learning Models (LLMs) and eager to create your first LLM-based application? Or maybe…</h3></div></a></div><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://pub.aimind.so/all-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa?source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><span class=\"be b du z dt\"><div class=\"ab q\"><span>2 min read</span><span class=\"jf l\" aria-hidden=\"true\"><span class=\"be b bf z dt\">·</span></span><span>Jul 1</span></div></span></a><div class=\"yo yp yq yr ys l\"><div class=\"ab co\"><div class=\"am yt yu yv yw yx yy yz za zb zc ab q\"><div class=\"ab q lj\"><div class=\"pw-multi-vote-icon ff js lk ll lm\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fai-mind-labs%2F1ff75bc7c9aa&amp;operation=register&amp;redirect=https%3A%2F%2Fpub.aimind.so%2Fall-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa&amp;user=DemoGPT&amp;userId=492215e5ab91&amp;source=-----1ff75bc7c9aa----3-----------------clap_footer----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><div class=\"ln ao lo lp lq lr am ls lt lu lm\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" aria-label=\"clap\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\"></path></svg></div></a></span></div><div class=\"pw-multi-vote-count l lv lw lx ly lz ma mb\"><p class=\"be b du z dt\"><span class=\"mc\">--</span></p></div></div><div class=\"zd l\"><div><div class=\"bl\" aria-hidden=\"false\"><a class=\"af fg ah ln aj ak al mg an ao ap aq ar as at mf ab q mh mi\" aria-label=\"responses\" href=\"https://pub.aimind.so/all-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----84c850470dcb----3---------------------a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" class=\"\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg></a></div></div></div></div><div class=\"ab q ze zf\"><div><div class=\"bl\" aria-hidden=\"false\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ff75bc7c9aa&amp;operation=register&amp;redirect=https%3A%2F%2Fpub.aimind.so%2Fall-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa&amp;source=-----84c850470dcb----3-----------------bookmark_preview----a958a20f_1290_4998_bbe1_5efb12664f74-------\" rel=\"noopener follow\"><svg width=\"25\" height=\"25\" viewBox=\"0 0 25 25\" fill=\"none\" class=\"mk\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"#292929\"></path></svg></a></span></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class=\"uc bg ud dj dk zj zk zl\"></div><a class=\"be b bf z bj sc zp zq zr zs mh zt es et eu zu zv zw ex zx zy zz aba abb ey ez fa bl fb\" href=\"https://medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><div class=\"l nw\">See more recommendations</div></a></div></div></div><div class=\"h k j\"><div class=\"uc bg ud uj\"></div><div class=\"ab ca\"><div class=\"ch bg fv fw fx fy\"><div class=\"uk ab lj jo\"><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://help.medium.com/hc/en-us?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Help</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.statuspage.io/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Status</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://about.medium.com/creators/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Writers</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://blog.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Blog</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Careers</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Privacy</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Terms</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/about?autoplay=1&amp;source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">About</p></a></div><div class=\"ul um l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://speechify.com/medium?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Text to speech</p></a></div><div class=\"ul l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.com/business?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__=\"main-20230825-200253-6154416feb\"</script><script>window.__GRAPHQL_URI__ = \"https://towardsdatascience.com/_/graphql\"</script><script>window.__PRELOADED_STATE__ = {\"algolia\":{\"queries\":{}},\"cache\":{\"experimentGroupSet\":true,\"reason\":\"This request is not using the cache middleware worker\",\"group\":\"disabled\",\"tags\":[\"group-edgeCachePosts\",\"post-84c850470dcb\",\"user-fa31366b2eda\",\"collection-7f60cf5620c9\"],\"serverVariantState\":\"\",\"middlewareEnabled\":false,\"cacheStatus\":\"DYNAMIC\",\"shouldUseCache\":false,\"vary\":[],\"inDisabledExperiment\":false},\"client\":{\"hydrated\":false,\"isUs\":false,\"isNativeMedium\":false,\"isSafariMobile\":false,\"isSafari\":false,\"isFirefox\":false,\"routingEntity\":{\"type\":\"COLLECTION\",\"id\":\"7f60cf5620c9\",\"explicit\":true},\"viewerIsBot\":false},\"debug\":{\"requestId\":\"9089aad3-36a6-444e-83f4-6c1b358c24f2\",\"hybridDevServices\":[],\"originalSpanCarrier\":{\"ot-tracer-spanid\":\"36042a0c4deb383d\",\"ot-tracer-traceid\":\"172f9b7a285cba00\",\"ot-tracer-sampled\":\"true\"}},\"multiVote\":{\"clapsPerPost\":{}},\"navigation\":{\"branch\":{\"show\":null,\"hasRendered\":null,\"blockedByCTA\":false},\"hideGoogleOneTap\":false,\"hasRenderedAlternateUserBanner\":null,\"currentLocation\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\",\"host\":\"towardsdatascience.com\",\"hostname\":\"towardsdatascience.com\",\"referrer\":\"\",\"hasSetReferrer\":false,\"susiModal\":{\"step\":null,\"operation\":\"register\"},\"postRead\":false},\"config\":{\"nodeEnv\":\"production\",\"version\":\"main-20230825-200253-6154416feb\",\"target\":\"production\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\\\u002F\\\\u002Fcdn-client.medium.com\\\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\\\u002F\\\\u002Fglyph.medium.com\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk\",\"recaptcha3Key\":\"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5\",\"datadog\":{\"applicationId\":\"6702d87d-a7e0-42fe-bbcb-95b469547ea0\",\"clientToken\":\"pub853ea8d17ad6821d9f8f11861d23dfed\",\"rumToken\":\"pubf9cc52896502b9413b68ba36fc0c7162\",\"context\":{\"deployment\":{\"target\":\"production\",\"tag\":\"main-20230825-200253-6154416feb\",\"commit\":\"6154416feb686f04f429400f5935262015d7326e\"}},\"datacenter\":\"us\"},\"googleAnalyticsCode\":\"G-7JY7T788PK\",\"googlePay\":{\"apiVersion\":\"2\",\"apiVersionMinor\":\"0\",\"merchantId\":\"BCR2DN6TV7EMTGBM\",\"merchantName\":\"Medium\",\"instanceMerchantId\":\"13685562959212738550\"},\"applePay\":{\"version\":3},\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"],\"mediumMastodonDomainName\":\"me.dm\",\"mediumOwnedAndOperatedCollectionIds\":[\"8a9336e5bb4\",\"b7e45b22fec3\",\"193b68bd4fba\",\"8d6b8a439e32\",\"54c98c43354d\",\"3f6ecf56618\",\"d944778ce714\",\"92d2092dc598\",\"ae2a65f35510\",\"1285ba81cada\",\"544c7006046e\",\"fc8964313712\",\"40187e704f1c\",\"88d9857e584e\",\"7b6769f2748b\",\"bcc38c8f6edf\",\"cef6983b292\",\"cb8577c9149e\",\"444d13b52878\",\"713d7dbc99b0\",\"ef8e90590e66\",\"191186aaafa0\",\"55760f21cdc5\",\"9dc80918cc93\",\"bdc4052bbdba\",\"8ccfed20cbb2\"],\"tierOneDomains\":[\"medium.com\",\"thebolditalic.com\",\"arcdigital.media\",\"towardsdatascience.com\",\"uxdesign.cc\",\"codeburst.io\",\"psiloveyou.xyz\",\"writingcooperative.com\",\"entrepreneurshandbook.co\",\"prototypr.io\",\"betterhumans.coach.me\",\"theascent.pub\"],\"topicsToFollow\":[\"d61cf867d93f\",\"8a146bc21b28\",\"1eca0103fff3\",\"4d562ee63426\",\"aef1078a3ef5\",\"e15e46793f8d\",\"6158eb913466\",\"55f1c20aba7a\",\"3d18b94f6858\",\"4861fee224fd\",\"63c6f1f93ee\",\"1d98b3a9a871\",\"decb52b64abf\",\"ae5d4995e225\",\"830cded25262\"],\"topicToTagMappings\":{\"accessibility\":\"accessibility\",\"addiction\":\"addiction\",\"android-development\":\"android-development\",\"art\":\"art\",\"artificial-intelligence\":\"artificial-intelligence\",\"astrology\":\"astrology\",\"basic-income\":\"basic-income\",\"beauty\":\"beauty\",\"biotech\":\"biotech\",\"blockchain\":\"blockchain\",\"books\":\"books\",\"business\":\"business\",\"cannabis\":\"cannabis\",\"cities\":\"cities\",\"climate-change\":\"climate-change\",\"comics\":\"comics\",\"coronavirus\":\"coronavirus\",\"creativity\":\"creativity\",\"cryptocurrency\":\"cryptocurrency\",\"culture\":\"culture\",\"cybersecurity\":\"cybersecurity\",\"data-science\":\"data-science\",\"design\":\"design\",\"digital-life\":\"digital-life\",\"disability\":\"disability\",\"economy\":\"economy\",\"education\":\"education\",\"equality\":\"equality\",\"family\":\"family\",\"feminism\":\"feminism\",\"fiction\":\"fiction\",\"film\":\"film\",\"fitness\":\"fitness\",\"food\":\"food\",\"freelancing\":\"freelancing\",\"future\":\"future\",\"gadgets\":\"gadgets\",\"gaming\":\"gaming\",\"gun-control\":\"gun-control\",\"health\":\"health\",\"history\":\"history\",\"humor\":\"humor\",\"immigration\":\"immigration\",\"ios-development\":\"ios-development\",\"javascript\":\"javascript\",\"justice\":\"justice\",\"language\":\"language\",\"leadership\":\"leadership\",\"lgbtqia\":\"lgbtqia\",\"lifestyle\":\"lifestyle\",\"machine-learning\":\"machine-learning\",\"makers\":\"makers\",\"marketing\":\"marketing\",\"math\":\"math\",\"media\":\"media\",\"mental-health\":\"mental-health\",\"mindfulness\":\"mindfulness\",\"money\":\"money\",\"music\":\"music\",\"neuroscience\":\"neuroscience\",\"nonfiction\":\"nonfiction\",\"outdoors\":\"outdoors\",\"parenting\":\"parenting\",\"pets\":\"pets\",\"philosophy\":\"philosophy\",\"photography\":\"photography\",\"podcasts\":\"podcast\",\"poetry\":\"poetry\",\"politics\":\"politics\",\"privacy\":\"privacy\",\"product-management\":\"product-management\",\"productivity\":\"productivity\",\"programming\":\"programming\",\"psychedelics\":\"psychedelics\",\"psychology\":\"psychology\",\"race\":\"race\",\"relationships\":\"relationships\",\"religion\":\"religion\",\"remote-work\":\"remote-work\",\"san-francisco\":\"san-francisco\",\"science\":\"science\",\"self\":\"self\",\"self-driving-cars\":\"self-driving-cars\",\"sexuality\":\"sexuality\",\"social-media\":\"social-media\",\"society\":\"society\",\"software-engineering\":\"software-engineering\",\"space\":\"space\",\"spirituality\":\"spirituality\",\"sports\":\"sports\",\"startups\":\"startup\",\"style\":\"style\",\"technology\":\"technology\",\"transportation\":\"transportation\",\"travel\":\"travel\",\"true-crime\":\"true-crime\",\"tv\":\"tv\",\"ux\":\"ux\",\"venture-capital\":\"venture-capital\",\"visual-design\":\"visual-design\",\"work\":\"work\",\"world\":\"world\",\"writing\":\"writing\"},\"defaultImages\":{\"avatar\":{\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\",\"height\":150,\"width\":150},\"orgLogo\":{\"imageId\":\"1*OMF3fSqH8t4xBJ9-6oZDZw.png\",\"height\":106,\"width\":545},\"postLogo\":{\"imageId\":\"1*kFrc4tBFM_tCis-2Ic87WA.png\",\"height\":810,\"width\":1440},\"postPreviewImage\":{\"imageId\":\"1*hn4v1tCaJy7cWMyb0bpNpQ.png\",\"height\":386,\"width\":579}},\"collectionStructuredData\":{\"8d6b8a439e32\":{\"name\":\"Elemental\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fcdn-images-1.medium.com\\\\u002Fmax\\\\u002F980\\\\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png\",\"width\":980,\"height\":159}}},\"3f6ecf56618\":{\"name\":\"Forge\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fcdn-images-1.medium.com\\\\u002Fmax\\\\u002F596\\\\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png\",\"width\":596,\"height\":183}}},\"ae2a65f35510\":{\"name\":\"GEN\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fmax\\\\u002F264\\\\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png\",\"width\":264,\"height\":140}}},\"88d9857e584e\":{\"name\":\"LEVEL\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fmax\\\\u002F540\\\\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png\",\"width\":540,\"height\":108}}},\"7b6769f2748b\":{\"name\":\"Marker\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fcdn-images-1.medium.com\\\\u002Fmax\\\\u002F383\\\\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png\",\"width\":383,\"height\":92}}},\"444d13b52878\":{\"name\":\"OneZero\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fmax\\\\u002F540\\\\u002F1*cw32fIqCbRWzwJaoQw6BUg.png\",\"width\":540,\"height\":123}}},\"8ccfed20cbb2\":{\"name\":\"Zora\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Farticles\\\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\u002F\\\\u002Fmiro.medium.com\\\\u002Fmax\\\\u002F540\\\\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png\",\"width\":540,\"height\":106}}}},\"embeddedPostIds\":{\"coronavirus\":\"cd3010f9d81f\"},\"sharedCdcMessaging\":{\"COVID_APPLICABLE_TAG_SLUGS\":[],\"COVID_APPLICABLE_TOPIC_NAMES\":[],\"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE\":[],\"COVID_MESSAGES\":{\"tierA\":{\"text\":\"For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":66,\"end\":73,\"href\":\"https:\\\\u002F\\\\u002Fwww.cdc.gov\\\\u002Fcoronavirus\\\\u002F2019-nCoV\"}]},\"tierB\":{\"text\":\"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.\",\"markups\":[{\"start\":37,\"end\":45,\"href\":\"https:\\\\u002F\\\\u002Fhelp.medium.com\\\\u002Fhc\\\\u002Fen-us\\\\u002Fcategories\\\\u002F201931128-Policies-Safety\"},{\"start\":125,\"end\":132,\"href\":\"https:\\\\u002F\\\\u002Fwww.cdc.gov\\\\u002Fcoronavirus\\\\u002F2019-nCoV\"}]},\"paywall\":{\"text\":\"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":56,\"end\":70,\"href\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002Fmembership\"},{\"start\":138,\"end\":145,\"href\":\"https:\\\\u002F\\\\u002Fwww.cdc.gov\\\\u002Fcoronavirus\\\\u002F2019-nCoV\"}]},\"unbound\":{\"text\":\"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":45,\"end\":59,\"href\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002Fmembership\"},{\"start\":127,\"end\":134,\"href\":\"https:\\\\u002F\\\\u002Fwww.cdc.gov\\\\u002Fcoronavirus\\\\u002F2019-nCoV\"}]}},\"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST\":[\"3b31a67bff4a\"]},\"sharedVoteMessaging\":{\"TAGS\":[\"politics\",\"election-2020\",\"government\",\"us-politics\",\"election\",\"2020-presidential-race\",\"trump\",\"donald-trump\",\"democrats\",\"republicans\",\"congress\",\"republican-party\",\"democratic-party\",\"biden\",\"joe-biden\",\"maga\"],\"TOPICS\":[\"politics\",\"election\"],\"MESSAGE\":{\"text\":\"Find out more about the U.S. election results here.\",\"markups\":[{\"start\":46,\"end\":50,\"href\":\"https:\\\\u002F\\\\u002Fcookpolitical.com\\\\u002F2020-national-popular-vote-tracker\"}]},\"EXCLUDE_POSTS\":[\"397ef29e3ca5\"]},\"embedPostRules\":[],\"recircOptions\":{\"v1\":{\"limit\":3},\"v2\":{\"limit\":8}},\"braintreeClientKey\":\"production_zjkj96jm_m56f8fqpf7ngnrd4\",\"braintree\":{\"enabled\":true,\"merchantId\":\"m56f8fqpf7ngnrd4\",\"merchantAccountId\":{\"usd\":\"AMediumCorporation_instant\",\"eur\":\"amediumcorporation_EUR\",\"cad\":\"amediumcorporation_CAD\"},\"publicKey\":\"ds2nn34bg2z7j5gd\",\"braintreeEnvironment\":\"production\",\"dashboardUrl\":\"https:\\\\u002F\\\\u002Fwww.braintreegateway.com\\\\u002Fmerchants\",\"gracePeriodDurationInDays\":14,\"mediumMembershipPlanId\":{\"monthly\":\"ce105f8c57a3\",\"monthlyWithTrial\":\"d5ee3dbe3db8\",\"monthlyPremium\":\"fa741a9b47a2\",\"yearly\":\"a40ad4a43185\",\"yearlyStaff\":\"d74fb811198a\",\"yearlyWithTrial\":\"b3bc7350e5c7\",\"yearlyPremium\":\"e21bd2c12166\",\"monthlyCad\":\"p52orjkaceei\",\"yearlyCad\":\"h4q9g2up9ktt\"},\"braintreeDiscountId\":{\"oneMonthFree\":\"MONTHS_FREE_01\",\"threeMonthsFree\":\"MONTHS_FREE_03\",\"sixMonthsFree\":\"MONTHS_FREE_06\",\"fiftyPercentOffOneYear\":\"FIFTY_PERCENT_OFF_ONE_YEAR\"},\"3DSecureVersion\":\"2\",\"defaultCurrency\":\"usd\",\"providerPlanIdCurrency\":{\"4ycw\":\"usd\",\"rz3b\":\"usd\",\"3kqm\":\"usd\",\"jzw6\":\"usd\",\"c2q2\":\"usd\",\"nnsw\":\"usd\",\"q8qw\":\"usd\",\"d9y6\":\"usd\",\"fx7w\":\"cad\",\"nwf2\":\"cad\"}},\"paypalClientId\":\"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v\",\"paypal\":{\"host\":\"https:\\\\u002F\\\\u002Fapi.paypal.com:443\",\"clientMode\":\"production\",\"serverMode\":\"live\",\"webhookId\":\"4G466076A0294510S\",\"monthlyPlan\":{\"planId\":\"P-9WR0658853113943TMU5FDQA\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlan\":{\"planId\":\"P-7N8963881P8875835MU5JOPQ\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oneYearGift\":{\"name\":\"Medium Membership (1 Year, Digital Gift Code)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\\\\u002Fredeem.\",\"price\":\"50.00\",\"currency\":\"USD\",\"sku\":\"membership-gift-1-yr\"},\"oldMonthlyPlan\":{\"planId\":\"P-96U02458LM656772MJZUVH2Y\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlan\":{\"planId\":\"P-59P80963JF186412JJZU3SMI\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"monthlyPlanWithTrial\":{\"planId\":\"P-66C21969LR178604GJPVKUKY\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlanWithTrial\":{\"planId\":\"P-6XW32684EX226940VKCT2MFA\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oldMonthlyPlanNoSetupFee\":{\"planId\":\"P-4N046520HR188054PCJC7LJI\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlanNoSetupFee\":{\"planId\":\"P-7A4913502Y5181304CJEJMXQ\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"sdkUrl\":\"https:\\\\u002F\\\\u002Fwww.paypal.com\\\\u002Fsdk\\\\u002Fjs\"},\"stripePublishableKey\":\"pk_live_7FReX44VnNIInZwrIIx6ghjl\",\"log\":{\"json\":true,\"level\":\"info\"},\"imageUploadMaxSizeMb\":25,\"staffPicks\":{\"title\":\"Staff Picks\",\"catalogId\":\"c7bc6e1ee00f\"}},\"session\":{\"xsrf\":\"\"}}</script><script>window.__APOLLO_STATE__ = {\"ROOT_QUERY\":{\"__typename\":\"Query\",\"viewer\":null,\"collectionByDomainOrSlug({\\\\\"domainOrSlug\\\\\":\\\\\"towardsdatascience.com\\\\\"})\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"variantFlags\":[{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_apple_pay\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_entities_to_follow_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_lists_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"signup_services\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap,apple\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_client\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_continue_this_thread\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_server_upstream_deadlines\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_test_auth\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"disallow\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_miro_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_seamless_social_sharing\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_web_onboarding_upsell\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_remove_twitter_onboarding_step\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_offline_reading\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pill_based_home_feed\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"price_smoke_test_yearly\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_access\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rito_upstream_deadlines\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_digest_generation_pipeline\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"redefined_top_posts\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"can_send_tips_v0\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"custom_moc_preview_weight_threshold\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"8\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ml_rank_rex_anno\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_triton_recirc\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"price_smoke_test_monthly\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"sanity_check_aa_experiment_3\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"web_enable_syntax_highlighting\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_in_app_free_trial\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_for_members_username_selection\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_avatar_upload\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"onboarding_tags_from_top_views\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_offline_reading\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_footer_app_buttons\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_dynamic_paywall_programming\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_branch_io\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_verifications_service\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_digest_tagline\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_google_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tipping_v0_android\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_iceland_nux\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_twitter_auth_suggestions\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pre_pp_v4\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_members_only_audio\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_speechify_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"lo_non_moc_upsell\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"F\"}},{\"__typename\":\"VariantFlag\",\"name\":\"bevy_rds_double_write\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"disable_partner_program_enrollment\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_monthly_premium_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"12a660186432\"}},{\"__typename\":\"VariantFlag\",\"name\":\"android_two_hour_refresh\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_partner_program_v4\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_recirc_model\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"skip_fs_cache_user_vals\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_monthly_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"60e220181034\"}},{\"__typename\":\"VariantFlag\",\"name\":\"covid_19_cdc_banner\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_newsletter_lo_flow_custom_domains\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_integration\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_simplified_digest_v2_b\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_syntax_highlight\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_social_share_sheet\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_dynamic_aspirational_paywall\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_dynamic_programming_paywall\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_auto_follow_on_subscribe\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_legacy_feed_in_iceland\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_apple_sign_in\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_marketing_emails\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pp_v4\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_iceland_forced_android\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_easy_resubscribe\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_annual_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"2c754bcc2995\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_medium2_kbfd\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_automod\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_google_one_tap\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_starspace\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_updated_follower_email\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"author_fair_distribution_non_qp3\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_editor_new_publishing_flow\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_annual_premium_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"4a442ace1476\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_creator_welcome_email\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"coronavirus_topic_recirc\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"reader_fair_distribution_non_qp\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_image_sharer\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_app_flirty_thirty\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_annual_renewal_reminder_email\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_autorefresh\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_verified_book_author\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_author_cards\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_speechify_widget\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tick_landing_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_aurora_pub_follower_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tipping_v0_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"glyph_font_set\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"m2-unbound-source-serif-pro\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_verified_author\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"limit_post_referrers\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_for_members\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_new_push_notification_endpoint\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"textshots_userid\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_about_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_paypal\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_aggregator_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_signup_friction\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_group_gifting\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_home_post_menu\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"skip_sign_in_recaptcha\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"browsable_stream_config_bucket\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"curated-topics\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_trial_membership\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_apple_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"disable_edge_cache\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_import\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_response_markup\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pp_dashboard_referred_earnings\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tribute_landing_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"can_receive_tips_v0\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_updated_new_user_onboarding\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_author_cards_byline\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_stellate_metrics\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"explicit_signals_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_signup\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_dynamic_paywall_aspiriational\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_maim_the_meter\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tag_recs\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_google_pay\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_display_paywall_after_onboarding\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"signin_services\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap,apple\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_cache_less_following_feed\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_homepage\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_reading_history\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_sprig\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_topic_portals\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_lock_responses\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"limit_user_follows\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_email_sign_in_captcha\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}}],\"postResult({\\\\\"id\\\\\":\\\\\"84c850470dcb\\\\\"})\":{\"__ref\":\"Post:84c850470dcb\"},\"post({\\\\\"id\\\\\":\\\\\"84c850470dcb\\\\\"})\":{\"__ref\":\"Post:84c850470dcb\"},\"authorCollectionRecircFeed({\\\\\"input\\\\\":{\\\\\"authorId\\\\\":\\\\\"fa31366b2eda\\\\\",\\\\\"collectionId\\\\\":\\\\\"7f60cf5620c9\\\\\",\\\\\"paging\\\\\":{\\\\\"limit\\\\\":4},\\\\\"postId\\\\\":\\\\\"84c850470dcb\\\\\"}})\":{\"__typename\":\"AuthorCollectionRecircFeedResult\",\"items\":[{\"__typename\":\"HomeFeedItem\",\"post\":{\"__ref\":\"Post:552f0b41d021\"},\"feedId\":\"5085f58c-28f5-40cc-8720-a2fd4ff5c91e\"},{\"__typename\":\"HomeFeedItem\",\"post\":{\"__ref\":\"Post:7832ca4a37ef\"},\"feedId\":\"5085f58c-28f5-40cc-8720-a2fd4ff5c91e\"},{\"__typename\":\"HomeFeedItem\",\"post\":{\"__ref\":\"Post:df9823a04a32\"},\"feedId\":\"5085f58c-28f5-40cc-8720-a2fd4ff5c91e\"},{\"__typename\":\"HomeFeedItem\",\"post\":{\"__ref\":\"Post:bf2403804ada\"},\"feedId\":\"5085f58c-28f5-40cc-8720-a2fd4ff5c91e\"}]},\"recirc({\\\\\"paging\\\\\":{\\\\\"limit\\\\\":6},\\\\\"postId\\\\\":\\\\\"84c850470dcb\\\\\"})\":{\"__typename\":\"RexRecircResult\",\"items\":[{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:eb982c78ffac\"}},{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:5fe8f9e18efc\"}},{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:dc32d90b3a98\"}},{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:7743804e610e\"}},{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:fdc5bf240d6a\"}},{\"__typename\":\"RexRecircItem\",\"feedId\":\"a958a20f-1290-4998-bbe1-5efb12664f74\",\"post\":{\"__ref\":\"Post:1ff75bc7c9aa\"}}]},\"postCatalogRecirc({\\\\\"pagingOptions\\\\\":{\\\\\"limit\\\\\":4},\\\\\"postId\\\\\":\\\\\"84c850470dcb\\\\\"})\":{\"__typename\":\"CatalogsConnection\",\"catalogs\":[{\"__ref\":\"Catalog:5969c7449b7f\"},{\"__ref\":\"Catalog:e3668ea008e1\"},{\"__ref\":\"Catalog:0a856388a93a\"},{\"__ref\":\"Catalog:7a5756752f49\"}]}},\"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*VzTUkfeGymHP4Bvav-T-lA.png\"},\"Collection:7f60cf5620c9\":{\"__typename\":\"Collection\",\"id\":\"7f60cf5620c9\",\"favicon\":{\"__ref\":\"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png\"},\"customStyleSheet\":null,\"colorPalette\":{\"__typename\":\"ColorPalette\",\"highlightSpectrum\":{\"__typename\":\"ColorSpectrum\",\"backgroundColor\":\"#FFFFFFFF\",\"colorPoints\":[{\"__typename\":\"ColorPoint\",\"color\":\"#FFEDF4FC\",\"point\":0},{\"__typename\":\"ColorPoint\",\"color\":\"#FFE9F2FD\",\"point\":0.1},{\"__typename\":\"ColorPoint\",\"color\":\"#FFE6F1FD\",\"point\":0.2},{\"__typename\":\"ColorPoint\",\"color\":\"#FFE2EFFD\",\"point\":0.3},{\"__typename\":\"ColorPoint\",\"color\":\"#FFDFEEFD\",\"point\":0.4},{\"__typename\":\"ColorPoint\",\"color\":\"#FFDBECFE\",\"point\":0.5},{\"__typename\":\"ColorPoint\",\"color\":\"#FFD7EBFE\",\"point\":0.6},{\"__typename\":\"ColorPoint\",\"color\":\"#FFD4E9FE\",\"point\":0.7},{\"__typename\":\"ColorPoint\",\"color\":\"#FFD0E7FF\",\"point\":0.8},{\"__typename\":\"ColorPoint\",\"color\":\"#FFCCE6FF\",\"point\":0.9},{\"__typename\":\"ColorPoint\",\"color\":\"#FFC8E4FF\",\"point\":1}]},\"defaultBackgroundSpectrum\":{\"__typename\":\"ColorSpectrum\",\"backgroundColor\":\"#FFFFFFFF\",\"colorPoints\":[{\"__typename\":\"ColorPoint\",\"color\":\"#FF668AAA\",\"point\":0},{\"__typename\":\"ColorPoint\",\"color\":\"#FF61809D\",\"point\":0.1},{\"__typename\":\"ColorPoint\",\"color\":\"#FF5A7690\",\"point\":0.2},{\"__typename\":\"ColorPoint\",\"color\":\"#FF546C83\",\"point\":0.3},{\"__typename\":\"ColorPoint\",\"color\":\"#FF4D6275\",\"point\":0.4},{\"__typename\":\"ColorPoint\",\"color\":\"#FF455768\",\"point\":0.5},{\"__typename\":\"ColorPoint\",\"color\":\"#FF3D4C5A\",\"point\":0.6},{\"__typename\":\"ColorPoint\",\"color\":\"#FF34414C\",\"point\":0.7},{\"__typename\":\"ColorPoint\",\"color\":\"#FF2B353E\",\"point\":0.8},{\"__typename\":\"ColorPoint\",\"color\":\"#FF21282F\",\"point\":0.9},{\"__typename\":\"ColorPoint\",\"color\":\"#FF161B1F\",\"point\":1}]},\"tintBackgroundSpectrum\":{\"__typename\":\"ColorSpectrum\",\"backgroundColor\":\"#FF355876\",\"colorPoints\":[{\"__typename\":\"ColorPoint\",\"color\":\"#FF355876\",\"point\":0},{\"__typename\":\"ColorPoint\",\"color\":\"#FF4D6C88\",\"point\":0.1},{\"__typename\":\"ColorPoint\",\"color\":\"#FF637F99\",\"point\":0.2},{\"__typename\":\"ColorPoint\",\"color\":\"#FF7791A8\",\"point\":0.3},{\"__typename\":\"ColorPoint\",\"color\":\"#FF8CA2B7\",\"point\":0.4},{\"__typename\":\"ColorPoint\",\"color\":\"#FF9FB3C6\",\"point\":0.5},{\"__typename\":\"ColorPoint\",\"color\":\"#FFB2C3D4\",\"point\":0.6},{\"__typename\":\"ColorPoint\",\"color\":\"#FFC5D2E1\",\"point\":0.7},{\"__typename\":\"ColorPoint\",\"color\":\"#FFD7E2EE\",\"point\":0.8},{\"__typename\":\"ColorPoint\",\"color\":\"#FFE9F1FA\",\"point\":0.9},{\"__typename\":\"ColorPoint\",\"color\":\"#FFFBFFFF\",\"point\":1}]}},\"googleAnalyticsId\":null,\"editors\":[{\"__typename\":\"CollectionMastheadUserItem\",\"user\":{\"__ref\":\"User:7e12c71dfa81\"}},{\"__typename\":\"CollectionMastheadUserItem\",\"user\":{\"__ref\":\"User:e6ad8abedec9\"}},{\"__typename\":\"CollectionMastheadUserItem\",\"user\":{\"__ref\":\"User:895063a310f4\"}}],\"name\":\"Towards Data Science\",\"avatar\":{\"__ref\":\"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg\"},\"domain\":\"towardsdatascience.com\",\"slug\":\"towards-data-science\",\"description\":\"Your home for data science. A Medium publication sharing concepts, ideas and codes.\",\"subscriberCount\":671558,\"viewerEdge\":{\"__ref\":\"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_1853ac6e151f\"},\"twitterUsername\":\"TDataScience\",\"facebookPageId\":null,\"logo\":{\"__ref\":\"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png\"}},\"User:7e12c71dfa81\":{\"__typename\":\"User\",\"id\":\"7e12c71dfa81\"},\"User:e6ad8abedec9\":{\"__typename\":\"User\",\"id\":\"e6ad8abedec9\"},\"User:895063a310f4\":{\"__typename\":\"User\",\"id\":\"895063a310f4\"},\"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*CJe3891yB1A1mzMdqemkdg.jpeg\"},\"LinkedAccounts:fa31366b2eda\":{\"__typename\":\"LinkedAccounts\",\"mastodon\":null,\"id\":\"fa31366b2eda\"},\"UserViewerEdge:userId:fa31366b2eda-viewerId:lo_1853ac6e151f\":{\"__typename\":\"UserViewerEdge\",\"id\":\"userId:fa31366b2eda-viewerId:lo_1853ac6e151f\",\"isFollowing\":false,\"isUser\":false},\"NewsletterV3:25711c8db9ff\":{\"__typename\":\"NewsletterV3\",\"id\":\"25711c8db9ff\",\"type\":\"NEWSLETTER_TYPE_AUTHOR\",\"slug\":\"fa31366b2eda\",\"name\":\"fa31366b2eda\",\"collection\":null,\"user\":{\"__ref\":\"User:fa31366b2eda\"}},\"User:fa31366b2eda\":{\"__typename\":\"User\",\"id\":\"fa31366b2eda\",\"name\":\"Arjun Sarkar\",\"username\":\"arjun-sarkar786\",\"newsletterV3\":{\"__ref\":\"NewsletterV3:25711c8db9ff\"},\"linkedAccounts\":{\"__ref\":\"LinkedAccounts:fa31366b2eda\"},\"isSuspended\":false,\"imageId\":\"0*zwki88OLZVcjhvge\",\"mediumMemberAt\":0,\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":2629},\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"arjun-sarkar786.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"Ph.D. student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany. LinkedIn-https:\\\\u002F\\\\u002Fwww.linkedin.com\\\\u002Fin\\\\u002Farjun-sarkar-9a051777\\\\u002F\",\"isPartnerProgramEnrolled\":false,\"viewerEdge\":{\"__ref\":\"UserViewerEdge:userId:fa31366b2eda-viewerId:lo_1853ac6e151f\"},\"viewerIsUser\":false,\"postSubscribeMembershipUpsellShownAt\":0,\"allowNotes\":true,\"twitterScreenName\":\"\"},\"Topic:1eca0103fff3\":{\"__typename\":\"Topic\",\"slug\":\"machine-learning\",\"id\":\"1eca0103fff3\",\"name\":\"Machine Learning\"},\"Paragraph:52124fa8bdc4_0\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_0\",\"name\":\"2ae4\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Build your own Transformer from scratch using Pytorch\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_1\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_1\",\"name\":\"f3e2\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Building a Transformer model step by step in Pytorch\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\",\"originalHeight\":2514,\"originalWidth\":3353,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:52124fa8bdc4_2\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_2\",\"name\":\"e2f9\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"},\"text\":\"Figure 1. Photo by Kevin Ku on Unsplash\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":19,\"end\":27,\"href\":\"https:\\\\u002F\\\\u002Funsplash.com\\\\u002F@ikukevk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":31,\"end\":39,\"href\":\"https:\\\\u002F\\\\u002Funsplash.com\\\\u002Fs\\\\u002Fphotos\\\\u002Fdeep-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_3\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_3\",\"name\":\"d20e\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In this tutorial, we will build a basic Transformer model from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_4\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_4\",\"name\":\"87ad\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To understand Transformer models in detail kindly visit these two articles:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_5\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_5\",\"name\":\"352a\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"1. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":0,\"end\":94,\"href\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002Ftowards-data-science\\\\u002Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_6\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_6\",\"name\":\"9340\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"2. All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":0,\"end\":94,\"href\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002Ftowards-data-science\\\\u002Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_7\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_7\",\"name\":\"166f\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To build our Transformer model, we’ll follow these steps:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_8\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_8\",\"name\":\"2d64\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Import necessary libraries and modules\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_9\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_9\",\"name\":\"f428\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_10\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_10\",\"name\":\"3f2f\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Build the Encoder and Decoder layers\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_11\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_11\",\"name\":\"a3bd\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Combine Encoder and Decoder layers to create the complete Transformer model\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_12\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_12\",\"name\":\"8fb2\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Prepare sample data\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_13\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_13\",\"name\":\"6e50\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Train the model\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_14\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_14\",\"name\":\"24e9\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Let’s start by importing the necessary libraries and modules.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_15\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_15\",\"name\":\"85ec\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"import torch\\\\nimport torch.nn as nn\\\\nimport torch.optim as optim\\\\nimport torch.utils.data as data\\\\nimport math\\\\nimport copy\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"kotlin\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_16\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_16\",\"name\":\"c34a\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Now, we’ll define the basic building blocks of the Transformer model.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_17\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_17\",\"name\":\"7671\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Multi-Head Attention\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:0*--TCGWYxwASbv2ra.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*--TCGWYxwASbv2ra.png\",\"originalHeight\":906,\"originalWidth\":1215,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:52124fa8bdc4_18\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_18\",\"name\":\"53ef\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:0*--TCGWYxwASbv2ra.png\"},\"text\":\"Figure 2. Multi-Head Attention (source: image created by author)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_19\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_19\",\"name\":\"10a9\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_20\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_20\",\"name\":\"9cf5\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class MultiHeadAttention(nn.Module):\\\\n    def __init__(self, d_model, num_heads):\\\\n        super(MultiHeadAttention, self).__init__()\\\\n        assert d_model % num_heads == 0, \\\\\"d_model must be divisible by num_heads\\\\\"\\\\n        \\\\n        self.d_model = d_model\\\\n        self.num_heads = num_heads\\\\n        self.d_k = d_model \\\\u002F\\\\u002F num_heads\\\\n        \\\\n        self.W_q = nn.Linear(d_model, d_model)\\\\n        self.W_k = nn.Linear(d_model, d_model)\\\\n        self.W_v = nn.Linear(d_model, d_model)\\\\n        self.W_o = nn.Linear(d_model, d_model)\\\\n        \\\\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\\\\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) \\\\u002F math.sqrt(self.d_k)\\\\n        if mask is not None:\\\\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\\\\n        attn_probs = torch.softmax(attn_scores, dim=-1)\\\\n        output = torch.matmul(attn_probs, V)\\\\n        return output\\\\n        \\\\n    def split_heads(self, x):\\\\n        batch_size, seq_length, d_model = x.size()\\\\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\\\\n        \\\\n    def combine_heads(self, x):\\\\n        batch_size, _, seq_length, d_k = x.size()\\\\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\\\\n        \\\\n    def forward(self, Q, K, V, mask=None):\\\\n        Q = self.split_heads(self.W_q(Q))\\\\n        K = self.split_heads(self.W_k(K))\\\\n        V = self.split_heads(self.W_v(V))\\\\n        \\\\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\\\\n        output = self.W_o(self.combine_heads(attn_output))\\\\n        return output\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_21\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_21\",\"name\":\"3965\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The MultiHeadAttention code initializes the module with input parameters and linear transformation layers. It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_22\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_22\",\"name\":\"bcae\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Position-wise Feed-Forward Networks\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_23\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_23\",\"name\":\"d070\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class PositionWiseFeedForward(nn.Module):\\\\n    def __init__(self, d_model, d_ff):\\\\n        super(PositionWiseFeedForward, self).__init__()\\\\n        self.fc1 = nn.Linear(d_model, d_ff)\\\\n        self.fc2 = nn.Linear(d_ff, d_model)\\\\n        self.relu = nn.ReLU()\\\\n\\\\n    def forward(self, x):\\\\n        return self.fc2(self.relu(self.fc1(x)))\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"ruby\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_24\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_24\",\"name\":\"5ad2\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements a position-wise feed-forward network. The class initializes with two linear transformation layers and a ReLU activation function. The forward method applies these transformations and activation function sequentially to compute the output. This process enables the model to consider the position of input elements while making predictions.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_25\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_25\",\"name\":\"69ba\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Positional Encoding\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_26\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_26\",\"name\":\"2f9a\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_27\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_27\",\"name\":\"9b1a\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class PositionalEncoding(nn.Module):\\\\n    def __init__(self, d_model, max_seq_length):\\\\n        super(PositionalEncoding, self).__init__()\\\\n        \\\\n        pe = torch.zeros(max_seq_length, d_model)\\\\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\\\\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) \\\\u002F d_model))\\\\n        \\\\n        pe[:, 0::2] = torch.sin(position * div_term)\\\\n        pe[:, 1::2] = torch.cos(position * div_term)\\\\n        \\\\n        self.register_buffer(\\'pe\\', pe.unsqueeze(0))\\\\n        \\\\n    def forward(self, x):\\\\n        return x + self.pe[:, :x.size(1)]\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"scss\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_28\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_28\",\"name\":\"18d5\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values. The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term. The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_29\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_29\",\"name\":\"730e\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Now, we’ll build the Encoder and Decoder layers.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_30\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_30\",\"name\":\"e8c3\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Encoder Layer\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:0*bPKV4ekQr9ZjYkWJ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*bPKV4ekQr9ZjYkWJ.png\",\"originalHeight\":473,\"originalWidth\":276,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:52124fa8bdc4_31\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_31\",\"name\":\"d922\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:0*bPKV4ekQr9ZjYkWJ.png\"},\"text\":\"Figure 3. The Encoder part of the transformer network (Source: image from the original paper)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_32\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_32\",\"name\":\"3062\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_33\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_33\",\"name\":\"51dc\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class EncoderLayer(nn.Module):\\\\n    def __init__(self, d_model, num_heads, d_ff, dropout):\\\\n        super(EncoderLayer, self).__init__()\\\\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\\\\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\\\\n        self.norm1 = nn.LayerNorm(d_model)\\\\n        self.norm2 = nn.LayerNorm(d_model)\\\\n        self.dropout = nn.Dropout(dropout)\\\\n        \\\\n    def forward(self, x, mask):\\\\n        attn_output = self.self_attn(x, x, x, mask)\\\\n        x = self.norm1(x + self.dropout(attn_output))\\\\n        ff_output = self.feed_forward(x)\\\\n        x = self.norm2(x + self.dropout(ff_output))\\\\n        return x\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_34\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_34\",\"name\":\"008b\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_35\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_35\",\"name\":\"c227\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Decoder Layer\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:0*SPZgT4k8GQi37H__.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*SPZgT4k8GQi37H__.png\",\"originalHeight\":749,\"originalWidth\":276,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:52124fa8bdc4_36\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_36\",\"name\":\"3eae\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:0*SPZgT4k8GQi37H__.png\"},\"text\":\"Figure 4. The Decoder part of the Transformer network (Souce: Image from the original paper)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_37\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_37\",\"name\":\"b9dd\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_38\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_38\",\"name\":\"196d\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class DecoderLayer(nn.Module):\\\\n    def __init__(self, d_model, num_heads, d_ff, dropout):\\\\n        super(DecoderLayer, self).__init__()\\\\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\\\\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\\\\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\\\\n        self.norm1 = nn.LayerNorm(d_model)\\\\n        self.norm2 = nn.LayerNorm(d_model)\\\\n        self.norm3 = nn.LayerNorm(d_model)\\\\n        self.dropout = nn.Dropout(dropout)\\\\n        \\\\n    def forward(self, x, enc_output, src_mask, tgt_mask):\\\\n        attn_output = self.self_attn(x, x, x, tgt_mask)\\\\n        x = self.norm1(x + self.dropout(attn_output))\\\\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\\\\n        x = self.norm2(x + self.dropout(attn_output))\\\\n        ff_output = self.feed_forward(x)\\\\n        x = self.norm3(x + self.dropout(ff_output))\\\\n        return x\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_39\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_39\",\"name\":\"2a69\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_40\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_40\",\"name\":\"6196\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The forward method computes the decoder layer output by performing the following steps:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_41\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_41\",\"name\":\"1f3d\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Calculate the masked self-attention output and add it to the input tensor, followed by dropout and layer normalization.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_42\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_42\",\"name\":\"877d\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Compute the cross-attention output between the decoder and encoder outputs, and add it to the normalized masked self-attention output, followed by dropout and layer normalization.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_43\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_43\",\"name\":\"f616\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Calculate the position-wise feed-forward output and combine it with the normalized cross-attention output, followed by dropout and layer normalization.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_44\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_44\",\"name\":\"0e37\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Return the processed tensor.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_45\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_45\",\"name\":\"0beb\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"These operations enable the decoder to generate target sequences based on the input and the encoder output.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_46\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_46\",\"name\":\"8064\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Now, let’s combine the Encoder and Decoder layers to create the complete Transformer model.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_47\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_47\",\"name\":\"b207\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Transformer Model\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:0*ljYs7oOlKC71SzSr.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*ljYs7oOlKC71SzSr.png\",\"originalHeight\":739,\"originalWidth\":545,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:52124fa8bdc4_48\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_48\",\"name\":\"f5da\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:0*ljYs7oOlKC71SzSr.png\"},\"text\":\"Figure 5. The Transformer Network (Source: Image from the original paper)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_49\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_49\",\"name\":\"1a77\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Merging it all together:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_50\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_50\",\"name\":\"3bdd\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class Transformer(nn.Module):\\\\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\\\\n        super(Transformer, self).__init__()\\\\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\\\\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\\\\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\\\\n\\\\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\\\\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\\\\n\\\\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\\\\n        self.dropout = nn.Dropout(dropout)\\\\n\\\\n    def generate_mask(self, src, tgt):\\\\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\\\\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\\\\n        seq_length = tgt.size(1)\\\\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\\\\n        tgt_mask = tgt_mask & nopeak_mask\\\\n        return src_mask, tgt_mask\\\\n\\\\n    def forward(self, src, tgt):\\\\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\\\\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\\\\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\\\\n\\\\n        enc_output = src_embedded\\\\n        for enc_layer in self.encoder_layers:\\\\n            enc_output = enc_layer(enc_output, src_mask)\\\\n\\\\n        dec_output = tgt_embedded\\\\n        for dec_layer in self.decoder_layers:\\\\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\\\\n\\\\n        output = self.fc(dec_output)\\\\n        return output\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"scss\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_51\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_51\",\"name\":\"117f\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The Transformer class combines the previously defined modules to create a complete Transformer model. During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_52\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_52\",\"name\":\"8fb5\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer model’s output through the following steps:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_53\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_53\",\"name\":\"bd1e\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Generate source and target masks using the generate_mask method.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_54\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_54\",\"name\":\"dd66\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Compute source and target embeddings, and apply positional encoding and dropout.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_55\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_55\",\"name\":\"4586\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Process the source sequence through encoder layers, updating the enc_output tensor.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_56\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_56\",\"name\":\"b6b3\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Process the target sequence through decoder layers, using enc_output and masks, and updating the dec_output tensor.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_57\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_57\",\"name\":\"bf3e\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Apply the linear projection layer to the decoder output, obtaining output logits.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_58\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_58\",\"name\":\"67a9\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_59\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_59\",\"name\":\"66ad\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Preparing Sample Data\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_60\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_60\",\"name\":\"4429\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In this example, we will create a toy dataset for demonstration purposes. In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_61\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_61\",\"name\":\"ac94\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"src_vocab_size = 5000\\\\ntgt_vocab_size = 5000\\\\nd_model = 512\\\\nnum_heads = 8\\\\nnum_layers = 6\\\\nd_ff = 2048\\\\nmax_seq_length = 100\\\\ndropout = 0.1\\\\n\\\\ntransformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\\\\n\\\\n# Generate random sample data\\\\nsrc_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\\\\ntgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"ini\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_62\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_62\",\"name\":\"4483\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Training the Model\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_63\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_63\",\"name\":\"4509\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Now we’ll train the model using the sample data. In practice, you would use a larger dataset and split it into training and validation sets.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_64\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_64\",\"name\":\"6107\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"criterion = nn.CrossEntropyLoss(ignore_index=0)\\\\noptimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\\\\n\\\\ntransformer.train()\\\\n\\\\nfor epoch in range(100):\\\\n    optimizer.zero_grad()\\\\n    output = transformer(src_data, tgt_data[:, :-1])\\\\n    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\\\\n    loss.backward()\\\\n    optimizer.step()\\\\n    print(f\\\\\"Epoch: {epoch+1}, Loss: {loss.item()}\\\\\")\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"AUTO\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_65\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_65\",\"name\":\"c4f7\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"We can use this way to build a simple Transformer from scratch in Pytorch. All Large Language Models use these Transformer encoder or decoder blocks for training. Hence understanding the network that started it all is extremely important. Hope this article helps all looking to deep dive into LLM’s.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_66\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_66\",\"name\":\"004c\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"References\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_67\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_67\",\"name\":\"e52a\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Attention is all you need\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:52124fa8bdc4_68\":{\"__typename\":\"Paragraph\",\"id\":\"52124fa8bdc4_68\",\"name\":\"8667\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, {. Kaiser, and I. Polosukhin. Advances in Neural Information Processing Systems , page 5998–6008. (2017)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":0,\"end\":10,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F0\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":12,\"end\":22,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F1\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":24,\"end\":33,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F2\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":35,\"end\":47,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F3\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":49,\"end\":57,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F4\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":59,\"end\":67,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F5\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":69,\"end\":78,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F6\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":84,\"end\":97,\"href\":\"https:\\\\u002F\\\\u002Fwww.bibsonomy.org\\\\u002Fperson\\\\u002F1c9bf08cbcb15680c807e12a01dd8c929\\\\u002Fauthor\\\\u002F7\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":99,\"end\":167,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":168,\"end\":172,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_1853ac6e151f\":{\"__typename\":\"CollectionViewerEdge\",\"id\":\"collectionId:7f60cf5620c9-viewerId:lo_1853ac6e151f\",\"isEditor\":false},\"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*cFFKn8rFH4ZndmaYeAs6iQ.png\",\"originalWidth\":2381,\"originalHeight\":743},\"Tag:towards-data-science\":{\"__typename\":\"Tag\",\"id\":\"towards-data-science\",\"displayTitle\":\"Towards Data Science\",\"normalizedTagSlug\":\"towards-data-science\"},\"Tag:deep-learning\":{\"__typename\":\"Tag\",\"id\":\"deep-learning\",\"displayTitle\":\"Deep Learning\",\"normalizedTagSlug\":\"deep-learning\"},\"Tag:machine-learning\":{\"__typename\":\"Tag\",\"id\":\"machine-learning\",\"displayTitle\":\"Machine Learning\",\"normalizedTagSlug\":\"machine-learning\"},\"Tag:transformers\":{\"__typename\":\"Tag\",\"id\":\"transformers\",\"displayTitle\":\"Transformers\",\"normalizedTagSlug\":\"transformers\"},\"Tag:chatgpt\":{\"__typename\":\"Tag\",\"id\":\"chatgpt\",\"displayTitle\":\"ChatGPT\",\"normalizedTagSlug\":\"chatgpt\"},\"Post:84c850470dcb\":{\"__typename\":\"Post\",\"id\":\"84c850470dcb\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"content({\\\\\"postMeteringOptions\\\\\":{\\\\\"forceTruncation\\\\\":false}})\":{\"__typename\":\"PostContent\",\"isLockedPreviewOnly\":false,\"bodyModel\":{\"__typename\":\"RichText\",\"sections\":[{\"__typename\":\"Section\",\"name\":\"9adb\",\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null},{\"__typename\":\"Section\",\"name\":\"72c7\",\"startIndex\":7,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null},{\"__typename\":\"Section\",\"name\":\"a016\",\"startIndex\":14,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null},{\"__typename\":\"Section\",\"name\":\"0397\",\"startIndex\":59,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null},{\"__typename\":\"Section\",\"name\":\"1df9\",\"startIndex\":65,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null},{\"__typename\":\"Section\",\"name\":\"6d80\",\"startIndex\":66,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null}],\"paragraphs\":[{\"__ref\":\"Paragraph:52124fa8bdc4_0\"},{\"__ref\":\"Paragraph:52124fa8bdc4_1\"},{\"__ref\":\"Paragraph:52124fa8bdc4_2\"},{\"__ref\":\"Paragraph:52124fa8bdc4_3\"},{\"__ref\":\"Paragraph:52124fa8bdc4_4\"},{\"__ref\":\"Paragraph:52124fa8bdc4_5\"},{\"__ref\":\"Paragraph:52124fa8bdc4_6\"},{\"__ref\":\"Paragraph:52124fa8bdc4_7\"},{\"__ref\":\"Paragraph:52124fa8bdc4_8\"},{\"__ref\":\"Paragraph:52124fa8bdc4_9\"},{\"__ref\":\"Paragraph:52124fa8bdc4_10\"},{\"__ref\":\"Paragraph:52124fa8bdc4_11\"},{\"__ref\":\"Paragraph:52124fa8bdc4_12\"},{\"__ref\":\"Paragraph:52124fa8bdc4_13\"},{\"__ref\":\"Paragraph:52124fa8bdc4_14\"},{\"__ref\":\"Paragraph:52124fa8bdc4_15\"},{\"__ref\":\"Paragraph:52124fa8bdc4_16\"},{\"__ref\":\"Paragraph:52124fa8bdc4_17\"},{\"__ref\":\"Paragraph:52124fa8bdc4_18\"},{\"__ref\":\"Paragraph:52124fa8bdc4_19\"},{\"__ref\":\"Paragraph:52124fa8bdc4_20\"},{\"__ref\":\"Paragraph:52124fa8bdc4_21\"},{\"__ref\":\"Paragraph:52124fa8bdc4_22\"},{\"__ref\":\"Paragraph:52124fa8bdc4_23\"},{\"__ref\":\"Paragraph:52124fa8bdc4_24\"},{\"__ref\":\"Paragraph:52124fa8bdc4_25\"},{\"__ref\":\"Paragraph:52124fa8bdc4_26\"},{\"__ref\":\"Paragraph:52124fa8bdc4_27\"},{\"__ref\":\"Paragraph:52124fa8bdc4_28\"},{\"__ref\":\"Paragraph:52124fa8bdc4_29\"},{\"__ref\":\"Paragraph:52124fa8bdc4_30\"},{\"__ref\":\"Paragraph:52124fa8bdc4_31\"},{\"__ref\":\"Paragraph:52124fa8bdc4_32\"},{\"__ref\":\"Paragraph:52124fa8bdc4_33\"},{\"__ref\":\"Paragraph:52124fa8bdc4_34\"},{\"__ref\":\"Paragraph:52124fa8bdc4_35\"},{\"__ref\":\"Paragraph:52124fa8bdc4_36\"},{\"__ref\":\"Paragraph:52124fa8bdc4_37\"},{\"__ref\":\"Paragraph:52124fa8bdc4_38\"},{\"__ref\":\"Paragraph:52124fa8bdc4_39\"},{\"__ref\":\"Paragraph:52124fa8bdc4_40\"},{\"__ref\":\"Paragraph:52124fa8bdc4_41\"},{\"__ref\":\"Paragraph:52124fa8bdc4_42\"},{\"__ref\":\"Paragraph:52124fa8bdc4_43\"},{\"__ref\":\"Paragraph:52124fa8bdc4_44\"},{\"__ref\":\"Paragraph:52124fa8bdc4_45\"},{\"__ref\":\"Paragraph:52124fa8bdc4_46\"},{\"__ref\":\"Paragraph:52124fa8bdc4_47\"},{\"__ref\":\"Paragraph:52124fa8bdc4_48\"},{\"__ref\":\"Paragraph:52124fa8bdc4_49\"},{\"__ref\":\"Paragraph:52124fa8bdc4_50\"},{\"__ref\":\"Paragraph:52124fa8bdc4_51\"},{\"__ref\":\"Paragraph:52124fa8bdc4_52\"},{\"__ref\":\"Paragraph:52124fa8bdc4_53\"},{\"__ref\":\"Paragraph:52124fa8bdc4_54\"},{\"__ref\":\"Paragraph:52124fa8bdc4_55\"},{\"__ref\":\"Paragraph:52124fa8bdc4_56\"},{\"__ref\":\"Paragraph:52124fa8bdc4_57\"},{\"__ref\":\"Paragraph:52124fa8bdc4_58\"},{\"__ref\":\"Paragraph:52124fa8bdc4_59\"},{\"__ref\":\"Paragraph:52124fa8bdc4_60\"},{\"__ref\":\"Paragraph:52124fa8bdc4_61\"},{\"__ref\":\"Paragraph:52124fa8bdc4_62\"},{\"__ref\":\"Paragraph:52124fa8bdc4_63\"},{\"__ref\":\"Paragraph:52124fa8bdc4_64\"},{\"__ref\":\"Paragraph:52124fa8bdc4_65\"},{\"__ref\":\"Paragraph:52124fa8bdc4_66\"},{\"__ref\":\"Paragraph:52124fa8bdc4_67\"},{\"__ref\":\"Paragraph:52124fa8bdc4_68\"}]},\"validatedShareKey\":\"\",\"shareKeyCreator\":null},\"creator\":{\"__ref\":\"User:fa31366b2eda\"},\"inResponseToEntityType\":null,\"isLocked\":false,\"isMarkedPaywallOnly\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_NONE\",\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\",\"primaryTopic\":{\"__ref\":\"Topic:1eca0103fff3\"},\"topics\":[{\"__typename\":\"Topic\",\"slug\":\"machine-learning\"},{\"__typename\":\"Topic\",\"slug\":\"data-science\"}],\"isPublished\":true,\"latestPublishedVersion\":\"52124fa8bdc4\",\"visibility\":\"PUBLIC\",\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":5},\"createdAt\":1682377215940,\"firstPublishedAt\":1682515187389,\"latestPublishedAt\":1682515187389,\"clapCount\":404,\"allowResponses\":true,\"isLimitedState\":false,\"title\":\"Build your own Transformer from scratch using Pytorch\",\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\",\"socialTitle\":\"\",\"socialDek\":\"\",\"noIndex\":null,\"canonicalUrl\":\"\",\"metaDescription\":\"\",\"readingTime\":6.9805031446540875,\"previewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Building a Transformer model step by step in Pytorch\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*QM4AIEtMb0Z9Wj2TqLV1Tw.jpeg\"},\"isShortform\":false,\"seoTitle\":\"\",\"updatedAt\":1684487512007,\"shortformType\":\"SHORTFORM_TYPE_LINK\",\"seoDescription\":\"\",\"isSuspended\":false,\"license\":\"ALL_RIGHTS_RESERVED\",\"tags\":[{\"__ref\":\"Tag:towards-data-science\"},{\"__ref\":\"Tag:deep-learning\"},{\"__ref\":\"Tag:machine-learning\"},{\"__ref\":\"Tag:transformers\"},{\"__ref\":\"Tag:chatgpt\"}],\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"layerCake\":6,\"detectedLanguage\":\"en\",\"wordCount\":1629,\"inResponseToPostResult\":null,\"inResponseToCatalogResult\":null,\"curationEligibleAt\":1682381615392,\"isNewsletter\":false,\"isPublishToEmail\":true},\"ImageMetadata:1*Rv_pntt-N2WL7LMbIptHxQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*Rv_pntt-N2WL7LMbIptHxQ.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Post:552f0b41d021\":{\"__typename\":\"Post\",\"id\":\"552f0b41d021\",\"title\":\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 1\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Attention, Self-Attention, Multi-head Attention, and Transformers\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*Rv_pntt-N2WL7LMbIptHxQ.png\"},\"creator\":{\"__ref\":\"User:fa31366b2eda\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":15},\"visibility\":\"PUBLIC\",\"clapCount\":943,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1644919880232,\"firstPublishedAt\":1644884496670,\"readingTime\":11.666037735849057,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021\"},\"ImageMetadata:1*LliAcJQ4of2KId2dpkpvDg.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*LliAcJQ4of2KId2dpkpvDg.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:39db050c2ac2\":{\"__typename\":\"User\",\"id\":\"39db050c2ac2\",\"name\":\"Bex T.\",\"username\":\"ibexorigin\",\"mediumMemberAt\":1612944784000,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":20308},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"ibexorigin.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"BEXGBoost | DataCamp Instructor |🥇Top 10 AI\\\\u002FML Writer on Medium | Kaggle Master | https:\\\\u002F\\\\u002Fwww.linkedin.com\\\\u002Fin\\\\u002Fbextuychiev\\\\u002F\",\"imageId\":\"1*dEWNYKjVLledNST1EBuL_A.png\"},\"Post:7832ca4a37ef\":{\"__typename\":\"Post\",\"id\":\"7832ca4a37ef\",\"title\":\"130 ML Tricks And Resources Curated Carefully From 3 Years (Plus Free eBook)\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Each one is worth your time\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*LliAcJQ4of2KId2dpkpvDg.png\"},\"creator\":{\"__ref\":\"User:39db050c2ac2\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002F130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":10},\"visibility\":\"LOCKED\",\"clapCount\":3063,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1690898572727,\"firstPublishedAt\":1690898572727,\"readingTime\":47.922641509433966,\"isLocked\":true,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"130-ml-tricks-and-resources-curated-carefully-from-3-years-plus-free-ebook-7832ca4a37ef\"},\"ImageMetadata:1*jsksHuIVtuXDh-aLpwZDbw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*jsksHuIVtuXDh-aLpwZDbw.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:dc89da634938\":{\"__typename\":\"User\",\"id\":\"dc89da634938\",\"name\":\"Maxime Labonne\",\"username\":\"mlabonne\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":1680},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":true},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Ph.D., Sr. Machine Learning Scientist @ JPMorgan • Author of \\\\\"Hands-On Graph Neural Networks\\\\\" • twitter.com\\\\u002Fmaximelabonne\",\"imageId\":\"1*4DZEZLsIpQAI7efBPvYUxw.jpeg\"},\"Post:df9823a04a32\":{\"__typename\":\"Post\",\"id\":\"df9823a04a32\",\"title\":\"Fine-Tune Your Own Llama 2 Model in a Colab Notebook\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"A practical introduction to LLM fine-tuning\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*jsksHuIVtuXDh-aLpwZDbw.png\"},\"creator\":{\"__ref\":\"User:dc89da634938\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Ffine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":33},\"visibility\":\"LOCKED\",\"clapCount\":1855,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1692875444615,\"firstPublishedAt\":1690308484943,\"readingTime\":11.220754716981132,\"isLocked\":true,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32\"},\"ImageMetadata:0*Wvg_pNDViACfg-IK.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*Wvg_pNDViACfg-IK.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Post:bf2403804ada\":{\"__typename\":\"Post\",\"id\":\"bf2403804ada\",\"title\":\"All you need to know about ‘Attention’ and ‘Transformers’\\u200a—\\u200aIn-depth Understanding\\u200a—\\u200aPart 2\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Attention, Self-Attention, Multi-head Attention, Masked Multi-head Attention, Transformers, BERT, and GPT\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:0*Wvg_pNDViACfg-IK.png\"},\"creator\":{\"__ref\":\"User:fa31366b2eda\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fall-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":5},\"visibility\":\"PUBLIC\",\"clapCount\":501,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1663098075715,\"firstPublishedAt\":1663079542559,\"readingTime\":8.70188679245283,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada\"},\"ImageMetadata:1*njagJOgiT-VTJjQ18bugcw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*njagJOgiT-VTJjQ18bugcw.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:3ab8d3143e32\":{\"__typename\":\"User\",\"id\":\"3ab8d3143e32\",\"name\":\"Dominik Polzer\",\"username\":\"dmnkplzr\",\"mediumMemberAt\":1611573769554,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":1668},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"dmnkplzr.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"Machine Learning Engineer. I like to break down machine learning concepts into easy to understand pieces. linkedin.com\\\\u002Fin\\\\u002Fpolzerdo\\\\u002F\",\"imageId\":\"1*KqpicOFO7jh7FXGjoJ2Bcg.jpeg\"},\"Post:eb982c78ffac\":{\"__typename\":\"Post\",\"id\":\"eb982c78ffac\",\"title\":\"All You Need to Know to Build Your First LLM App\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"A step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*njagJOgiT-VTJjQ18bugcw.png\"},\"creator\":{\"__ref\":\"User:3ab8d3143e32\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Ftowardsdatascience.com\\\\u002Fall-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac\",\"collection\":{\"__ref\":\"Collection:7f60cf5620c9\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":42},\"visibility\":\"LOCKED\",\"clapCount\":4639,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1691137369731,\"firstPublishedAt\":1687406041840,\"readingTime\":25.813207547169814,\"isLocked\":true,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac\"},\"ImageMetadata:0*brmcPLvJpiQWjZpY\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*brmcPLvJpiQWjZpY\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:931fc8afcda1\":{\"__typename\":\"User\",\"id\":\"931fc8afcda1\",\"name\":\"Fahim Rustamy, PhD\",\"username\":\"faheemrustamy\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":64},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Machine Learning Specialist\",\"imageId\":\"0*qmd1yyz_btTHNCDI.jpg\"},\"Post:5fe8f9e18efc\":{\"__typename\":\"Post\",\"id\":\"5fe8f9e18efc\",\"title\":\"Vision Transformers vs. Convolutional Neural Networks\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"This blog post is inspired by the paper titled AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE from google’s…\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:0*brmcPLvJpiQWjZpY\"},\"creator\":{\"__ref\":\"User:931fc8afcda1\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002F@faheemrustamy\\\\u002Fvision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc\",\"collection\":null,\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":6},\"visibility\":\"PUBLIC\",\"clapCount\":385,\"pendingCollection\":null,\"statusForCollection\":null,\"pinnedAt\":0,\"latestPublishedAt\":1685911052762,\"firstPublishedAt\":1685911052762,\"readingTime\":6.399056603773585,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"vision-transformers-vs-convolutional-neural-networks-5fe8f9e18efc\"},\"ImageMetadata:1*07Jm441iYuRuuNlB297C3w.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*07Jm441iYuRuuNlB297C3w.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:a497412b5611\":{\"__typename\":\"User\",\"id\":\"a497412b5611\",\"name\":\"Yule Wang, PhD\",\"username\":\"yulemoon\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":335},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Don\\'t just add the story to your list. Read it :p MLE, Physics PhD. YT channel: youtube.com\\\\u002F@yulewang_machinelearning LinkedIn: linkedin.com\\\\u002Fin\\\\u002Fyule-wang-ml\\\\u002F\",\"imageId\":\"1*9VFHaHkwy2zxEocPrlMOhQ.png\"},\"Post:dc32d90b3a98\":{\"__typename\":\"Post\",\"id\":\"dc32d90b3a98\",\"title\":\"Step-by-Step Illustrated Explanations of Transformer\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"with detailed explained Attention Mechanism\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*07Jm441iYuRuuNlB297C3w.png\"},\"creator\":{\"__ref\":\"User:a497412b5611\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002F@yulemoon\\\\u002Fdetailed-explanations-of-transformer-step-by-step-dc32d90b3a98\",\"collection\":null,\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":3},\"visibility\":\"PUBLIC\",\"clapCount\":243,\"pendingCollection\":null,\"statusForCollection\":null,\"pinnedAt\":0,\"latestPublishedAt\":1682348879301,\"firstPublishedAt\":1677513764108,\"readingTime\":7.561320754716981,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"detailed-explanations-of-transformer-step-by-step-dc32d90b3a98\"},\"ImageMetadata:0*wMAccADhdvhe9yf8\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*wMAccADhdvhe9yf8\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:4a992408a97e\":{\"__typename\":\"User\",\"id\":\"4a992408a97e\",\"name\":\"FunCry\",\"username\":\"funcry\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":18},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"Email: andy.pokai.chen@gmail.com\",\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\"},\"Post:7743804e610e\":{\"__typename\":\"Post\",\"id\":\"7743804e610e\",\"title\":\"In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Introduction\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:0*wMAccADhdvhe9yf8\"},\"creator\":{\"__ref\":\"User:4a992408a97e\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002F@funcry\\\\u002Fin-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e\",\"collection\":null,\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":0},\"visibility\":\"PUBLIC\",\"clapCount\":64,\"pendingCollection\":null,\"statusForCollection\":null,\"pinnedAt\":0,\"latestPublishedAt\":1677943402958,\"firstPublishedAt\":1677652259798,\"readingTime\":7.436792452830188,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e\"},\"ImageMetadata:1*ZwtGweo0PSFPwUVpDH-p7w.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*ZwtGweo0PSFPwUVpDH-p7w.jpeg\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:ffffcb9e269d\":{\"__typename\":\"User\",\"id\":\"ffffcb9e269d\",\"name\":\"ai geek (wishesh)\",\"username\":\"_aigeek\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":10},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"ai curious. my curiosity grows with my knowledge, but so does my ignorance.\",\"imageId\":\"1*IpHbl7Dg0Q35MFdCW4bK4Q.jpeg\"},\"Post:fdc5bf240d6a\":{\"__typename\":\"Post\",\"id\":\"fdc5bf240d6a\",\"title\":\"Best Practices for Deploying Large Language Models (LLMs) in Production\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Large Language Models (LLMs) have revolutionized the field of natural language processing and understanding, enabling a wide range of AI…\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*ZwtGweo0PSFPwUVpDH-p7w.jpeg\"},\"creator\":{\"__ref\":\"User:ffffcb9e269d\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Fmedium.com\\\\u002F@_aigeek\\\\u002Fbest-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a\",\"collection\":null,\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":0},\"visibility\":\"PUBLIC\",\"clapCount\":3,\"pendingCollection\":null,\"statusForCollection\":null,\"pinnedAt\":0,\"latestPublishedAt\":1687786914944,\"firstPublishedAt\":1687786914944,\"readingTime\":9.50377358490566,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"best-practices-for-deploying-large-language-models-llms-in-production-fdc5bf240d6a\"},\"ImageMetadata:1*GxLaWT8zNdNzY7KxV8rlCA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*GxLaWT8zNdNzY7KxV8rlCA.png\",\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"User:492215e5ab91\":{\"__typename\":\"User\",\"id\":\"492215e5ab91\",\"name\":\"DemoGPT\",\"username\":\"demogpt\",\"mediumMemberAt\":0,\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":39},\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"demogpt.medium.com\"}},\"hasSubdomain\":true,\"bio\":\"https:\\\\u002F\\\\u002Fgithub.com\\\\u002Fmelih-unsal\\\\u002FDemoGPT DemoGPT enables you to create quick demos by just using prompt. ⭐ Star to support our work!\",\"imageId\":\"1*qC8NCPcO6Q_MIByXRE9-bA.png\"},\"Collection:9c6d01af5370\":{\"__typename\":\"Collection\",\"id\":\"9c6d01af5370\",\"slug\":\"ai-mind-labs\",\"name\":\"AI Mind\",\"domain\":\"pub.aimind.so\",\"description\":\"Learn, explore, or build the future of AI with top stories on the latest trends, tools, and technology. Share your crazy success stories or AI-fueled fails in a supportive community.\",\"subscriberCount\":151,\"avatar\":{\"__ref\":\"ImageMetadata:1*XPMKaE8Qk4bPytz539rNeQ.gif\"}},\"ImageMetadata:1*XPMKaE8Qk4bPytz539rNeQ.gif\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*XPMKaE8Qk4bPytz539rNeQ.gif\"},\"Post:1ff75bc7c9aa\":{\"__typename\":\"Post\",\"id\":\"1ff75bc7c9aa\",\"title\":\"All You Need to Know to Build Your First LLM App\",\"extendedPreviewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"Are you intrigued by the potential of Language Learning Models (LLMs) and eager to create your first LLM-based application? Or maybe…\",\"isFullContent\":false},\"previewImage\":{\"__ref\":\"ImageMetadata:1*GxLaWT8zNdNzY7KxV8rlCA.png\"},\"creator\":{\"__ref\":\"User:492215e5ab91\"},\"isPublished\":true,\"mediumUrl\":\"https:\\\\u002F\\\\u002Fpub.aimind.so\\\\u002Fall-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa\",\"collection\":{\"__ref\":\"Collection:9c6d01af5370\"},\"isLimitedState\":false,\"allowResponses\":true,\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":0},\"visibility\":\"PUBLIC\",\"clapCount\":53,\"pendingCollection\":null,\"statusForCollection\":\"APPROVED\",\"pinnedAt\":0,\"latestPublishedAt\":1690815082388,\"firstPublishedAt\":1688204735485,\"readingTime\":1.7962264150943397,\"isLocked\":false,\"sequence\":null,\"isSeries\":false,\"uniqueSlug\":\"all-you-need-to-know-to-build-your-first-llm-app-1ff75bc7c9aa\"},\"User:a32c340ea342\":{\"__typename\":\"User\",\"username\":\"MediumStaff\",\"id\":\"a32c340ea342\"},\"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:lo_1853ac6e151f\":{\"__typename\":\"CatalogViewerEdge\",\"followersCount\":92,\"id\":\"catalogId:5969c7449b7f-viewerId:lo_1853ac6e151f\"},\"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*3OsUtsnlTx9Svm4c.jpg\",\"alt\":\"Image by vectorjuice on FreePik\"},\"Post:23a2173eecae\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg\"},\"id\":\"23a2173eecae\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6482363e212e6c41a481d903\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"6482363e212e6c41a481d903\",\"entity\":{\"__ref\":\"Post:23a2173eecae\"}},\"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*IPZF1hcDWwpPqOz2vL7NxQ.png\",\"alt\":null},\"Post:3bc2644d4507\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png\"},\"id\":\"3bc2644d4507\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"641cac35672446f81159a840\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"641cac35672446f81159a840\",\"entity\":{\"__ref\":\"Post:3bc2644d4507\"}},\"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*0fHUKyg3xtpNWpop35PR4g.png\",\"alt\":null},\"Post:59c16ae76e3e\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png\"},\"id\":\"59c16ae76e3e\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6422e78d7bc8cca169b3ce68\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"6422e78d7bc8cca169b3ce68\",\"entity\":{\"__ref\":\"Post:59c16ae76e3e\"}},\"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*FS8L4XLW_3mHDyRKNwjKvA.jpeg\",\"alt\":null},\"Post:2bf00851551e\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg\"},\"id\":\"2bf00851551e\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6421a9dc3b730f2980719d64\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"6421a9dc3b730f2980719d64\",\"entity\":{\"__ref\":\"Post:2bf00851551e\"}},\"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*OhWzHPkxXbslpwYyCZ2HKA.png\",\"alt\":null},\"Post:1ce5fca96286\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png\"},\"id\":\"1ce5fca96286\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"641cac41b70e26f89f868177\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"641cac41b70e26f89f868177\",\"entity\":{\"__ref\":\"Post:1ce5fca96286\"}},\"Catalog:5969c7449b7f\":{\"__typename\":\"Catalog\",\"id\":\"5969c7449b7f\",\"name\":\"The New Chatbots: ChatGPT, Bard, and Beyond\",\"postItemsCount\":13,\"predefined\":null,\"creator\":{\"__ref\":\"User:a32c340ea342\"},\"viewerEdge\":{\"__ref\":\"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:lo_1853ac6e151f\"},\"itemsConnection:(limit:5)\":{\"__typename\":\"CatalogItemsConnection\",\"items\":[{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6482363e212e6c41a481d903\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"641cac35672446f81159a840\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6422e78d7bc8cca169b3ce68\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"6421a9dc3b730f2980719d64\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"641cac41b70e26f89f868177\\\\\"}\"}]}},\"User:65e22f4ac01c\":{\"__typename\":\"User\",\"username\":\"ben.putney\",\"id\":\"65e22f4ac01c\"},\"CatalogViewerEdge:catalogId:e3668ea008e1-viewerId:lo_1853ac6e151f\":{\"__typename\":\"CatalogViewerEdge\",\"followersCount\":307,\"id\":\"catalogId:e3668ea008e1-viewerId:lo_1853ac6e151f\"},\"ImageMetadata:0*r4yjMpEmqzHCUvWC.jpg\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*r4yjMpEmqzHCUvWC.jpg\",\"alt\":null},\"Post:b82b9ea831e0\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*r4yjMpEmqzHCUvWC.jpg\"},\"id\":\"b82b9ea831e0\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64dd0a1fa9164fa438851822\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64dd0a1fa9164fa438851822\",\"entity\":{\"__ref\":\"Post:b82b9ea831e0\"}},\"ImageMetadata:1*bv2KUVNLi2sFNjBTdoBmWw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*bv2KUVNLi2sFNjBTdoBmWw.png\",\"alt\":null},\"Post:5bda439fa506\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*bv2KUVNLi2sFNjBTdoBmWw.png\"},\"id\":\"5bda439fa506\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64dce8236851745fea469328\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64dce8236851745fea469328\",\"entity\":{\"__ref\":\"Post:5bda439fa506\"}},\"ImageMetadata:0*zsngbTOmFCy6sUCx.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*zsngbTOmFCy6sUCx.jpeg\",\"alt\":null},\"Post:e377af0ffdaa\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*zsngbTOmFCy6sUCx.jpeg\"},\"id\":\"e377af0ffdaa\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0199b5330ca62c672fa8e\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63d0199b5330ca62c672fa8e\",\"entity\":{\"__ref\":\"Post:e377af0ffdaa\"}},\"ImageMetadata:1*EPYpvWdUxGbCkbgwLUAarg.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*EPYpvWdUxGbCkbgwLUAarg.png\",\"alt\":null},\"Post:5f40d55184c4\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*EPYpvWdUxGbCkbgwLUAarg.png\"},\"id\":\"5f40d55184c4\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0182884e5c1fa7c270bcb\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63d0182884e5c1fa7c270bcb\",\"entity\":{\"__ref\":\"Post:5f40d55184c4\"}},\"ImageMetadata:0*4JkYIO0SWzjCCATB.jpg\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*4JkYIO0SWzjCCATB.jpg\",\"alt\":null},\"Post:a432aa037ee1\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*4JkYIO0SWzjCCATB.jpg\"},\"id\":\"a432aa037ee1\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0162d64907a64cb199dff\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63d0162d64907a64cb199dff\",\"entity\":{\"__ref\":\"Post:a432aa037ee1\"}},\"Catalog:e3668ea008e1\":{\"__typename\":\"Catalog\",\"id\":\"e3668ea008e1\",\"name\":\"Predictive Modeling w\\\\u002F Python\",\"postItemsCount\":20,\"predefined\":null,\"creator\":{\"__ref\":\"User:65e22f4ac01c\"},\"viewerEdge\":{\"__ref\":\"CatalogViewerEdge:catalogId:e3668ea008e1-viewerId:lo_1853ac6e151f\"},\"itemsConnection:(limit:5)\":{\"__typename\":\"CatalogItemsConnection\",\"items\":[{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64dd0a1fa9164fa438851822\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64dce8236851745fea469328\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0199b5330ca62c672fa8e\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0182884e5c1fa7c270bcb\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63d0162d64907a64cb199dff\\\\\"}\"}]}},\"User:2eb23a991a63\":{\"__typename\":\"User\",\"username\":\"AMGAS14\",\"id\":\"2eb23a991a63\"},\"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:lo_1853ac6e151f\":{\"__typename\":\"CatalogViewerEdge\",\"followersCount\":163,\"id\":\"catalogId:0a856388a93a-viewerId:lo_1853ac6e151f\"},\"ImageMetadata:1*0yqyeKfH-r_6PsgvS4_pgA.jpeg\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*0yqyeKfH-r_6PsgvS4_pgA.jpeg\",\"alt\":null},\"Post:51d2f4cd2ccf\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*0yqyeKfH-r_6PsgvS4_pgA.jpeg\"},\"id\":\"51d2f4cd2ccf\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64ea3d7b990b3c2f415c1f78\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64ea3d7b990b3c2f415c1f78\",\"entity\":{\"__ref\":\"Post:51d2f4cd2ccf\"}},\"ImageMetadata:1*wpbt4dgRYD9O9tEUnPdtPQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*wpbt4dgRYD9O9tEUnPdtPQ.png\",\"alt\":null},\"Post:b28d7cedc9f5\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*wpbt4dgRYD9O9tEUnPdtPQ.png\"},\"id\":\"b28d7cedc9f5\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e892623b068ee4ab135d60\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64e892623b068ee4ab135d60\",\"entity\":{\"__ref\":\"Post:b28d7cedc9f5\"}},\"ImageMetadata:1*-M7fV5SGuGauCrlQHElzxw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*-M7fV5SGuGauCrlQHElzxw.png\",\"alt\":null},\"Post:a6419dc38775\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*-M7fV5SGuGauCrlQHElzxw.png\"},\"id\":\"a6419dc38775\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e8925ba5845a2bdb2aa084\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64e8925ba5845a2bdb2aa084\",\"entity\":{\"__ref\":\"Post:a6419dc38775\"}},\"ImageMetadata:0*u2iqkejjtfBF14Bj\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*u2iqkejjtfBF14Bj\",\"alt\":null},\"Post:61767e468e66\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*u2iqkejjtfBF14Bj\"},\"id\":\"61767e468e66\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e8922df2f24ddc685443c4\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64e8922df2f24ddc685443c4\",\"entity\":{\"__ref\":\"Post:61767e468e66\"}},\"ImageMetadata:1*BGS5vwf2CUqJ8HxUQ5SGkA.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*BGS5vwf2CUqJ8HxUQ5SGkA.png\",\"alt\":null},\"Post:c81044081e6f\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*BGS5vwf2CUqJ8HxUQ5SGkA.png\"},\"id\":\"c81044081e6f\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e892089626dc97ddb2ee2b\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"64e892089626dc97ddb2ee2b\",\"entity\":{\"__ref\":\"Post:c81044081e6f\"}},\"Catalog:0a856388a93a\":{\"__typename\":\"Catalog\",\"id\":\"0a856388a93a\",\"name\":\"Natural Language Processing\",\"postItemsCount\":544,\"predefined\":null,\"creator\":{\"__ref\":\"User:2eb23a991a63\"},\"viewerEdge\":{\"__ref\":\"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:lo_1853ac6e151f\"},\"itemsConnection:(limit:5)\":{\"__typename\":\"CatalogItemsConnection\",\"items\":[{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64ea3d7b990b3c2f415c1f78\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e892623b068ee4ab135d60\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e8925ba5845a2bdb2aa084\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e8922df2f24ddc685443c4\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"64e892089626dc97ddb2ee2b\\\\\"}\"}]}},\"User:795f94e67359\":{\"__typename\":\"User\",\"username\":\"MediumForTeams\",\"id\":\"795f94e67359\"},\"CatalogViewerEdge:catalogId:7a5756752f49-viewerId:lo_1853ac6e151f\":{\"__typename\":\"CatalogViewerEdge\",\"followersCount\":160,\"id\":\"catalogId:7a5756752f49-viewerId:lo_1853ac6e151f\"},\"ImageMetadata:0*_eYHSSUS0abUxmDU\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*_eYHSSUS0abUxmDU\",\"alt\":null},\"Post:7ec25c57ca94\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*_eYHSSUS0abUxmDU\"},\"id\":\"7ec25c57ca94\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ec663b2cbf4a9d8f6a09\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63c7ec663b2cbf4a9d8f6a09\",\"entity\":{\"__ref\":\"Post:7ec25c57ca94\"}},\"ImageMetadata:1*wXgeNtz5OJ5O9T3c3mQRRw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*wXgeNtz5OJ5O9T3c3mQRRw.png\",\"alt\":null},\"Post:aa824ad89623\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*wXgeNtz5OJ5O9T3c3mQRRw.png\"},\"id\":\"aa824ad89623\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ecb03b2cbf4a9d8f6a13\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63c7ecb03b2cbf4a9d8f6a13\",\"entity\":{\"__ref\":\"Post:aa824ad89623\"}},\"ImageMetadata:0*tIipcmrInD5UMpQI.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"0*tIipcmrInD5UMpQI.png\",\"alt\":null},\"Post:a25fa9f54442\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:0*tIipcmrInD5UMpQI.png\"},\"id\":\"a25fa9f54442\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed6e945adc0d2f12ee29\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63c7ed6e945adc0d2f12ee29\",\"entity\":{\"__ref\":\"Post:a25fa9f54442\"}},\"ImageMetadata:1*uPt_ccZmvCMnQahIs43ahg.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*uPt_ccZmvCMnQahIs43ahg.png\",\"alt\":null},\"Post:8342dd70d2bd\":{\"__typename\":\"Post\",\"previewImage\":{\"__ref\":\"ImageMetadata:1*uPt_ccZmvCMnQahIs43ahg.png\"},\"id\":\"8342dd70d2bd\"},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed5c1322ec40550cfe21\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63c7ed5c1322ec40550cfe21\",\"entity\":{\"__ref\":\"Post:8342dd70d2bd\"}},\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed5515e8da9c2c688cbd\\\\\"}\":{\"__typename\":\"CatalogItemV2\",\"catalogItemId\":\"63c7ed5515e8da9c2c688cbd\",\"entity\":{\"__ref\":\"Post:3bc2644d4507\"}},\"Catalog:7a5756752f49\":{\"__typename\":\"Catalog\",\"id\":\"7a5756752f49\",\"name\":\"What is ChatGPT?\",\"postItemsCount\":9,\"predefined\":null,\"creator\":{\"__ref\":\"User:795f94e67359\"},\"viewerEdge\":{\"__ref\":\"CatalogViewerEdge:catalogId:7a5756752f49-viewerId:lo_1853ac6e151f\"},\"itemsConnection:(limit:5)\":{\"__typename\":\"CatalogItemsConnection\",\"items\":[{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ec663b2cbf4a9d8f6a09\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ecb03b2cbf4a9d8f6a13\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed6e945adc0d2f12ee29\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed5c1322ec40550cfe21\\\\\"}\"},{\"__ref\":\"CatalogItemV2:{\\\\\"catalogItemId\\\\\":\\\\\"63c7ed5515e8da9c2c688cbd\\\\\"}\"}]}}}</script><script src=\"https://cdn-client.medium.com/lite/static/js/manifest.52bd5824.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/6036.d874957b.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/main.df738459.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/instrumentation.63e6e68a.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/6068.466148a0.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/4398.780b79a2.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/6733.c6c17f3e.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/4711.73746114.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/8695.e94d3d6a.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9662.6f49aad7.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/3154.b5b628cf.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/5203.6e50c2c5.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1957.184f7c3e.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9599.473d1298.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1711.6abc82e9.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/5268.340f7f3b.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9114.c80fe402.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/5459.cfc2e69b.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/6804.971ca132.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9174.587860c8.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/4129.dbbad263.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/8580.1dc03c85.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1802.29605d4c.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/4078.9fb8a750.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/8883.1b34e6e2.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9408.0a5d0210.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/923.b5d375ce.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1743.91c7efb0.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/2550.97769d28.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/7939.d9667a53.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/9150.a9db6cd7.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/5005.4ccc91b2.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/2804.34ef344c.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/1006.97cfd7bf.chunk.js\"></script>\\n<script src=\"https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.c8e99352.chunk.js\"></script><script>window.main();</script><script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/v8b253dfea2ab4077af8c6f58422dfbfd1689876627854\" integrity=\"sha512-bjgnUKX4azu3dLTVtie9u6TKqgx29RBwfj3QXYt5EKfWM/9hPSAI/4qcV5NACjwAo8UtTeWefx6Zq5PHcMm7Tg==\" data-cf-beacon=\\'{\"rayId\":\"7fcfc86f4e5a292b\",\"version\":\"2023.8.0\",\"b\":1,\"token\":\"0b5f665943484354a59c39c6833f7078\",\"si\":100}\\' crossorigin=\"anonymous\"></script>\\n</body></html>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "5CHvcg0Qlg0mfmsuY214GV",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8tpsRz9tIIsa",
        "outputId": "adf28032-1dc3-458f-bb3e-28f11da6c0e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "results = soup.find_all(['h1', 'p'])\n",
        "results  #array"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"be b dw dx eg dy dz eh ea eb ei ec ed ej ee ef ek el em eo ep eq er es et eu ev ew ex ey ez fa bl fb\" data-testid=\"headerSignUpButton\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign up</a></span></p>,\n",
              " <p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign In</a></span></p>,\n",
              " <p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"be b dw dx eg dy dz eh ea eb ei ec ed ej ee ef ek el em eo ep eq er es et eu ev ew ex ey ez fa bl fb\" data-testid=\"headerSignUpButton\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign up</a></span></p>,\n",
              " <p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign In</a></span></p>,\n",
              " <h1 class=\"pw-post-title go gp gq be gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl hm bj\" data-testid=\"storyTitle\" id=\"2ae4\">Build your own Transformer from scratch using Pytorch</h1>,\n",
              " <p class=\"be b jc jd bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar je\" data-testid=\"authorName\" href=\"https://arjun-sarkar786.medium.com/?source=post_page-----84c850470dcb--------------------------------\" rel=\"noopener follow\">Arjun Sarkar</a></p>,\n",
              " <p class=\"be b jc jd dt\"><span><a class=\"jh ji ah ai aj ak al am an ao ap aq ar eu jj jk\" href=\"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffa31366b2eda&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-your-own-transformer-from-scratch-using-pytorch-84c850470dcb&amp;user=Arjun+Sarkar&amp;userId=fa31366b2eda&amp;source=post_page-fa31366b2eda----84c850470dcb---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p>,\n",
              " <p class=\"be b bf z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">5</span></p>,\n",
              " <p class=\"be b bf z dt\">Listen</p>,\n",
              " <p class=\"be b bf z dt\">Share</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"d20e\">In this tutorial, we will build a basic Transformer model from scratch using PyTorch. The Transformer model, introduced by Vaswani et al. in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"87ad\">To understand Transformer models in detail kindly visit these two articles:</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"166f\">To build our Transformer model, we’ll follow these steps:</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"24e9\">Let’s start by importing the necessary libraries and modules.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"c34a\">Now, we’ll define the basic building blocks of the Transformer model.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"7671\">Multi-Head Attention</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"10a9\">The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"3965\">The MultiHeadAttention code initializes the module with input parameters and linear transformation layers. It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads. The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"bcae\">Position-wise Feed-Forward Networks</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"5ad2\">The PositionWiseFeedForward class extends PyTorch’s nn.Module and implements a position-wise feed-forward network. The class initializes with two linear transformation layers and a ReLU activation function. The forward method applies these transformations and activation function sequentially to compute the output. This process enables the model to consider the position of input elements while making predictions.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"69ba\">Positional Encoding</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\" id=\"2f9a\">Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"18d5\">The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values. The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term. The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"730e\">Now, we’ll build the Encoder and Decoder layers.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"e8c3\">Encoder Layer</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"3062\">An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"008b\">The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer. The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result. Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"c227\">Decoder Layer</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"b9dd\">A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"2a69\">The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"6196\">The forward method computes the decoder layer output by performing the following steps:</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"0beb\">These operations enable the decoder to generate target sequences based on the input and the encoder output.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"8064\">Now, let’s combine the Encoder and Decoder layers to create the complete Transformer model.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"b207\">Transformer Model</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"1a77\">Merging it all together:</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"117f\">The Transformer class combines the previously defined modules to create a complete Transformer model. During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"8fb5\">The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens. The forward method computes the Transformer model’s output through the following steps:</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"67a9\">These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt rs hq pc qv rt ht pf qx ru qz ra rb rv rd re rf rw rh ri rj bj\" id=\"66ad\">Preparing Sample Data</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\" id=\"4429\">In this example, we will create a toy dataset for demonstration purposes. In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"4483\">Training the Model</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\" id=\"4509\">Now we’ll train the model using the sample data. In practice, you would use a larger dataset and split it into training and validation sets.</p>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho oe of og hr oh oi oj ok ol om on oo op oq or os ot ou ov ow gj bj\" id=\"c4f7\">We can use this way to build a simple Transformer from scratch in Pytorch. All Large Language Models use these Transformer encoder or decoder blocks for training. Hence understanding the network that started it all is extremely important. Hope this article helps all looking to deep dive into LLM’s.</p>,\n",
              " <h1 class=\"qs oy gq be oz qt rs hq pc qv rt ht pf qx ru qz ra rb rv rd re rf rw rh ri rj bj\" id=\"004c\">References</h1>,\n",
              " <h1 class=\"qs oy gq be oz qt qu hq pc qv qw ht pf qx qy qz ra rb rc rd re rf rg rh ri rj bj\" id=\"e52a\">Attention is all you need</h1>,\n",
              " <p class=\"pw-post-body-paragraph ob oc gq od b ho rl of og hr rm oi oj ok rn om on oo ro oq or os rp ou ov ow gj bj\" id=\"8667\"><a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/0\" rel=\"noopener ugc nofollow\" target=\"_blank\">A. Vaswani</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/1\" rel=\"noopener ugc nofollow\" target=\"_blank\">N. Shazeer</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/2\" rel=\"noopener ugc nofollow\" target=\"_blank\">N. Parmar</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/3\" rel=\"noopener ugc nofollow\" target=\"_blank\">J. Uszkoreit</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/4\" rel=\"noopener ugc nofollow\" target=\"_blank\">L. Jones</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/5\" rel=\"noopener ugc nofollow\" target=\"_blank\">A. Gomez</a>, <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/6\" rel=\"noopener ugc nofollow\" target=\"_blank\">{. Kaiser</a>, and <a class=\"af oa\" href=\"https://www.bibsonomy.org/person/1c9bf08cbcb15680c807e12a01dd8c929/author/7\" rel=\"noopener ugc nofollow\" target=\"_blank\">I. Polosukhin</a>. <em class=\"rx\">Advances in Neural Information Processing Systems , page 5998–6008. </em>(<em class=\"rx\">2017</em>)</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b bf z dt\"><span class=\"pw-responses-count md me\">5</span></p>,\n",
              " <p class=\"be b bf z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b bf z bj\">Ph.D. student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany. LinkedIn-<a class=\"af ag ah ai aj ak al am an ao ap aq ar oa gk\" href=\"https://www.linkedin.com/in/arjun-sarkar-9a051777/\" rel=\"noopener ugc nofollow\">https://www.linkedin.com/in/arjun-sarkar-9a051777/</a></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Arjun Sarkar</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">15</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Bex T.</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">10</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Maxime Labonne</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">33</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Arjun Sarkar</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">5</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Dominik Polzer</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Towards Data Science</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">42</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Fahim Rustamy, PhD</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">6</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">Yule Wang, PhD</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\"><span class=\"pw-responses-count md me\">3</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">FunCry</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">ai geek (wishesh)</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">DemoGPT</p>,\n",
              " <p class=\"be b du z dt\">in</p>,\n",
              " <p class=\"be b du z jt ju jv jw jx jy jz ka bj\">AI Mind</p>,\n",
              " <p class=\"be b du z dt\"><span class=\"mc\">--</span></p>,\n",
              " <p class=\"be b du z dt\">Help</p>,\n",
              " <p class=\"be b du z dt\">Status</p>,\n",
              " <p class=\"be b du z dt\">Writers</p>,\n",
              " <p class=\"be b du z dt\">Blog</p>,\n",
              " <p class=\"be b du z dt\">Careers</p>,\n",
              " <p class=\"be b du z dt\">Privacy</p>,\n",
              " <p class=\"be b du z dt\">Terms</p>,\n",
              " <p class=\"be b du z dt\">About</p>,\n",
              " <p class=\"be b du z dt\">Text to speech</p>,\n",
              " <p class=\"be b du z dt\">Teams</p>]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "XxQ3HIaIrwW0h3mLhxJ1Tm",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm4ABUBTIIsa",
        "outputId": "ef66b5a9-787f-4060-b97d-0b9361285b6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [result.text for result in results]\n",
        "ARTICLE = ' '.join(text)"
      ],
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "d9IlnlzhsXoMZwD3owJu7n",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "id": "YVci762OIIsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
        "ARTICLE = ARTICLE.replace('!', '!<eos>')\n",
        "ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
        "sentences = ARTICLE.split('<eos>')\n",
        "sentences"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sign up Sign In Sign up Sign In Build your own Transformer from scratch using Pytorch Arjun Sarkar Follow Towards Data Science -- 5 Listen Share In this tutorial, we will build a basic Transformer model from scratch using PyTorch.',\n",
              " ' The Transformer model, introduced by Vaswani et al.',\n",
              " ' in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization.',\n",
              " ' It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.',\n",
              " ' To understand Transformer models in detail kindly visit these two articles: To build our Transformer model, we’ll follow these steps: Let’s start by importing the necessary libraries and modules.',\n",
              " ' Now, we’ll define the basic building blocks of the Transformer model.',\n",
              " ' Multi-Head Attention The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence.',\n",
              " ' It consists of multiple “attention heads” that capture different aspects of the input sequence.',\n",
              " ' The MultiHeadAttention code initializes the module with input parameters and linear transformation layers.',\n",
              " ' It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads.',\n",
              " ' The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.',\n",
              " ' Position-wise Feed-Forward Networks The PositionWiseFeedForward class extends PyTorch’s nn.',\n",
              " 'Module and implements a position-wise feed-forward network.',\n",
              " ' The class initializes with two linear transformation layers and a ReLU activation function.',\n",
              " ' The forward method applies these transformations and activation function sequentially to compute the output.',\n",
              " ' This process enables the model to consider the position of input elements while making predictions.',\n",
              " ' Positional Encoding Positional Encoding is used to inject the position information of each token in the input sequence.',\n",
              " ' It uses sine and cosine functions of different frequencies to generate the positional encoding.',\n",
              " ' The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values.',\n",
              " ' The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term.',\n",
              " ' The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence.',\n",
              " ' Now, we’ll build the Encoder and Decoder layers.',\n",
              " ' Encoder Layer An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.',\n",
              " ' The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer.',\n",
              " ' The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result.',\n",
              " ' Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.',\n",
              " ' Decoder Layer A Decoder layer consists of two Multi-Head Attention layers, a Position-wise Feed-Forward layer, and three Layer Normalization layers.',\n",
              " ' The DecoderLayer initializes with input parameters and components such as MultiHeadAttention modules for masked self-attention and cross-attention, a PositionWiseFeedForward module, three layer normalization modules, and a dropout layer.',\n",
              " ' The forward method computes the decoder layer output by performing the following steps: These operations enable the decoder to generate target sequences based on the input and the encoder output.',\n",
              " ' Now, let’s combine the Encoder and Decoder layers to create the complete Transformer model.',\n",
              " ' Transformer Model Merging it all together: The Transformer class combines the previously defined modules to create a complete Transformer model.',\n",
              " ' During initialization, the Transformer module sets up input parameters and initializes various components, including embedding layers for source and target sequences, a PositionalEncoding module, EncoderLayer and DecoderLayer modules to create stacked layers, a linear layer for projecting decoder output, and a dropout layer.',\n",
              " ' The generate_mask method creates binary masks for source and target sequences to ignore padding tokens and prevent the decoder from attending to future tokens.',\n",
              " ' The forward method computes the Transformer model’s output through the following steps: These steps enable the Transformer model to process input sequences and generate output sequences based on the combined functionality of its components.',\n",
              " ' Preparing Sample Data In this example, we will create a toy dataset for demonstration purposes.',\n",
              " ' In practice, you would use a larger dataset, preprocess the text, and create vocabulary mappings for source and target languages.',\n",
              " ' Training the Model Now we’ll train the model using the sample data.',\n",
              " ' In practice, you would use a larger dataset and split it into training and validation sets.',\n",
              " ' We can use this way to build a simple Transformer from scratch in Pytorch.',\n",
              " ' All Large Language Models use these Transformer encoder or decoder blocks for training.',\n",
              " ' Hence understanding the network that started it all is extremely important.',\n",
              " ' Hope this article helps all looking to deep dive into LLM’s.',\n",
              " ' References Attention is all you need A.',\n",
              " ' Vaswani, N.',\n",
              " ' Shazeer, N.',\n",
              " ' Parmar, J.',\n",
              " ' Uszkoreit, L.',\n",
              " ' Jones, A.',\n",
              " ' Gomez, {.',\n",
              " ' Kaiser, and I.',\n",
              " ' Polosukhin.',\n",
              " ' Advances in Neural Information Processing Systems , page 5998–6008.',\n",
              " ' (2017) -- -- 5 Towards Data Science Ph.',\n",
              " 'D.',\n",
              " ' student — Deep Learning on Biomedical Images at the Leibniz Institute-HKI, Germany.',\n",
              " ' LinkedIn-https://www.',\n",
              " 'linkedin.',\n",
              " 'com/in/arjun-sarkar-9a051777/ Arjun Sarkar in Towards Data Science -- 15 Bex T.',\n",
              " ' in Towards Data Science -- 10 Maxime Labonne in Towards Data Science -- 33 Arjun Sarkar in Towards Data Science -- 5 Dominik Polzer in Towards Data Science -- 42 Fahim Rustamy, PhD -- 6 Yule Wang, PhD -- 3 FunCry -- ai geek (wishesh) -- DemoGPT in AI Mind -- Help Status Writers Blog Careers Privacy Terms About Text to speech Teams']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "Uc3z66KQF6NigpoIr0iKow",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k2-Zc_JIIsa",
        "outputId": "523868f1-aa9a-4615-90a6-b7bdd78b6f97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_chunk = 500\n",
        "current_chunk = 0\n",
        "chunks = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    if len(chunks) == current_chunk + 1: # if we don't have a current chunk\n",
        "        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
        "            chunks[current_chunk].extend(sentence.split(' '))\n",
        "        else:\n",
        "            current_chunk += 1\n",
        "            chunks.append(sentence.split(' '))\n",
        "    else:\n",
        "        print(current_chunk)\n",
        "        chunks.append(sentence.split(' ')) # split into individual words"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "nBJvRZf1jeKi8IM3gylZTu",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yh4VIA1IIsb",
        "outputId": "0c69cb78-7ac7-47e1-8a37-77f0326784f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk_id in range(len(chunks)):\n",
        "    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
        "chunks[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sign up Sign In Sign up Sign In Build your own Transformer from scratch using Pytorch Arjun Sarkar Follow Towards Data Science -- 5 Listen Share In this tutorial, we will build a basic Transformer model from scratch using PyTorch.  The Transformer model, introduced by Vaswani et al.  in the paper “Attention is All You Need,” is a deep learning architecture designed for sequence-to-sequence tasks, such as machine translation and text summarization.  It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.  To understand Transformer models in detail kindly visit these two articles: To build our Transformer model, we’ll follow these steps: Let’s start by importing the necessary libraries and modules.  Now, we’ll define the basic building blocks of the Transformer model.  Multi-Head Attention The Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence.  It consists of multiple “attention heads” that capture different aspects of the input sequence.  The MultiHeadAttention code initializes the module with input parameters and linear transformation layers.  It calculates attention scores, reshapes the input tensor into multiple heads, and combines the attention outputs from all heads.  The forward method computes the multi-head self-attention, allowing the model to focus on some different aspects of the input sequence.  Position-wise Feed-Forward Networks The PositionWiseFeedForward class extends PyTorch’s nn. Module and implements a position-wise feed-forward network.  The class initializes with two linear transformation layers and a ReLU activation function.  The forward method applies these transformations and activation function sequentially to compute the output.  This process enables the model to consider the position of input elements while making predictions.  Positional Encoding Positional Encoding is used to inject the position information of each token in the input sequence.  It uses sine and cosine functions of different frequencies to generate the positional encoding.  The PositionalEncoding class initializes with input parameters d_model and max_seq_length, creating a tensor to store positional encoding values.  The class calculates sine and cosine values for even and odd indices, respectively, based on the scaling factor div_term.  The forward method computes the positional encoding by adding the stored positional encoding values to the input tensor, allowing the model to capture the position information of the input sequence.  Now, we’ll build the Encoder and Decoder layers.  Encoder Layer An Encoder layer consists of a Multi-Head Attention layer, a Position-wise Feed-Forward layer, and two Layer Normalization layers.  The EncoderLayer class initializes with input parameters and components, including a MultiHeadAttention module, a PositionWiseFeedForward module, two layer normalization modules, and a dropout layer.  The forward methods computes the encoder layer output by applying self-attention, adding the attention output to the input tensor, and normalizing the result.  Then, it computes the position-wise feed-forward output, combines it with the normalized self-attention output, and normalizes the final result before returning the processed tensor.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "4SsNd346FCWYh6SdrPOSwR",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "RXJ1hRCCIIsb",
        "outputId": "433dc44c-3fc5-4c8e-8f37-8fc2e4641f70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "iPykXDKqn1vCs2GMQtNPax",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXcL3j4XIIsc",
        "outputId": "f85ffb13-0af3-46bb-dc73-5a6e85fa3cde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = summarizer(chunks, max_length=86, min_length=10, do_sample=False)  # set the max and min length of the summary that you want, summary of each text is limited to 86 words.\n",
        "res # res is a dictionary"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'The Transformer model is a deep learning architecture designed for sequence-to-sequence tasks. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.'},\n",
              " {'summary_text': 'The Transformer class combines the previously defined modules to create a complete Transformer model. All Large Language Models use these Transformer encoder or decoder blocks for training. We can use this way to build a simple Transformer from scratch in Pytorch.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "mWYUi2Zozb9wI4tgvxFEkf",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOCSs-mXIIsc",
        "outputId": "50d4f20d-d2e8-4012-acb1-5ec5327bc8b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res[0]['summary_text'] # grabbing the value using the key"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer model is a deep learning architecture designed for sequence-to-sequence tasks. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "r8h9J2gRKjMLYJ9oCQ7TP0",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "J5tP5_MRIIsc",
        "outputId": "5954f020-6c03-44a9-818c-dddef6873070"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([summ['summary_text'] for summ in res])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Transformer model is a deep learning architecture designed for sequence-to-sequence tasks. It is based on self-attention mechanisms and has become the foundation for many state-of-the-art natural language processing models, like GPT and BERT. The Transformer class combines the previously defined modules to create a complete Transformer model. All Large Language Models use these Transformer encoder or decoder blocks for training. We can use this way to build a simple Transformer from scratch in Pytorch.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "oPBkS4yVqo5ZyWGLBZHzNV",
          "type": "CODE",
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "F6YMY__XIIsc",
        "outputId": "6c3ba093-a58c-4925-9624-c266edccb006"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python"
    },
    "datalore": {
      "computation_mode": "JUPYTER",
      "package_manager": "pip",
      "base_environment": "default",
      "packages": [],
      "report_row_ids": [],
      "version": 3
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29ee1b7d96f347ac87d2c85b7df18d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d35939e546c41dea7312bbdfc2de293",
              "IPY_MODEL_4b3b81b6a2b746eaaf2e1b735d6bfc59",
              "IPY_MODEL_f7053e9a9c5345dcb86798ca8df7d550"
            ],
            "layout": "IPY_MODEL_4b3672eba4ee41f09fdb245e2b26b822"
          }
        },
        "3d35939e546c41dea7312bbdfc2de293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8613e45a006648d9997631ea4272c4bc",
            "placeholder": "​",
            "style": "IPY_MODEL_1eecc1beae1342d099daa3bfdb07390b",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "4b3b81b6a2b746eaaf2e1b735d6bfc59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_243516f7b77645e1bd1d425fcb3f4c77",
            "max": 1585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d081f67995b4eebb37936893ce7ad6c",
            "value": 1585
          }
        },
        "f7053e9a9c5345dcb86798ca8df7d550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae41d02ab5324cf38bc4423177d4348d",
            "placeholder": "​",
            "style": "IPY_MODEL_8191c6824dd649439918b15f42449067",
            "value": " 1.58k/1.58k [00:00&lt;00:00, 26.4kB/s]"
          }
        },
        "4b3672eba4ee41f09fdb245e2b26b822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8613e45a006648d9997631ea4272c4bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eecc1beae1342d099daa3bfdb07390b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "243516f7b77645e1bd1d425fcb3f4c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d081f67995b4eebb37936893ce7ad6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae41d02ab5324cf38bc4423177d4348d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8191c6824dd649439918b15f42449067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10d9de26ea494895a9bdc89e2868e0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94a29508c2984e7197c3401c873fdfeb",
              "IPY_MODEL_bc0dab1658eb40428652618f65842398",
              "IPY_MODEL_65e212d1b64e46efa0bc9d37f0cd23a1"
            ],
            "layout": "IPY_MODEL_23493061151f4ffbbfe8aaf95850a6e9"
          }
        },
        "94a29508c2984e7197c3401c873fdfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e6f7fbae61e4363b92839697f583fdb",
            "placeholder": "​",
            "style": "IPY_MODEL_2ac305a9b41c4646b41881a062e2185a",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "bc0dab1658eb40428652618f65842398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1906740a37b94f399b4601ef7d192b10",
            "max": 1625270765,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44bb76ee3f1a419ca63ab41fed99716a",
            "value": 1625270765
          }
        },
        "65e212d1b64e46efa0bc9d37f0cd23a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec82a23e7f3d4fd6ac526ee792dcfdd2",
            "placeholder": "​",
            "style": "IPY_MODEL_67c5899d27264bb9b7371c894f5e4e34",
            "value": " 1.63G/1.63G [00:16&lt;00:00, 64.2MB/s]"
          }
        },
        "23493061151f4ffbbfe8aaf95850a6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e6f7fbae61e4363b92839697f583fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac305a9b41c4646b41881a062e2185a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1906740a37b94f399b4601ef7d192b10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bb76ee3f1a419ca63ab41fed99716a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec82a23e7f3d4fd6ac526ee792dcfdd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c5899d27264bb9b7371c894f5e4e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d699a90d1d8e4d2d965e77e3655f6f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c47610d259ad482c93f8b919636368ba",
              "IPY_MODEL_02f0a6d1ce8e4ec8b3b70a383d1c0124",
              "IPY_MODEL_dcbc9d6444b846dd8d8ff55cd1360c5c"
            ],
            "layout": "IPY_MODEL_42945f080d7d46fba74bac12646641f7"
          }
        },
        "c47610d259ad482c93f8b919636368ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51cb1a0725604c278b390494be7780bd",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc6bd7eaa5e436b96dbee8a2b3f0cbb",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "02f0a6d1ce8e4ec8b3b70a383d1c0124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d5f57027de48aea175a8c60a03a82e",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75909d21de3b44d1a5850145f7ef58ba",
            "value": 363
          }
        },
        "dcbc9d6444b846dd8d8ff55cd1360c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03efe9ea03bb4e9bbccaea7392bff3c5",
            "placeholder": "​",
            "style": "IPY_MODEL_057606b171a34fbe804ce0e36cedf03b",
            "value": " 363/363 [00:00&lt;00:00, 3.71kB/s]"
          }
        },
        "42945f080d7d46fba74bac12646641f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51cb1a0725604c278b390494be7780bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc6bd7eaa5e436b96dbee8a2b3f0cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7d5f57027de48aea175a8c60a03a82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75909d21de3b44d1a5850145f7ef58ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03efe9ea03bb4e9bbccaea7392bff3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057606b171a34fbe804ce0e36cedf03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee28e93593284c32b35f846ca909ffcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_946bef713c5b425999d75eb5ba2a643b",
              "IPY_MODEL_e1d3c09d38ba422eb6f6e06a90008f68",
              "IPY_MODEL_57f3022848c14ea2b37c2a67d4f886e2"
            ],
            "layout": "IPY_MODEL_55b315fd16cf4faaa4cf5b3fef1b43c3"
          }
        },
        "946bef713c5b425999d75eb5ba2a643b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7e2da9f4acd4396ac34a9bd7631e4df",
            "placeholder": "​",
            "style": "IPY_MODEL_541ab76cb0ee482eb8b6f0ec01081a64",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "e1d3c09d38ba422eb6f6e06a90008f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75160d163d854664b461fa7f895a3348",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32199f660eac4a5cbe3cde821a588555",
            "value": 898823
          }
        },
        "57f3022848c14ea2b37c2a67d4f886e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1093ec4782904073b39536328580a06e",
            "placeholder": "​",
            "style": "IPY_MODEL_698787a400bd4cd39af24f9f105cff4f",
            "value": " 899k/899k [00:00&lt;00:00, 6.94MB/s]"
          }
        },
        "55b315fd16cf4faaa4cf5b3fef1b43c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e2da9f4acd4396ac34a9bd7631e4df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541ab76cb0ee482eb8b6f0ec01081a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75160d163d854664b461fa7f895a3348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32199f660eac4a5cbe3cde821a588555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1093ec4782904073b39536328580a06e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698787a400bd4cd39af24f9f105cff4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f56d89e4def34d4ab7a8dd528d79639d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d131dee1b6aa402ab5bb5f06f5bea702",
              "IPY_MODEL_b34bb350ad314fb79bd7c1c7189dcd89",
              "IPY_MODEL_88509447ff3b4c3c823ed0f7ebbff72c"
            ],
            "layout": "IPY_MODEL_bdf25daaebe04550a1a7b79b7fd46695"
          }
        },
        "d131dee1b6aa402ab5bb5f06f5bea702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2740ca4a456d4f91aee237a08b999292",
            "placeholder": "​",
            "style": "IPY_MODEL_2a93e6c725b74b3a922448665ebe174f",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "b34bb350ad314fb79bd7c1c7189dcd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6f716bfc9b64843b4bdb07f084bf984",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15d626dc5c7647c1b72d851688ef4024",
            "value": 456318
          }
        },
        "88509447ff3b4c3c823ed0f7ebbff72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0906ee44fc4c440aab4c6e51a6568b86",
            "placeholder": "​",
            "style": "IPY_MODEL_d257a40dc32b45efb17969d5fdf45495",
            "value": " 456k/456k [00:00&lt;00:00, 6.97MB/s]"
          }
        },
        "bdf25daaebe04550a1a7b79b7fd46695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2740ca4a456d4f91aee237a08b999292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a93e6c725b74b3a922448665ebe174f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6f716bfc9b64843b4bdb07f084bf984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d626dc5c7647c1b72d851688ef4024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0906ee44fc4c440aab4c6e51a6568b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d257a40dc32b45efb17969d5fdf45495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b60dd14e57a4baf958bd82b2bdd28b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6070947f67e54c7db0a07d3a80363d8b",
              "IPY_MODEL_611de2c8477c4d66aeea4dbb98fd221a",
              "IPY_MODEL_1d9c17d53f7c47e2a0e28de6041de85e"
            ],
            "layout": "IPY_MODEL_2b76093fa11640f8b6f38cd93cd64033"
          }
        },
        "6070947f67e54c7db0a07d3a80363d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbbd256b6194f0e8efbed69956b5d2a",
            "placeholder": "​",
            "style": "IPY_MODEL_cb62c22770ba47febd146186be221880",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "611de2c8477c4d66aeea4dbb98fd221a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6dbf618b9db459bb06c28fcc0756075",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bbc28ddc8044ad299e715830ea303cd",
            "value": 1355863
          }
        },
        "1d9c17d53f7c47e2a0e28de6041de85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_044acbcbb6cd442880c8a7b4b44632fc",
            "placeholder": "​",
            "style": "IPY_MODEL_4567677e22ac4da284a3aae3dfdc5b65",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 18.7MB/s]"
          }
        },
        "2b76093fa11640f8b6f38cd93cd64033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bbbd256b6194f0e8efbed69956b5d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb62c22770ba47febd146186be221880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6dbf618b9db459bb06c28fcc0756075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bbc28ddc8044ad299e715830ea303cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "044acbcbb6cd442880c8a7b4b44632fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4567677e22ac4da284a3aae3dfdc5b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}